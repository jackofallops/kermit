package search

// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for license information.
//
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.

import (
	"encoding/json"

	"github.com/Azure/go-autorest/autorest"
	"github.com/Azure/go-autorest/autorest/date"
)

// The package's fully qualified name.
const fqdn = "home/runner/work/kermit/kermit/sdk/search/2024-03-01-preview/search"

// ASCIIFoldingTokenFilter converts alphabetic, numeric, and symbolic Unicode characters which are not in
// the first 127 ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such
// equivalents exist. This token filter is implemented using Apache Lucene.
type ASCIIFoldingTokenFilter struct {
	// PreserveOriginal - A value indicating whether the original token will be kept. Default is false.
	PreserveOriginal *bool `json:"preserveOriginal,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) MarshalJSON() ([]byte, error) {
	aftf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter
	objectMap := make(map[string]interface{})
	if aftf.PreserveOriginal != nil {
		objectMap["preserveOriginal"] = aftf.PreserveOriginal
	}
	if aftf.Name != nil {
		objectMap["name"] = aftf.Name
	}
	if aftf.OdataType != "" {
		objectMap["@odata.type"] = aftf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return &aftf, true
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &aftf, true
}

// Alias represents an index alias, which describes a mapping from the alias name to an index. The alias
// name can be used in place of the index name for supported operations.
type Alias struct {
	autorest.Response `json:"-"`
	// Name - The name of the alias.
	Name *string `json:"name,omitempty"`
	// Indexes - The name of the index this alias maps to. Only one index name may be specified.
	Indexes *[]string `json:"indexes,omitempty"`
	// ETag - The ETag of the alias.
	ETag *string `json:"@odata.etag,omitempty"`
}

// AmlSkill the AML skill allows you to extend AI enrichment with a custom Azure Machine Learning (AML)
// model. Once an AML model is trained and deployed, an AML skill integrates it into AI enrichment.
type AmlSkill struct {
	// ScoringURI - (Required for no authentication or key authentication) The scoring URI of the AML service to which the JSON payload will be sent. Only the https URI scheme is allowed.
	ScoringURI *string `json:"uri,omitempty"`
	// AuthenticationKey - (Required for key authentication) The key for the AML service.
	AuthenticationKey *string `json:"key,omitempty"`
	// ResourceID - (Required for token authentication). The Azure Resource Manager resource ID of the AML service. It should be in the format subscriptions/{guid}/resourceGroups/{resource-group-name}/Microsoft.MachineLearningServices/workspaces/{workspace-name}/services/{service_name}.
	ResourceID *string `json:"resourceId,omitempty"`
	// Timeout - (Optional) When specified, indicates the timeout for the http client making the API call.
	Timeout *string `json:"timeout,omitempty"`
	// Region - (Optional for token authentication). The region the AML service is deployed in.
	Region *string `json:"region,omitempty"`
	// DegreeOfParallelism - (Optional) When specified, indicates the number of calls the indexer will make in parallel to the endpoint you have provided. You can decrease this value if your endpoint is failing under too high of a request load, or raise it if your endpoint is able to accept more requests and you would like an increase in the performance of the indexer. If not set, a default value of 5 is used. The degreeOfParallelism can be set to a maximum of 10 and a minimum of 1.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for AmlSkill.
func (as AmlSkill) MarshalJSON() ([]byte, error) {
	as.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill
	objectMap := make(map[string]interface{})
	if as.ScoringURI != nil {
		objectMap["uri"] = as.ScoringURI
	}
	if as.AuthenticationKey != nil {
		objectMap["key"] = as.AuthenticationKey
	}
	if as.ResourceID != nil {
		objectMap["resourceId"] = as.ResourceID
	}
	if as.Timeout != nil {
		objectMap["timeout"] = as.Timeout
	}
	if as.Region != nil {
		objectMap["region"] = as.Region
	}
	if as.DegreeOfParallelism != nil {
		objectMap["degreeOfParallelism"] = as.DegreeOfParallelism
	}
	if as.Name != nil {
		objectMap["name"] = as.Name
	}
	if as.Description != nil {
		objectMap["description"] = as.Description
	}
	if as.Context != nil {
		objectMap["context"] = as.Context
	}
	if as.Inputs != nil {
		objectMap["inputs"] = as.Inputs
	}
	if as.Outputs != nil {
		objectMap["outputs"] = as.Outputs
	}
	if as.OdataType != "" {
		objectMap["@odata.type"] = as.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsAmlSkill() (*AmlSkill, bool) {
	return &as, true
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for AmlSkill.
func (as AmlSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &as, true
}

// AnalyzeRequest specifies some text and analysis components used to break that text into tokens.
type AnalyzeRequest struct {
	// Text - The text to break into tokens.
	Text *string `json:"text,omitempty"`
	// Analyzer - The name of the analyzer to use to break the given text. Possible values include: 'LexicalAnalyzerNameArMicrosoft', 'LexicalAnalyzerNameArLucene', 'LexicalAnalyzerNameHyLucene', 'LexicalAnalyzerNameBnMicrosoft', 'LexicalAnalyzerNameEuLucene', 'LexicalAnalyzerNameBgMicrosoft', 'LexicalAnalyzerNameBgLucene', 'LexicalAnalyzerNameCaMicrosoft', 'LexicalAnalyzerNameCaLucene', 'LexicalAnalyzerNameZhHansMicrosoft', 'LexicalAnalyzerNameZhHansLucene', 'LexicalAnalyzerNameZhHantMicrosoft', 'LexicalAnalyzerNameZhHantLucene', 'LexicalAnalyzerNameHrMicrosoft', 'LexicalAnalyzerNameCsMicrosoft', 'LexicalAnalyzerNameCsLucene', 'LexicalAnalyzerNameDaMicrosoft', 'LexicalAnalyzerNameDaLucene', 'LexicalAnalyzerNameNlMicrosoft', 'LexicalAnalyzerNameNlLucene', 'LexicalAnalyzerNameEnMicrosoft', 'LexicalAnalyzerNameEnLucene', 'LexicalAnalyzerNameEtMicrosoft', 'LexicalAnalyzerNameFiMicrosoft', 'LexicalAnalyzerNameFiLucene', 'LexicalAnalyzerNameFrMicrosoft', 'LexicalAnalyzerNameFrLucene', 'LexicalAnalyzerNameGlLucene', 'LexicalAnalyzerNameDeMicrosoft', 'LexicalAnalyzerNameDeLucene', 'LexicalAnalyzerNameElMicrosoft', 'LexicalAnalyzerNameElLucene', 'LexicalAnalyzerNameGuMicrosoft', 'LexicalAnalyzerNameHeMicrosoft', 'LexicalAnalyzerNameHiMicrosoft', 'LexicalAnalyzerNameHiLucene', 'LexicalAnalyzerNameHuMicrosoft', 'LexicalAnalyzerNameHuLucene', 'LexicalAnalyzerNameIsMicrosoft', 'LexicalAnalyzerNameIDMicrosoft', 'LexicalAnalyzerNameIDLucene', 'LexicalAnalyzerNameGaLucene', 'LexicalAnalyzerNameItMicrosoft', 'LexicalAnalyzerNameItLucene', 'LexicalAnalyzerNameJaMicrosoft', 'LexicalAnalyzerNameJaLucene', 'LexicalAnalyzerNameKnMicrosoft', 'LexicalAnalyzerNameKoMicrosoft', 'LexicalAnalyzerNameKoLucene', 'LexicalAnalyzerNameLvMicrosoft', 'LexicalAnalyzerNameLvLucene', 'LexicalAnalyzerNameLtMicrosoft', 'LexicalAnalyzerNameMlMicrosoft', 'LexicalAnalyzerNameMsMicrosoft', 'LexicalAnalyzerNameMrMicrosoft', 'LexicalAnalyzerNameNbMicrosoft', 'LexicalAnalyzerNameNoLucene', 'LexicalAnalyzerNameFaLucene', 'LexicalAnalyzerNamePlMicrosoft', 'LexicalAnalyzerNamePlLucene', 'LexicalAnalyzerNamePtBrMicrosoft', 'LexicalAnalyzerNamePtBrLucene', 'LexicalAnalyzerNamePtPtMicrosoft', 'LexicalAnalyzerNamePtPtLucene', 'LexicalAnalyzerNamePaMicrosoft', 'LexicalAnalyzerNameRoMicrosoft', 'LexicalAnalyzerNameRoLucene', 'LexicalAnalyzerNameRuMicrosoft', 'LexicalAnalyzerNameRuLucene', 'LexicalAnalyzerNameSrCyrillicMicrosoft', 'LexicalAnalyzerNameSrLatinMicrosoft', 'LexicalAnalyzerNameSkMicrosoft', 'LexicalAnalyzerNameSlMicrosoft', 'LexicalAnalyzerNameEsMicrosoft', 'LexicalAnalyzerNameEsLucene', 'LexicalAnalyzerNameSvMicrosoft', 'LexicalAnalyzerNameSvLucene', 'LexicalAnalyzerNameTaMicrosoft', 'LexicalAnalyzerNameTeMicrosoft', 'LexicalAnalyzerNameThMicrosoft', 'LexicalAnalyzerNameThLucene', 'LexicalAnalyzerNameTrMicrosoft', 'LexicalAnalyzerNameTrLucene', 'LexicalAnalyzerNameUkMicrosoft', 'LexicalAnalyzerNameUrMicrosoft', 'LexicalAnalyzerNameViMicrosoft', 'LexicalAnalyzerNameStandardLucene', 'LexicalAnalyzerNameStandardASCIIFoldingLucene', 'LexicalAnalyzerNameKeyword', 'LexicalAnalyzerNamePattern', 'LexicalAnalyzerNameSimple', 'LexicalAnalyzerNameStop', 'LexicalAnalyzerNameWhitespace'
	Analyzer LexicalAnalyzerName `json:"analyzer,omitempty"`
	// Tokenizer - The name of the tokenizer to use to break the given text. Possible values include: 'LexicalTokenizerNameClassic', 'LexicalTokenizerNameEdgeNGram', 'LexicalTokenizerNameKeyword', 'LexicalTokenizerNameLetter', 'LexicalTokenizerNameLowercase', 'LexicalTokenizerNameMicrosoftLanguageTokenizer', 'LexicalTokenizerNameMicrosoftLanguageStemmingTokenizer', 'LexicalTokenizerNameNGram', 'LexicalTokenizerNamePathHierarchy', 'LexicalTokenizerNamePattern', 'LexicalTokenizerNameStandard', 'LexicalTokenizerNameUaxURLEmail', 'LexicalTokenizerNameWhitespace'
	Tokenizer LexicalTokenizerName `json:"tokenizer,omitempty"`
	// Normalizer - The name of the normalizer to use to normalize the given text. Possible values include: 'LexicalNormalizerNameASCIIFolding', 'LexicalNormalizerNameElision', 'LexicalNormalizerNameLowercase', 'LexicalNormalizerNameStandard', 'LexicalNormalizerNameUppercase'
	Normalizer LexicalNormalizerName `json:"normalizer,omitempty"`
	// TokenFilters - An optional list of token filters to use when breaking the given text.
	TokenFilters *[]TokenFilterName `json:"tokenFilters,omitempty"`
	// CharFilters - An optional list of character filters to use when breaking the given text.
	CharFilters *[]CharFilterName `json:"charFilters,omitempty"`
}

// AnalyzeResult the result of testing an analyzer on text.
type AnalyzeResult struct {
	autorest.Response `json:"-"`
	// Tokens - The list of tokens returned by the analyzer specified in the request.
	Tokens *[]AnalyzedTokenInfo `json:"tokens,omitempty"`
}

// AnalyzedTokenInfo information about a token returned by an analyzer.
type AnalyzedTokenInfo struct {
	// Token - READ-ONLY; The token returned by the analyzer.
	Token *string `json:"token,omitempty"`
	// StartOffset - READ-ONLY; The index of the first character of the token in the input text.
	StartOffset *int32 `json:"startOffset,omitempty"`
	// EndOffset - READ-ONLY; The index of the last character of the token in the input text.
	EndOffset *int32 `json:"endOffset,omitempty"`
	// Position - READ-ONLY; The position of the token in the input text relative to other tokens. The first token in the input text has position 0, the next has position 1, and so on. Depending on the analyzer used, some tokens might have the same position, for example if they are synonyms of each other.
	Position *int32 `json:"position,omitempty"`
}

// MarshalJSON is the custom marshaler for AnalyzedTokenInfo.
func (ati AnalyzedTokenInfo) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// AzureActiveDirectoryApplicationCredentials credentials of a registered application created for your
// search service, used for authenticated access to the encryption keys stored in Azure Key Vault.
type AzureActiveDirectoryApplicationCredentials struct {
	// ApplicationID - An AAD Application ID that was granted the required access permissions to the Azure Key Vault that is to be used when encrypting your data at rest. The Application ID should not be confused with the Object ID for your AAD Application.
	ApplicationID *string `json:"applicationId,omitempty"`
	// ApplicationSecret - The authentication key of the specified AAD application.
	ApplicationSecret *string `json:"applicationSecret,omitempty"`
}

// AzureOpenAIEmbeddingSkill allows you to generate a vector embedding for a given text input using the
// Azure OpenAI resource.
type AzureOpenAIEmbeddingSkill struct {
	// ResourceURI - The resource URI for your Azure OpenAI resource.
	ResourceURI *string `json:"resourceUri,omitempty"`
	// DeploymentID - ID of your Azure OpenAI model deployment on the designated resource.
	DeploymentID *string `json:"deploymentId,omitempty"`
	// APIKey - API key for the designated Azure OpenAI resource.
	APIKey *string `json:"apiKey,omitempty"`
	// AuthIdentity - The user-assigned managed identity used for outbound connections.
	AuthIdentity BasicIndexerDataIdentity `json:"authIdentity,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) MarshalJSON() ([]byte, error) {
	aoaes.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill
	objectMap := make(map[string]interface{})
	if aoaes.ResourceURI != nil {
		objectMap["resourceUri"] = aoaes.ResourceURI
	}
	if aoaes.DeploymentID != nil {
		objectMap["deploymentId"] = aoaes.DeploymentID
	}
	if aoaes.APIKey != nil {
		objectMap["apiKey"] = aoaes.APIKey
	}
	objectMap["authIdentity"] = aoaes.AuthIdentity
	if aoaes.Name != nil {
		objectMap["name"] = aoaes.Name
	}
	if aoaes.Description != nil {
		objectMap["description"] = aoaes.Description
	}
	if aoaes.Context != nil {
		objectMap["context"] = aoaes.Context
	}
	if aoaes.Inputs != nil {
		objectMap["inputs"] = aoaes.Inputs
	}
	if aoaes.Outputs != nil {
		objectMap["outputs"] = aoaes.Outputs
	}
	if aoaes.OdataType != "" {
		objectMap["@odata.type"] = aoaes.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return &aoaes, true
}

// AsIndexerSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for AzureOpenAIEmbeddingSkill.
func (aoaes AzureOpenAIEmbeddingSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &aoaes, true
}

// UnmarshalJSON is the custom unmarshaler for AzureOpenAIEmbeddingSkill struct.
func (aoaes *AzureOpenAIEmbeddingSkill) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "resourceUri":
			if v != nil {
				var resourceURI string
				err = json.Unmarshal(*v, &resourceURI)
				if err != nil {
					return err
				}
				aoaes.ResourceURI = &resourceURI
			}
		case "deploymentId":
			if v != nil {
				var deploymentID string
				err = json.Unmarshal(*v, &deploymentID)
				if err != nil {
					return err
				}
				aoaes.DeploymentID = &deploymentID
			}
		case "apiKey":
			if v != nil {
				var APIKey string
				err = json.Unmarshal(*v, &APIKey)
				if err != nil {
					return err
				}
				aoaes.APIKey = &APIKey
			}
		case "authIdentity":
			if v != nil {
				authIdentity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				aoaes.AuthIdentity = authIdentity
			}
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				aoaes.Name = &name
			}
		case "description":
			if v != nil {
				var description string
				err = json.Unmarshal(*v, &description)
				if err != nil {
					return err
				}
				aoaes.Description = &description
			}
		case "context":
			if v != nil {
				var context string
				err = json.Unmarshal(*v, &context)
				if err != nil {
					return err
				}
				aoaes.Context = &context
			}
		case "inputs":
			if v != nil {
				var inputs []InputFieldMappingEntry
				err = json.Unmarshal(*v, &inputs)
				if err != nil {
					return err
				}
				aoaes.Inputs = &inputs
			}
		case "outputs":
			if v != nil {
				var outputs []OutputFieldMappingEntry
				err = json.Unmarshal(*v, &outputs)
				if err != nil {
					return err
				}
				aoaes.Outputs = &outputs
			}
		case "@odata.type":
			if v != nil {
				var odataType OdataTypeBasicIndexerSkill
				err = json.Unmarshal(*v, &odataType)
				if err != nil {
					return err
				}
				aoaes.OdataType = odataType
			}
		}
	}

	return nil
}

// AzureOpenAIParameters specifies the parameters for connecting to the Azure OpenAI resource.
type AzureOpenAIParameters struct {
	// ResourceURI - The resource URI of the Azure OpenAI resource.
	ResourceURI *string `json:"resourceUri,omitempty"`
	// DeploymentID - ID of the Azure OpenAI model deployment on the designated resource.
	DeploymentID *string `json:"deploymentId,omitempty"`
	// APIKey - API key of the designated Azure OpenAI resource.
	APIKey *string `json:"apiKey,omitempty"`
	// AuthIdentity - The user-assigned managed identity used for outbound connections.
	AuthIdentity BasicIndexerDataIdentity `json:"authIdentity,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for AzureOpenAIParameters struct.
func (aoap *AzureOpenAIParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "resourceUri":
			if v != nil {
				var resourceURI string
				err = json.Unmarshal(*v, &resourceURI)
				if err != nil {
					return err
				}
				aoap.ResourceURI = &resourceURI
			}
		case "deploymentId":
			if v != nil {
				var deploymentID string
				err = json.Unmarshal(*v, &deploymentID)
				if err != nil {
					return err
				}
				aoap.DeploymentID = &deploymentID
			}
		case "apiKey":
			if v != nil {
				var APIKey string
				err = json.Unmarshal(*v, &APIKey)
				if err != nil {
					return err
				}
				aoap.APIKey = &APIKey
			}
		case "authIdentity":
			if v != nil {
				authIdentity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				aoap.AuthIdentity = authIdentity
			}
		}
	}

	return nil
}

// AzureOpenAIVectorizer specifies the Azure OpenAI resource used to vectorize a query string.
type AzureOpenAIVectorizer struct {
	// AzureOpenAIParameters - Contains the parameters specific to Azure OpenAI embedding vectorization.
	AzureOpenAIParameters *AzureOpenAIParameters `json:"azureOpenAIParameters,omitempty"`
	// Name - The name to associate with this particular vectorization method.
	Name *string `json:"name,omitempty"`
	// Kind - Possible values include: 'KindBasicVectorSearchVectorizerKindVectorSearchVectorizer', 'KindBasicVectorSearchVectorizerKindAzureOpenAI', 'KindBasicVectorSearchVectorizerKindCustomWebAPI'
	Kind KindBasicVectorSearchVectorizer `json:"kind,omitempty"`
}

// MarshalJSON is the custom marshaler for AzureOpenAIVectorizer.
func (aoav AzureOpenAIVectorizer) MarshalJSON() ([]byte, error) {
	aoav.Kind = KindBasicVectorSearchVectorizerKindAzureOpenAI
	objectMap := make(map[string]interface{})
	if aoav.AzureOpenAIParameters != nil {
		objectMap["azureOpenAIParameters"] = aoav.AzureOpenAIParameters
	}
	if aoav.Name != nil {
		objectMap["name"] = aoav.Name
	}
	if aoav.Kind != "" {
		objectMap["kind"] = aoav.Kind
	}
	return json.Marshal(objectMap)
}

// AsAzureOpenAIVectorizer is the BasicVectorSearchVectorizer implementation for AzureOpenAIVectorizer.
func (aoav AzureOpenAIVectorizer) AsAzureOpenAIVectorizer() (*AzureOpenAIVectorizer, bool) {
	return &aoav, true
}

// AsCustomVectorizer is the BasicVectorSearchVectorizer implementation for AzureOpenAIVectorizer.
func (aoav AzureOpenAIVectorizer) AsCustomVectorizer() (*CustomVectorizer, bool) {
	return nil, false
}

// AsVectorSearchVectorizer is the BasicVectorSearchVectorizer implementation for AzureOpenAIVectorizer.
func (aoav AzureOpenAIVectorizer) AsVectorSearchVectorizer() (*VectorSearchVectorizer, bool) {
	return nil, false
}

// AsBasicVectorSearchVectorizer is the BasicVectorSearchVectorizer implementation for AzureOpenAIVectorizer.
func (aoav AzureOpenAIVectorizer) AsBasicVectorSearchVectorizer() (BasicVectorSearchVectorizer, bool) {
	return &aoav, true
}

// BM25Similarity ranking function based on the Okapi BM25 similarity algorithm. BM25 is a TF-IDF-like
// algorithm that includes length normalization (controlled by the 'b' parameter) as well as term frequency
// saturation (controlled by the 'k1' parameter).
type BM25Similarity struct {
	// K1 - This property controls the scaling function between the term frequency of each matching terms and the final relevance score of a document-query pair. By default, a value of 1.2 is used. A value of 0.0 means the score does not scale with an increase in term frequency.
	K1 *float64 `json:"k1,omitempty"`
	// B - This property controls how the length of a document affects the relevance score. By default, a value of 0.75 is used. A value of 0.0 means no length normalization is applied, while a value of 1.0 means the score is fully normalized by the length of the document.
	B *float64 `json:"b,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicSimilarityOdataTypeSimilarity', 'OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchClassicSimilarity', 'OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchBM25Similarity'
	OdataType OdataTypeBasicSimilarity `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for BM25Similarity.
func (bs BM25Similarity) MarshalJSON() ([]byte, error) {
	bs.OdataType = OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchBM25Similarity
	objectMap := make(map[string]interface{})
	if bs.K1 != nil {
		objectMap["k1"] = bs.K1
	}
	if bs.B != nil {
		objectMap["b"] = bs.B
	}
	if bs.OdataType != "" {
		objectMap["@odata.type"] = bs.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicSimilarity is the BasicSimilarity implementation for BM25Similarity.
func (bs BM25Similarity) AsClassicSimilarity() (*ClassicSimilarity, bool) {
	return nil, false
}

// AsBM25Similarity is the BasicSimilarity implementation for BM25Similarity.
func (bs BM25Similarity) AsBM25Similarity() (*BM25Similarity, bool) {
	return &bs, true
}

// AsSimilarity is the BasicSimilarity implementation for BM25Similarity.
func (bs BM25Similarity) AsSimilarity() (*Similarity, bool) {
	return nil, false
}

// AsBasicSimilarity is the BasicSimilarity implementation for BM25Similarity.
func (bs BM25Similarity) AsBasicSimilarity() (BasicSimilarity, bool) {
	return &bs, true
}

// BasicCharFilter base type for character filters.
type BasicCharFilter interface {
	AsMappingCharFilter() (*MappingCharFilter, bool)
	AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool)
	AsCharFilter() (*CharFilter, bool)
}

// CharFilter base type for character filters.
type CharFilter struct {
	// Name - The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicCharFilterOdataTypeCharFilter', 'OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchMappingCharFilter', 'OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchPatternReplaceCharFilter'
	OdataType OdataTypeBasicCharFilter `json:"@odata.type,omitempty"`
}

func unmarshalBasicCharFilter(body []byte) (BasicCharFilter, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchMappingCharFilter):
		var mcf MappingCharFilter
		err := json.Unmarshal(body, &mcf)
		return mcf, err
	case string(OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchPatternReplaceCharFilter):
		var prcf PatternReplaceCharFilter
		err := json.Unmarshal(body, &prcf)
		return prcf, err
	default:
		var cf CharFilter
		err := json.Unmarshal(body, &cf)
		return cf, err
	}
}
func unmarshalBasicCharFilterArray(body []byte) ([]BasicCharFilter, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	cfArray := make([]BasicCharFilter, len(rawMessages))

	for index, rawMessage := range rawMessages {
		cf, err := unmarshalBasicCharFilter(*rawMessage)
		if err != nil {
			return nil, err
		}
		cfArray[index] = cf
	}
	return cfArray, nil
}

// MarshalJSON is the custom marshaler for CharFilter.
func (cf CharFilter) MarshalJSON() ([]byte, error) {
	cf.OdataType = OdataTypeBasicCharFilterOdataTypeCharFilter
	objectMap := make(map[string]interface{})
	if cf.Name != nil {
		objectMap["name"] = cf.Name
	}
	if cf.OdataType != "" {
		objectMap["@odata.type"] = cf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsMappingCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsMappingCharFilter() (*MappingCharFilter, bool) {
	return nil, false
}

// AsPatternReplaceCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool) {
	return nil, false
}

// AsCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsCharFilter() (*CharFilter, bool) {
	return &cf, true
}

// AsBasicCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsBasicCharFilter() (BasicCharFilter, bool) {
	return &cf, true
}

// CjkBigramTokenFilter forms bigrams of CJK terms that are generated from the standard tokenizer. This
// token filter is implemented using Apache Lucene.
type CjkBigramTokenFilter struct {
	// IgnoreScripts - The scripts to ignore.
	IgnoreScripts *[]CjkBigramTokenFilterScripts `json:"ignoreScripts,omitempty"`
	// OutputUnigrams - A value indicating whether to output both unigrams and bigrams (if true), or just bigrams (if false). Default is false.
	OutputUnigrams *bool `json:"outputUnigrams,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) MarshalJSON() ([]byte, error) {
	cbtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter
	objectMap := make(map[string]interface{})
	if cbtf.IgnoreScripts != nil {
		objectMap["ignoreScripts"] = cbtf.IgnoreScripts
	}
	if cbtf.OutputUnigrams != nil {
		objectMap["outputUnigrams"] = cbtf.OutputUnigrams
	}
	if cbtf.Name != nil {
		objectMap["name"] = cbtf.Name
	}
	if cbtf.OdataType != "" {
		objectMap["@odata.type"] = cbtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return &cbtf, true
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &cbtf, true
}

// ClassicSimilarity legacy similarity algorithm which uses the Lucene TFIDFSimilarity implementation of
// TF-IDF. This variation of TF-IDF introduces static document length normalization as well as coordinating
// factors that penalize documents that only partially match the searched queries.
type ClassicSimilarity struct {
	// OdataType - Possible values include: 'OdataTypeBasicSimilarityOdataTypeSimilarity', 'OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchClassicSimilarity', 'OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchBM25Similarity'
	OdataType OdataTypeBasicSimilarity `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ClassicSimilarity.
func (cs ClassicSimilarity) MarshalJSON() ([]byte, error) {
	cs.OdataType = OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchClassicSimilarity
	objectMap := make(map[string]interface{})
	if cs.OdataType != "" {
		objectMap["@odata.type"] = cs.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicSimilarity is the BasicSimilarity implementation for ClassicSimilarity.
func (cs ClassicSimilarity) AsClassicSimilarity() (*ClassicSimilarity, bool) {
	return &cs, true
}

// AsBM25Similarity is the BasicSimilarity implementation for ClassicSimilarity.
func (cs ClassicSimilarity) AsBM25Similarity() (*BM25Similarity, bool) {
	return nil, false
}

// AsSimilarity is the BasicSimilarity implementation for ClassicSimilarity.
func (cs ClassicSimilarity) AsSimilarity() (*Similarity, bool) {
	return nil, false
}

// AsBasicSimilarity is the BasicSimilarity implementation for ClassicSimilarity.
func (cs ClassicSimilarity) AsBasicSimilarity() (BasicSimilarity, bool) {
	return &cs, true
}

// ClassicTokenizer grammar-based tokenizer that is suitable for processing most European-language
// documents. This tokenizer is implemented using Apache Lucene.
type ClassicTokenizer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ClassicTokenizer.
func (ct ClassicTokenizer) MarshalJSON() ([]byte, error) {
	ct.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer
	objectMap := make(map[string]interface{})
	if ct.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = ct.MaxTokenLength
	}
	if ct.Name != nil {
		objectMap["name"] = ct.Name
	}
	if ct.OdataType != "" {
		objectMap["@odata.type"] = ct.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return &ct, true
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &ct, true
}

// BasicCognitiveServicesAccount base type for describing any Azure AI service resource attached to a skillset.
type BasicCognitiveServicesAccount interface {
	AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool)
	AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool)
	AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool)
}

// CognitiveServicesAccount base type for describing any Azure AI service resource attached to a skillset.
type CognitiveServicesAccount struct {
	// Description - Description of the Azure AI service resource attached to a skillset.
	Description *string `json:"description,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicCognitiveServicesAccountOdataTypeCognitiveServicesAccount', 'OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchDefaultCognitiveServices', 'OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchCognitiveServicesByKey'
	OdataType OdataTypeBasicCognitiveServicesAccount `json:"@odata.type,omitempty"`
}

func unmarshalBasicCognitiveServicesAccount(body []byte) (BasicCognitiveServicesAccount, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchDefaultCognitiveServices):
		var dcsa DefaultCognitiveServicesAccount
		err := json.Unmarshal(body, &dcsa)
		return dcsa, err
	case string(OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchCognitiveServicesByKey):
		var csak CognitiveServicesAccountKey
		err := json.Unmarshal(body, &csak)
		return csak, err
	default:
		var csa CognitiveServicesAccount
		err := json.Unmarshal(body, &csa)
		return csa, err
	}
}
func unmarshalBasicCognitiveServicesAccountArray(body []byte) ([]BasicCognitiveServicesAccount, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	csaArray := make([]BasicCognitiveServicesAccount, len(rawMessages))

	for index, rawMessage := range rawMessages {
		csa, err := unmarshalBasicCognitiveServicesAccount(*rawMessage)
		if err != nil {
			return nil, err
		}
		csaArray[index] = csa
	}
	return csaArray, nil
}

// MarshalJSON is the custom marshaler for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) MarshalJSON() ([]byte, error) {
	csa.OdataType = OdataTypeBasicCognitiveServicesAccountOdataTypeCognitiveServicesAccount
	objectMap := make(map[string]interface{})
	if csa.Description != nil {
		objectMap["description"] = csa.Description
	}
	if csa.OdataType != "" {
		objectMap["@odata.type"] = csa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsDefaultCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool) {
	return nil, false
}

// AsCognitiveServicesAccountKey is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool) {
	return nil, false
}

// AsCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool) {
	return &csa, true
}

// AsBasicCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsBasicCognitiveServicesAccount() (BasicCognitiveServicesAccount, bool) {
	return &csa, true
}

// CognitiveServicesAccountKey the multi-region account key of an Azure AI service resource that's attached
// to a skillset.
type CognitiveServicesAccountKey struct {
	// Key - The key used to provision the Azure AI service resource attached to a skillset.
	Key *string `json:"key,omitempty"`
	// Description - Description of the Azure AI service resource attached to a skillset.
	Description *string `json:"description,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicCognitiveServicesAccountOdataTypeCognitiveServicesAccount', 'OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchDefaultCognitiveServices', 'OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchCognitiveServicesByKey'
	OdataType OdataTypeBasicCognitiveServicesAccount `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) MarshalJSON() ([]byte, error) {
	csak.OdataType = OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchCognitiveServicesByKey
	objectMap := make(map[string]interface{})
	if csak.Key != nil {
		objectMap["key"] = csak.Key
	}
	if csak.Description != nil {
		objectMap["description"] = csak.Description
	}
	if csak.OdataType != "" {
		objectMap["@odata.type"] = csak.OdataType
	}
	return json.Marshal(objectMap)
}

// AsDefaultCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool) {
	return nil, false
}

// AsCognitiveServicesAccountKey is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool) {
	return &csak, true
}

// AsCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool) {
	return nil, false
}

// AsBasicCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsBasicCognitiveServicesAccount() (BasicCognitiveServicesAccount, bool) {
	return &csak, true
}

// CommonGramTokenFilter construct bigrams for frequently occurring terms while indexing. Single terms are
// still indexed too, with bigrams overlaid. This token filter is implemented using Apache Lucene.
type CommonGramTokenFilter struct {
	// CommonWords - The set of common words.
	CommonWords *[]string `json:"commonWords,omitempty"`
	// IgnoreCase - A value indicating whether common words matching will be case insensitive. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// UseQueryMode - A value that indicates whether the token filter is in query mode. When in query mode, the token filter generates bigrams and then removes common words and single terms followed by a common word. Default is false.
	UseQueryMode *bool `json:"queryMode,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) MarshalJSON() ([]byte, error) {
	cgtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter
	objectMap := make(map[string]interface{})
	if cgtf.CommonWords != nil {
		objectMap["commonWords"] = cgtf.CommonWords
	}
	if cgtf.IgnoreCase != nil {
		objectMap["ignoreCase"] = cgtf.IgnoreCase
	}
	if cgtf.UseQueryMode != nil {
		objectMap["queryMode"] = cgtf.UseQueryMode
	}
	if cgtf.Name != nil {
		objectMap["name"] = cgtf.Name
	}
	if cgtf.OdataType != "" {
		objectMap["@odata.type"] = cgtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return &cgtf, true
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &cgtf, true
}

// ConditionalSkill a skill that enables scenarios that require a Boolean operation to determine the data
// to assign to an output.
type ConditionalSkill struct {
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ConditionalSkill.
func (cs ConditionalSkill) MarshalJSON() ([]byte, error) {
	cs.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill
	objectMap := make(map[string]interface{})
	if cs.Name != nil {
		objectMap["name"] = cs.Name
	}
	if cs.Description != nil {
		objectMap["description"] = cs.Description
	}
	if cs.Context != nil {
		objectMap["context"] = cs.Context
	}
	if cs.Inputs != nil {
		objectMap["inputs"] = cs.Inputs
	}
	if cs.Outputs != nil {
		objectMap["outputs"] = cs.Outputs
	}
	if cs.OdataType != "" {
		objectMap["@odata.type"] = cs.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return &cs, true
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &cs, true
}

// CorsOptions defines options to control Cross-Origin Resource Sharing (CORS) for an index.
type CorsOptions struct {
	// AllowedOrigins - The list of origins from which JavaScript code will be granted access to your index. Can contain a list of hosts of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a single '*' to allow all origins (not recommended).
	AllowedOrigins *[]string `json:"allowedOrigins,omitempty"`
	// MaxAgeInSeconds - The duration for which browsers should cache CORS preflight responses. Defaults to 5 minutes.
	MaxAgeInSeconds *int64 `json:"maxAgeInSeconds,omitempty"`
}

// CustomAnalyzer allows you to take control over the process of converting text into indexable/searchable
// tokens. It's a user-defined configuration consisting of a single predefined tokenizer and one or more
// filters. The tokenizer is responsible for breaking text into tokens, and the filters for modifying
// tokens emitted by the tokenizer.
type CustomAnalyzer struct {
	// Tokenizer - The name of the tokenizer to use to divide continuous text into a sequence of tokens, such as breaking a sentence into words. Possible values include: 'LexicalTokenizerNameClassic', 'LexicalTokenizerNameEdgeNGram', 'LexicalTokenizerNameKeyword', 'LexicalTokenizerNameLetter', 'LexicalTokenizerNameLowercase', 'LexicalTokenizerNameMicrosoftLanguageTokenizer', 'LexicalTokenizerNameMicrosoftLanguageStemmingTokenizer', 'LexicalTokenizerNameNGram', 'LexicalTokenizerNamePathHierarchy', 'LexicalTokenizerNamePattern', 'LexicalTokenizerNameStandard', 'LexicalTokenizerNameUaxURLEmail', 'LexicalTokenizerNameWhitespace'
	Tokenizer LexicalTokenizerName `json:"tokenizer,omitempty"`
	// TokenFilters - A list of token filters used to filter out or modify the tokens generated by a tokenizer. For example, you can specify a lowercase filter that converts all characters to lowercase. The filters are run in the order in which they are listed.
	TokenFilters *[]TokenFilterName `json:"tokenFilters,omitempty"`
	// CharFilters - A list of character filters used to prepare input text before it is processed by the tokenizer. For instance, they can replace certain characters or symbols. The filters are run in the order in which they are listed.
	CharFilters *[]CharFilterName `json:"charFilters,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeLexicalAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CustomAnalyzer.
func (ca CustomAnalyzer) MarshalJSON() ([]byte, error) {
	ca.OdataType = OdataTypeMicrosoftAzureSearchCustomAnalyzer
	objectMap := make(map[string]interface{})
	if ca.Tokenizer != "" {
		objectMap["tokenizer"] = ca.Tokenizer
	}
	if ca.TokenFilters != nil {
		objectMap["tokenFilters"] = ca.TokenFilters
	}
	if ca.CharFilters != nil {
		objectMap["charFilters"] = ca.CharFilters
	}
	if ca.Name != nil {
		objectMap["name"] = ca.Name
	}
	if ca.OdataType != "" {
		objectMap["@odata.type"] = ca.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicLexicalAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return &ca, true
}

// AsPatternAnalyzer is the BasicLexicalAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsLuceneStandardAnalyzer is the BasicLexicalAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsLuceneStandardAnalyzer() (*LuceneStandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicLexicalAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsLexicalAnalyzer is the BasicLexicalAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsLexicalAnalyzer() (*LexicalAnalyzer, bool) {
	return nil, false
}

// AsBasicLexicalAnalyzer is the BasicLexicalAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsBasicLexicalAnalyzer() (BasicLexicalAnalyzer, bool) {
	return &ca, true
}

// CustomEntity an object that contains information about the matches that were found, and related
// metadata.
type CustomEntity struct {
	// Name - The top-level entity descriptor. Matches in the skill output will be grouped by this name, and it should represent the "normalized" form of the text being found.
	Name *string `json:"name,omitempty"`
	// Description - This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear with every match of its entity in the skill output.
	Description *string `json:"description,omitempty"`
	// Type - This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear with every match of its entity in the skill output.
	Type *string `json:"type,omitempty"`
	// Subtype - This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear with every match of its entity in the skill output.
	Subtype *string `json:"subtype,omitempty"`
	// ID - This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear with every match of its entity in the skill output.
	ID *string `json:"id,omitempty"`
	// CaseSensitive - Defaults to false. Boolean value denoting whether comparisons with the entity name should be sensitive to character casing. Sample case insensitive matches of "Microsoft" could be: microsoft, microSoft, MICROSOFT.
	CaseSensitive *bool `json:"caseSensitive,omitempty"`
	// AccentSensitive - Defaults to false. Boolean value denoting whether comparisons with the entity name should be sensitive to accent.
	AccentSensitive *bool `json:"accentSensitive,omitempty"`
	// FuzzyEditDistance - Defaults to 0. Maximum value of 5. Denotes the acceptable number of divergent characters that would still constitute a match with the entity name. The smallest possible fuzziness for any given match is returned. For instance, if the edit distance is set to 3, "Windows10" would still match "Windows", "Windows10" and "Windows 7". When case sensitivity is set to false, case differences do NOT count towards fuzziness tolerance, but otherwise do.
	FuzzyEditDistance *int32 `json:"fuzzyEditDistance,omitempty"`
	// DefaultCaseSensitive - Changes the default case sensitivity value for this entity. It be used to change the default value of all aliases caseSensitive values.
	DefaultCaseSensitive *bool `json:"defaultCaseSensitive,omitempty"`
	// DefaultAccentSensitive - Changes the default accent sensitivity value for this entity. It be used to change the default value of all aliases accentSensitive values.
	DefaultAccentSensitive *bool `json:"defaultAccentSensitive,omitempty"`
	// DefaultFuzzyEditDistance - Changes the default fuzzy edit distance value for this entity. It can be used to change the default value of all aliases fuzzyEditDistance values.
	DefaultFuzzyEditDistance *int32 `json:"defaultFuzzyEditDistance,omitempty"`
	// Aliases - An array of complex objects that can be used to specify alternative spellings or synonyms to the root entity name.
	Aliases *[]CustomEntityAlias `json:"aliases,omitempty"`
}

// CustomEntityAlias a complex object that can be used to specify alternative spellings or synonyms to the
// root entity name.
type CustomEntityAlias struct {
	// Text - The text of the alias.
	Text *string `json:"text,omitempty"`
	// CaseSensitive - Determine if the alias is case sensitive.
	CaseSensitive *bool `json:"caseSensitive,omitempty"`
	// AccentSensitive - Determine if the alias is accent sensitive.
	AccentSensitive *bool `json:"accentSensitive,omitempty"`
	// FuzzyEditDistance - Determine the fuzzy edit distance of the alias.
	FuzzyEditDistance *int32 `json:"fuzzyEditDistance,omitempty"`
}

// CustomEntityLookupSkill a skill looks for text from a custom, user-defined list of words and phrases.
type CustomEntityLookupSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`. Possible values include: 'CustomEntityLookupSkillLanguageDa', 'CustomEntityLookupSkillLanguageDe', 'CustomEntityLookupSkillLanguageEn', 'CustomEntityLookupSkillLanguageEs', 'CustomEntityLookupSkillLanguageFi', 'CustomEntityLookupSkillLanguageFr', 'CustomEntityLookupSkillLanguageIt', 'CustomEntityLookupSkillLanguageKo', 'CustomEntityLookupSkillLanguagePt'
	DefaultLanguageCode CustomEntityLookupSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// EntitiesDefinitionURI - Path to a JSON or CSV file containing all the target text to match against. This entity definition is read at the beginning of an indexer run. Any updates to this file during an indexer run will not take effect until subsequent runs. This config must be accessible over HTTPS.
	EntitiesDefinitionURI *string `json:"entitiesDefinitionUri,omitempty"`
	// InlineEntitiesDefinition - The inline CustomEntity definition.
	InlineEntitiesDefinition *[]CustomEntity `json:"inlineEntitiesDefinition,omitempty"`
	// GlobalDefaultCaseSensitive - A global flag for CaseSensitive. If CaseSensitive is not set in CustomEntity, this value will be the default value.
	GlobalDefaultCaseSensitive *bool `json:"globalDefaultCaseSensitive,omitempty"`
	// GlobalDefaultAccentSensitive - A global flag for AccentSensitive. If AccentSensitive is not set in CustomEntity, this value will be the default value.
	GlobalDefaultAccentSensitive *bool `json:"globalDefaultAccentSensitive,omitempty"`
	// GlobalDefaultFuzzyEditDistance - A global flag for FuzzyEditDistance. If FuzzyEditDistance is not set in CustomEntity, this value will be the default value.
	GlobalDefaultFuzzyEditDistance *int32 `json:"globalDefaultFuzzyEditDistance,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) MarshalJSON() ([]byte, error) {
	cels.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill
	objectMap := make(map[string]interface{})
	if cels.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = cels.DefaultLanguageCode
	}
	if cels.EntitiesDefinitionURI != nil {
		objectMap["entitiesDefinitionUri"] = cels.EntitiesDefinitionURI
	}
	if cels.InlineEntitiesDefinition != nil {
		objectMap["inlineEntitiesDefinition"] = cels.InlineEntitiesDefinition
	}
	if cels.GlobalDefaultCaseSensitive != nil {
		objectMap["globalDefaultCaseSensitive"] = cels.GlobalDefaultCaseSensitive
	}
	if cels.GlobalDefaultAccentSensitive != nil {
		objectMap["globalDefaultAccentSensitive"] = cels.GlobalDefaultAccentSensitive
	}
	if cels.GlobalDefaultFuzzyEditDistance != nil {
		objectMap["globalDefaultFuzzyEditDistance"] = cels.GlobalDefaultFuzzyEditDistance
	}
	if cels.Name != nil {
		objectMap["name"] = cels.Name
	}
	if cels.Description != nil {
		objectMap["description"] = cels.Description
	}
	if cels.Context != nil {
		objectMap["context"] = cels.Context
	}
	if cels.Inputs != nil {
		objectMap["inputs"] = cels.Inputs
	}
	if cels.Outputs != nil {
		objectMap["outputs"] = cels.Outputs
	}
	if cels.OdataType != "" {
		objectMap["@odata.type"] = cels.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return &cels, true
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for CustomEntityLookupSkill.
func (cels CustomEntityLookupSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &cels, true
}

// CustomNormalizer allows you to configure normalization for filterable, sortable, and facetable fields,
// which by default operate with strict matching. This is a user-defined configuration consisting of at
// least one or more filters, which modify the token that is stored.
type CustomNormalizer struct {
	// TokenFilters - A list of token filters used to filter out or modify the input token. For example, you can specify a lowercase filter that converts all characters to lowercase. The filters are run in the order in which they are listed.
	TokenFilters *[]TokenFilterName `json:"tokenFilters,omitempty"`
	// CharFilters - A list of character filters used to prepare input text before it is processed. For instance, they can replace certain characters or symbols. The filters are run in the order in which they are listed.
	CharFilters *[]CharFilterName `json:"charFilters,omitempty"`
	// Name - The name of the normalizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters. It cannot end in '.microsoft' nor '.lucene', nor be named 'asciifolding', 'standard', 'lowercase', 'uppercase', or 'elision'.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalNormalizerOdataTypeLexicalNormalizer', 'OdataTypeBasicLexicalNormalizerOdataTypeMicrosoftAzureSearchCustomNormalizer'
	OdataType OdataTypeBasicLexicalNormalizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CustomNormalizer.
func (cn CustomNormalizer) MarshalJSON() ([]byte, error) {
	cn.OdataType = OdataTypeBasicLexicalNormalizerOdataTypeMicrosoftAzureSearchCustomNormalizer
	objectMap := make(map[string]interface{})
	if cn.TokenFilters != nil {
		objectMap["tokenFilters"] = cn.TokenFilters
	}
	if cn.CharFilters != nil {
		objectMap["charFilters"] = cn.CharFilters
	}
	if cn.Name != nil {
		objectMap["name"] = cn.Name
	}
	if cn.OdataType != "" {
		objectMap["@odata.type"] = cn.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomNormalizer is the BasicLexicalNormalizer implementation for CustomNormalizer.
func (cn CustomNormalizer) AsCustomNormalizer() (*CustomNormalizer, bool) {
	return &cn, true
}

// AsLexicalNormalizer is the BasicLexicalNormalizer implementation for CustomNormalizer.
func (cn CustomNormalizer) AsLexicalNormalizer() (*LexicalNormalizer, bool) {
	return nil, false
}

// AsBasicLexicalNormalizer is the BasicLexicalNormalizer implementation for CustomNormalizer.
func (cn CustomNormalizer) AsBasicLexicalNormalizer() (BasicLexicalNormalizer, bool) {
	return &cn, true
}

// CustomVectorizer specifies a user-defined vectorizer for generating the vector embedding of a query
// string. Integration of an external vectorizer is achieved using the custom Web API interface of a
// skillset.
type CustomVectorizer struct {
	// CustomWebAPIParameters - Specifies the properties of the user-defined vectorizer.
	CustomWebAPIParameters *CustomWebAPIParameters `json:"customWebApiParameters,omitempty"`
	// Name - The name to associate with this particular vectorization method.
	Name *string `json:"name,omitempty"`
	// Kind - Possible values include: 'KindBasicVectorSearchVectorizerKindVectorSearchVectorizer', 'KindBasicVectorSearchVectorizerKindAzureOpenAI', 'KindBasicVectorSearchVectorizerKindCustomWebAPI'
	Kind KindBasicVectorSearchVectorizer `json:"kind,omitempty"`
}

// MarshalJSON is the custom marshaler for CustomVectorizer.
func (cv CustomVectorizer) MarshalJSON() ([]byte, error) {
	cv.Kind = KindBasicVectorSearchVectorizerKindCustomWebAPI
	objectMap := make(map[string]interface{})
	if cv.CustomWebAPIParameters != nil {
		objectMap["customWebApiParameters"] = cv.CustomWebAPIParameters
	}
	if cv.Name != nil {
		objectMap["name"] = cv.Name
	}
	if cv.Kind != "" {
		objectMap["kind"] = cv.Kind
	}
	return json.Marshal(objectMap)
}

// AsAzureOpenAIVectorizer is the BasicVectorSearchVectorizer implementation for CustomVectorizer.
func (cv CustomVectorizer) AsAzureOpenAIVectorizer() (*AzureOpenAIVectorizer, bool) {
	return nil, false
}

// AsCustomVectorizer is the BasicVectorSearchVectorizer implementation for CustomVectorizer.
func (cv CustomVectorizer) AsCustomVectorizer() (*CustomVectorizer, bool) {
	return &cv, true
}

// AsVectorSearchVectorizer is the BasicVectorSearchVectorizer implementation for CustomVectorizer.
func (cv CustomVectorizer) AsVectorSearchVectorizer() (*VectorSearchVectorizer, bool) {
	return nil, false
}

// AsBasicVectorSearchVectorizer is the BasicVectorSearchVectorizer implementation for CustomVectorizer.
func (cv CustomVectorizer) AsBasicVectorSearchVectorizer() (BasicVectorSearchVectorizer, bool) {
	return &cv, true
}

// CustomWebAPIParameters specifies the properties for connecting to a user-defined vectorizer.
type CustomWebAPIParameters struct {
	// URI - The URI of the Web API providing the vectorizer.
	URI *string `json:"uri,omitempty"`
	// HTTPHeaders - The headers required to make the HTTP request.
	HTTPHeaders map[string]*string `json:"httpHeaders"`
	// HTTPMethod - The method for the HTTP request.
	HTTPMethod *string `json:"httpMethod,omitempty"`
	// Timeout - The desired timeout for the request. Default is 30 seconds.
	Timeout *string `json:"timeout,omitempty"`
	// AuthResourceID - Applies to custom endpoints that connect to external code in an Azure function or some other application that provides the transformations. This value should be the application ID created for the function or app when it was registered with Azure Active Directory. When specified, the vectorization connects to the function or app using a managed ID (either system or user-assigned) of the search service and the access token of the function or app, using this value as the resource id for creating the scope of the access token.
	AuthResourceID *string `json:"authResourceId,omitempty"`
	// AuthIdentity - The user-assigned managed identity used for outbound connections. If an authResourceId is provided and it's not specified, the system-assigned managed identity is used. On updates to the indexer, if the identity is unspecified, the value remains unchanged. If set to "none", the value of this property is cleared.
	AuthIdentity BasicIndexerDataIdentity `json:"authIdentity,omitempty"`
}

// MarshalJSON is the custom marshaler for CustomWebAPIParameters.
func (cwap CustomWebAPIParameters) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if cwap.URI != nil {
		objectMap["uri"] = cwap.URI
	}
	if cwap.HTTPHeaders != nil {
		objectMap["httpHeaders"] = cwap.HTTPHeaders
	}
	if cwap.HTTPMethod != nil {
		objectMap["httpMethod"] = cwap.HTTPMethod
	}
	if cwap.Timeout != nil {
		objectMap["timeout"] = cwap.Timeout
	}
	if cwap.AuthResourceID != nil {
		objectMap["authResourceId"] = cwap.AuthResourceID
	}
	objectMap["authIdentity"] = cwap.AuthIdentity
	return json.Marshal(objectMap)
}

// UnmarshalJSON is the custom unmarshaler for CustomWebAPIParameters struct.
func (cwap *CustomWebAPIParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "uri":
			if v != nil {
				var URI string
				err = json.Unmarshal(*v, &URI)
				if err != nil {
					return err
				}
				cwap.URI = &URI
			}
		case "httpHeaders":
			if v != nil {
				var HTTPHeaders map[string]*string
				err = json.Unmarshal(*v, &HTTPHeaders)
				if err != nil {
					return err
				}
				cwap.HTTPHeaders = HTTPHeaders
			}
		case "httpMethod":
			if v != nil {
				var HTTPMethod string
				err = json.Unmarshal(*v, &HTTPMethod)
				if err != nil {
					return err
				}
				cwap.HTTPMethod = &HTTPMethod
			}
		case "timeout":
			if v != nil {
				var timeout string
				err = json.Unmarshal(*v, &timeout)
				if err != nil {
					return err
				}
				cwap.Timeout = &timeout
			}
		case "authResourceId":
			if v != nil {
				var authResourceID string
				err = json.Unmarshal(*v, &authResourceID)
				if err != nil {
					return err
				}
				cwap.AuthResourceID = &authResourceID
			}
		case "authIdentity":
			if v != nil {
				authIdentity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				cwap.AuthIdentity = authIdentity
			}
		}
	}

	return nil
}

// BasicDataChangeDetectionPolicy base type for data change detection policies.
type BasicDataChangeDetectionPolicy interface {
	AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool)
	AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool)
	AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool)
}

// DataChangeDetectionPolicy base type for data change detection policies.
type DataChangeDetectionPolicy struct {
	// OdataType - Possible values include: 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeDataChangeDetectionPolicy', 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy', 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy'
	OdataType OdataTypeBasicDataChangeDetectionPolicy `json:"@odata.type,omitempty"`
}

func unmarshalBasicDataChangeDetectionPolicy(body []byte) (BasicDataChangeDetectionPolicy, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy):
		var hwmcdp HighWaterMarkChangeDetectionPolicy
		err := json.Unmarshal(body, &hwmcdp)
		return hwmcdp, err
	case string(OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy):
		var sictp SQLIntegratedChangeTrackingPolicy
		err := json.Unmarshal(body, &sictp)
		return sictp, err
	default:
		var dcdp DataChangeDetectionPolicy
		err := json.Unmarshal(body, &dcdp)
		return dcdp, err
	}
}
func unmarshalBasicDataChangeDetectionPolicyArray(body []byte) ([]BasicDataChangeDetectionPolicy, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	dcdpArray := make([]BasicDataChangeDetectionPolicy, len(rawMessages))

	for index, rawMessage := range rawMessages {
		dcdp, err := unmarshalBasicDataChangeDetectionPolicy(*rawMessage)
		if err != nil {
			return nil, err
		}
		dcdpArray[index] = dcdp
	}
	return dcdpArray, nil
}

// MarshalJSON is the custom marshaler for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) MarshalJSON() ([]byte, error) {
	dcdp.OdataType = OdataTypeBasicDataChangeDetectionPolicyOdataTypeDataChangeDetectionPolicy
	objectMap := make(map[string]interface{})
	if dcdp.OdataType != "" {
		objectMap["@odata.type"] = dcdp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsHighWaterMarkChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool) {
	return nil, false
}

// AsSQLIntegratedChangeTrackingPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool) {
	return nil, false
}

// AsDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool) {
	return &dcdp, true
}

// AsBasicDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsBasicDataChangeDetectionPolicy() (BasicDataChangeDetectionPolicy, bool) {
	return &dcdp, true
}

// BasicDataDeletionDetectionPolicy base type for data deletion detection policies.
type BasicDataDeletionDetectionPolicy interface {
	AsSoftDeleteColumnDeletionDetectionPolicy() (*SoftDeleteColumnDeletionDetectionPolicy, bool)
	AsNativeBlobSoftDeleteDeletionDetectionPolicy() (*NativeBlobSoftDeleteDeletionDetectionPolicy, bool)
	AsDataDeletionDetectionPolicy() (*DataDeletionDetectionPolicy, bool)
}

// DataDeletionDetectionPolicy base type for data deletion detection policies.
type DataDeletionDetectionPolicy struct {
	// OdataType - Possible values include: 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeDataDeletionDetectionPolicy', 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy', 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchNativeBlobSoftDeleteDeletionDetectionPolicy'
	OdataType OdataTypeBasicDataDeletionDetectionPolicy `json:"@odata.type,omitempty"`
}

func unmarshalBasicDataDeletionDetectionPolicy(body []byte) (BasicDataDeletionDetectionPolicy, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy):
		var sdcddp SoftDeleteColumnDeletionDetectionPolicy
		err := json.Unmarshal(body, &sdcddp)
		return sdcddp, err
	case string(OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchNativeBlobSoftDeleteDeletionDetectionPolicy):
		var nbsdddp NativeBlobSoftDeleteDeletionDetectionPolicy
		err := json.Unmarshal(body, &nbsdddp)
		return nbsdddp, err
	default:
		var dddp DataDeletionDetectionPolicy
		err := json.Unmarshal(body, &dddp)
		return dddp, err
	}
}
func unmarshalBasicDataDeletionDetectionPolicyArray(body []byte) ([]BasicDataDeletionDetectionPolicy, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	dddpArray := make([]BasicDataDeletionDetectionPolicy, len(rawMessages))

	for index, rawMessage := range rawMessages {
		dddp, err := unmarshalBasicDataDeletionDetectionPolicy(*rawMessage)
		if err != nil {
			return nil, err
		}
		dddpArray[index] = dddp
	}
	return dddpArray, nil
}

// MarshalJSON is the custom marshaler for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) MarshalJSON() ([]byte, error) {
	dddp.OdataType = OdataTypeBasicDataDeletionDetectionPolicyOdataTypeDataDeletionDetectionPolicy
	objectMap := make(map[string]interface{})
	if dddp.OdataType != "" {
		objectMap["@odata.type"] = dddp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsSoftDeleteColumnDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) AsSoftDeleteColumnDeletionDetectionPolicy() (*SoftDeleteColumnDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsNativeBlobSoftDeleteDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) AsNativeBlobSoftDeleteDeletionDetectionPolicy() (*NativeBlobSoftDeleteDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) AsDataDeletionDetectionPolicy() (*DataDeletionDetectionPolicy, bool) {
	return &dddp, true
}

// AsBasicDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) AsBasicDataDeletionDetectionPolicy() (BasicDataDeletionDetectionPolicy, bool) {
	return &dddp, true
}

// DataSourceCredentials represents credentials that can be used to connect to a datasource.
type DataSourceCredentials struct {
	// ConnectionString - The connection string for the datasource. Set to `<unchanged>` (with brackets) if you don't want the connection string updated. Set to `<redacted>` if you want to remove the connection string value from the datasource.
	ConnectionString *string `json:"connectionString,omitempty"`
}

// DefaultCognitiveServicesAccount an empty object that represents the default Azure AI service resource
// for a skillset.
type DefaultCognitiveServicesAccount struct {
	// Description - Description of the Azure AI service resource attached to a skillset.
	Description *string `json:"description,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicCognitiveServicesAccountOdataTypeCognitiveServicesAccount', 'OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchDefaultCognitiveServices', 'OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchCognitiveServicesByKey'
	OdataType OdataTypeBasicCognitiveServicesAccount `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) MarshalJSON() ([]byte, error) {
	dcsa.OdataType = OdataTypeBasicCognitiveServicesAccountOdataTypeMicrosoftAzureSearchDefaultCognitiveServices
	objectMap := make(map[string]interface{})
	if dcsa.Description != nil {
		objectMap["description"] = dcsa.Description
	}
	if dcsa.OdataType != "" {
		objectMap["@odata.type"] = dcsa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsDefaultCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool) {
	return &dcsa, true
}

// AsCognitiveServicesAccountKey is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool) {
	return nil, false
}

// AsCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool) {
	return nil, false
}

// AsBasicCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsBasicCognitiveServicesAccount() (BasicCognitiveServicesAccount, bool) {
	return &dcsa, true
}

// DictionaryDecompounderTokenFilter decomposes compound words found in many Germanic languages. This token
// filter is implemented using Apache Lucene.
type DictionaryDecompounderTokenFilter struct {
	// WordList - The list of words to match against.
	WordList *[]string `json:"wordList,omitempty"`
	// MinWordSize - The minimum word size. Only words longer than this get processed. Default is 5. Maximum is 300.
	MinWordSize *int32 `json:"minWordSize,omitempty"`
	// MinSubwordSize - The minimum subword size. Only subwords longer than this are outputted. Default is 2. Maximum is 300.
	MinSubwordSize *int32 `json:"minSubwordSize,omitempty"`
	// MaxSubwordSize - The maximum subword size. Only subwords shorter than this are outputted. Default is 15. Maximum is 300.
	MaxSubwordSize *int32 `json:"maxSubwordSize,omitempty"`
	// OnlyLongestMatch - A value indicating whether to add only the longest matching subword to the output. Default is false.
	OnlyLongestMatch *bool `json:"onlyLongestMatch,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) MarshalJSON() ([]byte, error) {
	ddtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter
	objectMap := make(map[string]interface{})
	if ddtf.WordList != nil {
		objectMap["wordList"] = ddtf.WordList
	}
	if ddtf.MinWordSize != nil {
		objectMap["minWordSize"] = ddtf.MinWordSize
	}
	if ddtf.MinSubwordSize != nil {
		objectMap["minSubwordSize"] = ddtf.MinSubwordSize
	}
	if ddtf.MaxSubwordSize != nil {
		objectMap["maxSubwordSize"] = ddtf.MaxSubwordSize
	}
	if ddtf.OnlyLongestMatch != nil {
		objectMap["onlyLongestMatch"] = ddtf.OnlyLongestMatch
	}
	if ddtf.Name != nil {
		objectMap["name"] = ddtf.Name
	}
	if ddtf.OdataType != "" {
		objectMap["@odata.type"] = ddtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return &ddtf, true
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ddtf, true
}

// DistanceScoringFunction defines a function that boosts scores based on distance from a geographic
// location.
type DistanceScoringFunction struct {
	// Parameters - Parameter values for the distance scoring function.
	Parameters *DistanceScoringParameters `json:"distance,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'ScoringFunctionInterpolationLinear', 'ScoringFunctionInterpolationConstant', 'ScoringFunctionInterpolationQuadratic', 'ScoringFunctionInterpolationLogarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for DistanceScoringFunction.
func (dsf DistanceScoringFunction) MarshalJSON() ([]byte, error) {
	dsf.Type = TypeDistance
	objectMap := make(map[string]interface{})
	if dsf.Parameters != nil {
		objectMap["distance"] = dsf.Parameters
	}
	if dsf.FieldName != nil {
		objectMap["fieldName"] = dsf.FieldName
	}
	if dsf.Boost != nil {
		objectMap["boost"] = dsf.Boost
	}
	if dsf.Interpolation != "" {
		objectMap["interpolation"] = dsf.Interpolation
	}
	if dsf.Type != "" {
		objectMap["type"] = dsf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return &dsf, true
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &dsf, true
}

// DistanceScoringParameters provides parameter values to a distance scoring function.
type DistanceScoringParameters struct {
	// ReferencePointParameter - The name of the parameter passed in search queries to specify the reference location.
	ReferencePointParameter *string `json:"referencePointParameter,omitempty"`
	// BoostingDistance - The distance in kilometers from the reference location where the boosting range ends.
	BoostingDistance *float64 `json:"boostingDistance,omitempty"`
}

// DocumentExtractionSkill a skill that extracts content from a file within the enrichment pipeline.
type DocumentExtractionSkill struct {
	// ParsingMode - The parsingMode for the skill. Will be set to 'default' if not defined.
	ParsingMode *string `json:"parsingMode,omitempty"`
	// DataToExtract - The type of data to be extracted for the skill. Will be set to 'contentAndMetadata' if not defined.
	DataToExtract *string `json:"dataToExtract,omitempty"`
	// Configuration - A dictionary of configurations for the skill.
	Configuration interface{} `json:"configuration,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) MarshalJSON() ([]byte, error) {
	desVar.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill
	objectMap := make(map[string]interface{})
	if desVar.ParsingMode != nil {
		objectMap["parsingMode"] = desVar.ParsingMode
	}
	if desVar.DataToExtract != nil {
		objectMap["dataToExtract"] = desVar.DataToExtract
	}
	if desVar.Configuration != nil {
		objectMap["configuration"] = desVar.Configuration
	}
	if desVar.Name != nil {
		objectMap["name"] = desVar.Name
	}
	if desVar.Description != nil {
		objectMap["description"] = desVar.Description
	}
	if desVar.Context != nil {
		objectMap["context"] = desVar.Context
	}
	if desVar.Inputs != nil {
		objectMap["inputs"] = desVar.Inputs
	}
	if desVar.Outputs != nil {
		objectMap["outputs"] = desVar.Outputs
	}
	if desVar.OdataType != "" {
		objectMap["@odata.type"] = desVar.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return &desVar, true
}

// AsWebAPISkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for DocumentExtractionSkill.
func (desVar DocumentExtractionSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &desVar, true
}

// EdgeNGramTokenFilter generates n-grams of the given size(s) starting from the front or the back of an
// input token. This token filter is implemented using Apache Lucene.
type EdgeNGramTokenFilter struct {
	// MinGram - The minimum n-gram length. Default is 1. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Side - Specifies which side of the input the n-gram should be generated from. Default is "front". Possible values include: 'EdgeNGramTokenFilterSideFront', 'EdgeNGramTokenFilterSideBack'
	Side EdgeNGramTokenFilterSide `json:"side,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) MarshalJSON() ([]byte, error) {
	engtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter
	objectMap := make(map[string]interface{})
	if engtf.MinGram != nil {
		objectMap["minGram"] = engtf.MinGram
	}
	if engtf.MaxGram != nil {
		objectMap["maxGram"] = engtf.MaxGram
	}
	if engtf.Side != "" {
		objectMap["side"] = engtf.Side
	}
	if engtf.Name != nil {
		objectMap["name"] = engtf.Name
	}
	if engtf.OdataType != "" {
		objectMap["@odata.type"] = engtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return &engtf, true
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &engtf, true
}

// EdgeNGramTokenFilterV2 generates n-grams of the given size(s) starting from the front or the back of an
// input token. This token filter is implemented using Apache Lucene.
type EdgeNGramTokenFilterV2 struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Side - Specifies which side of the input the n-gram should be generated from. Default is "front". Possible values include: 'EdgeNGramTokenFilterSideFront', 'EdgeNGramTokenFilterSideBack'
	Side EdgeNGramTokenFilterSide `json:"side,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) MarshalJSON() ([]byte, error) {
	engtfv.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2
	objectMap := make(map[string]interface{})
	if engtfv.MinGram != nil {
		objectMap["minGram"] = engtfv.MinGram
	}
	if engtfv.MaxGram != nil {
		objectMap["maxGram"] = engtfv.MaxGram
	}
	if engtfv.Side != "" {
		objectMap["side"] = engtfv.Side
	}
	if engtfv.Name != nil {
		objectMap["name"] = engtfv.Name
	}
	if engtfv.OdataType != "" {
		objectMap["@odata.type"] = engtfv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return &engtfv, true
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &engtfv, true
}

// EdgeNGramTokenizer tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is
// implemented using Apache Lucene.
type EdgeNGramTokenizer struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// TokenChars - Character classes to keep in the tokens.
	TokenChars *[]TokenCharacterKind `json:"tokenChars,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) MarshalJSON() ([]byte, error) {
	engt.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer
	objectMap := make(map[string]interface{})
	if engt.MinGram != nil {
		objectMap["minGram"] = engt.MinGram
	}
	if engt.MaxGram != nil {
		objectMap["maxGram"] = engt.MaxGram
	}
	if engt.TokenChars != nil {
		objectMap["tokenChars"] = engt.TokenChars
	}
	if engt.Name != nil {
		objectMap["name"] = engt.Name
	}
	if engt.OdataType != "" {
		objectMap["@odata.type"] = engt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return &engt, true
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &engt, true
}

// ElisionTokenFilter removes elisions. For example, "l'avion" (the plane) will be converted to "avion"
// (plane). This token filter is implemented using Apache Lucene.
type ElisionTokenFilter struct {
	// Articles - The set of articles to remove.
	Articles *[]string `json:"articles,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ElisionTokenFilter.
func (etf ElisionTokenFilter) MarshalJSON() ([]byte, error) {
	etf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter
	objectMap := make(map[string]interface{})
	if etf.Articles != nil {
		objectMap["articles"] = etf.Articles
	}
	if etf.Name != nil {
		objectMap["name"] = etf.Name
	}
	if etf.OdataType != "" {
		objectMap["@odata.type"] = etf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return &etf, true
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &etf, true
}

// EntityLinkingSkill using the Text Analytics API, extracts linked entities from text.
type EntityLinkingSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`.
	DefaultLanguageCode *string `json:"defaultLanguageCode,omitempty"`
	// MinimumPrecision - A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified. If not set (default), or if explicitly set to null, all entities will be included.
	MinimumPrecision *float64 `json:"minimumPrecision,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics service. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EntityLinkingSkill.
func (els EntityLinkingSkill) MarshalJSON() ([]byte, error) {
	els.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill
	objectMap := make(map[string]interface{})
	if els.DefaultLanguageCode != nil {
		objectMap["defaultLanguageCode"] = els.DefaultLanguageCode
	}
	if els.MinimumPrecision != nil {
		objectMap["minimumPrecision"] = els.MinimumPrecision
	}
	if els.ModelVersion != nil {
		objectMap["modelVersion"] = els.ModelVersion
	}
	if els.Name != nil {
		objectMap["name"] = els.Name
	}
	if els.Description != nil {
		objectMap["description"] = els.Description
	}
	if els.Context != nil {
		objectMap["context"] = els.Context
	}
	if els.Inputs != nil {
		objectMap["inputs"] = els.Inputs
	}
	if els.Outputs != nil {
		objectMap["outputs"] = els.Outputs
	}
	if els.OdataType != "" {
		objectMap["@odata.type"] = els.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return &els, true
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for EntityLinkingSkill.
func (els EntityLinkingSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &els, true
}

// EntityRecognitionSkill this skill is deprecated. Use the V3.EntityRecognitionSkill instead.
type EntityRecognitionSkill struct {
	// Categories - A list of entity categories that should be extracted.
	Categories *[]EntityCategory `json:"categories,omitempty"`
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`. Possible values include: 'EntityRecognitionSkillLanguageAr', 'EntityRecognitionSkillLanguageCs', 'EntityRecognitionSkillLanguageZhHans', 'EntityRecognitionSkillLanguageZhHant', 'EntityRecognitionSkillLanguageDa', 'EntityRecognitionSkillLanguageNl', 'EntityRecognitionSkillLanguageEn', 'EntityRecognitionSkillLanguageFi', 'EntityRecognitionSkillLanguageFr', 'EntityRecognitionSkillLanguageDe', 'EntityRecognitionSkillLanguageEl', 'EntityRecognitionSkillLanguageHu', 'EntityRecognitionSkillLanguageIt', 'EntityRecognitionSkillLanguageJa', 'EntityRecognitionSkillLanguageKo', 'EntityRecognitionSkillLanguageNo', 'EntityRecognitionSkillLanguagePl', 'EntityRecognitionSkillLanguagePtPT', 'EntityRecognitionSkillLanguagePtBR', 'EntityRecognitionSkillLanguageRu', 'EntityRecognitionSkillLanguageEs', 'EntityRecognitionSkillLanguageSv', 'EntityRecognitionSkillLanguageTr'
	DefaultLanguageCode EntityRecognitionSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// IncludeTypelessEntities - Determines whether or not to include entities which are well known but don't conform to a pre-defined type. If this configuration is not set (default), set to null or set to false, entities which don't conform to one of the pre-defined types will not be surfaced.
	IncludeTypelessEntities *bool `json:"includeTypelessEntities,omitempty"`
	// MinimumPrecision - A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified. If not set (default), or if explicitly set to null, all entities will be included.
	MinimumPrecision *float64 `json:"minimumPrecision,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) MarshalJSON() ([]byte, error) {
	ers.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill
	objectMap := make(map[string]interface{})
	if ers.Categories != nil {
		objectMap["categories"] = ers.Categories
	}
	if ers.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ers.DefaultLanguageCode
	}
	if ers.IncludeTypelessEntities != nil {
		objectMap["includeTypelessEntities"] = ers.IncludeTypelessEntities
	}
	if ers.MinimumPrecision != nil {
		objectMap["minimumPrecision"] = ers.MinimumPrecision
	}
	if ers.Name != nil {
		objectMap["name"] = ers.Name
	}
	if ers.Description != nil {
		objectMap["description"] = ers.Description
	}
	if ers.Context != nil {
		objectMap["context"] = ers.Context
	}
	if ers.Inputs != nil {
		objectMap["inputs"] = ers.Inputs
	}
	if ers.Outputs != nil {
		objectMap["outputs"] = ers.Outputs
	}
	if ers.OdataType != "" {
		objectMap["@odata.type"] = ers.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return &ers, true
}

// AsSentimentSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ers, true
}

// EntityRecognitionSkillV3 using the Text Analytics API, extracts entities of different types from text.
type EntityRecognitionSkillV3 struct {
	// Categories - A list of entity categories that should be extracted.
	Categories *[]string `json:"categories,omitempty"`
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`.
	DefaultLanguageCode *string `json:"defaultLanguageCode,omitempty"`
	// MinimumPrecision - A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified. If not set (default), or if explicitly set to null, all entities will be included.
	MinimumPrecision *float64 `json:"minimumPrecision,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics API. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) MarshalJSON() ([]byte, error) {
	ersv.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill
	objectMap := make(map[string]interface{})
	if ersv.Categories != nil {
		objectMap["categories"] = ersv.Categories
	}
	if ersv.DefaultLanguageCode != nil {
		objectMap["defaultLanguageCode"] = ersv.DefaultLanguageCode
	}
	if ersv.MinimumPrecision != nil {
		objectMap["minimumPrecision"] = ersv.MinimumPrecision
	}
	if ersv.ModelVersion != nil {
		objectMap["modelVersion"] = ersv.ModelVersion
	}
	if ersv.Name != nil {
		objectMap["name"] = ersv.Name
	}
	if ersv.Description != nil {
		objectMap["description"] = ersv.Description
	}
	if ersv.Context != nil {
		objectMap["context"] = ersv.Context
	}
	if ersv.Inputs != nil {
		objectMap["inputs"] = ersv.Inputs
	}
	if ersv.Outputs != nil {
		objectMap["outputs"] = ersv.Outputs
	}
	if ersv.OdataType != "" {
		objectMap["@odata.type"] = ersv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return &ersv, true
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for EntityRecognitionSkillV3.
func (ersv EntityRecognitionSkillV3) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ersv, true
}

// ErrorAdditionalInfo the resource management error additional info.
type ErrorAdditionalInfo struct {
	// Type - READ-ONLY; The additional info type.
	Type *string `json:"type,omitempty"`
	// Info - READ-ONLY; The additional info.
	Info interface{} `json:"info,omitempty"`
}

// MarshalJSON is the custom marshaler for ErrorAdditionalInfo.
func (eai ErrorAdditionalInfo) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ErrorDetail the error detail.
type ErrorDetail struct {
	// Code - READ-ONLY; The error code.
	Code *string `json:"code,omitempty"`
	// Message - READ-ONLY; The error message.
	Message *string `json:"message,omitempty"`
	// Target - READ-ONLY; The error target.
	Target *string `json:"target,omitempty"`
	// Details - READ-ONLY; The error details.
	Details *[]ErrorDetail `json:"details,omitempty"`
	// AdditionalInfo - READ-ONLY; The error additional info.
	AdditionalInfo *[]ErrorAdditionalInfo `json:"additionalInfo,omitempty"`
}

// MarshalJSON is the custom marshaler for ErrorDetail.
func (ed ErrorDetail) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ErrorResponse common error response for all Azure Resource Manager APIs to return error details for
// failed operations. (This also follows the OData error response format.).
type ErrorResponse struct {
	// Error - The error object.
	Error *ErrorDetail `json:"error,omitempty"`
}

// ExhaustiveKnnParameters contains the parameters specific to exhaustive KNN algorithm.
type ExhaustiveKnnParameters struct {
	// Metric - The similarity metric to use for vector comparisons. Possible values include: 'VectorSearchAlgorithmMetricCosine', 'VectorSearchAlgorithmMetricEuclidean', 'VectorSearchAlgorithmMetricDotProduct'
	Metric VectorSearchAlgorithmMetric `json:"metric,omitempty"`
}

// ExhaustiveKnnVectorSearchAlgorithmConfiguration contains configuration options specific to the
// exhaustive KNN algorithm used during querying, which will perform brute-force search across the entire
// vector index.
type ExhaustiveKnnVectorSearchAlgorithmConfiguration struct {
	// Parameters - Contains the parameters specific to exhaustive KNN algorithm.
	Parameters *ExhaustiveKnnParameters `json:"exhaustiveKnnParameters,omitempty"`
	// Name - The name to associate with this particular configuration.
	Name *string `json:"name,omitempty"`
	// Kind - Possible values include: 'KindVectorSearchAlgorithmConfiguration', 'KindHnsw', 'KindExhaustiveKnn'
	Kind Kind `json:"kind,omitempty"`
}

// MarshalJSON is the custom marshaler for ExhaustiveKnnVectorSearchAlgorithmConfiguration.
func (ekvsac ExhaustiveKnnVectorSearchAlgorithmConfiguration) MarshalJSON() ([]byte, error) {
	ekvsac.Kind = KindExhaustiveKnn
	objectMap := make(map[string]interface{})
	if ekvsac.Parameters != nil {
		objectMap["exhaustiveKnnParameters"] = ekvsac.Parameters
	}
	if ekvsac.Name != nil {
		objectMap["name"] = ekvsac.Name
	}
	if ekvsac.Kind != "" {
		objectMap["kind"] = ekvsac.Kind
	}
	return json.Marshal(objectMap)
}

// AsHnswVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for ExhaustiveKnnVectorSearchAlgorithmConfiguration.
func (ekvsac ExhaustiveKnnVectorSearchAlgorithmConfiguration) AsHnswVectorSearchAlgorithmConfiguration() (*HnswVectorSearchAlgorithmConfiguration, bool) {
	return nil, false
}

// AsExhaustiveKnnVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for ExhaustiveKnnVectorSearchAlgorithmConfiguration.
func (ekvsac ExhaustiveKnnVectorSearchAlgorithmConfiguration) AsExhaustiveKnnVectorSearchAlgorithmConfiguration() (*ExhaustiveKnnVectorSearchAlgorithmConfiguration, bool) {
	return &ekvsac, true
}

// AsVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for ExhaustiveKnnVectorSearchAlgorithmConfiguration.
func (ekvsac ExhaustiveKnnVectorSearchAlgorithmConfiguration) AsVectorSearchAlgorithmConfiguration() (*VectorSearchAlgorithmConfiguration, bool) {
	return nil, false
}

// AsBasicVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for ExhaustiveKnnVectorSearchAlgorithmConfiguration.
func (ekvsac ExhaustiveKnnVectorSearchAlgorithmConfiguration) AsBasicVectorSearchAlgorithmConfiguration() (BasicVectorSearchAlgorithmConfiguration, bool) {
	return &ekvsac, true
}

// Field represents a field in an index definition, which describes the name, data type, and search
// behavior of a field.
type Field struct {
	// Name - The name of the field, which must be unique within the fields collection of the index or parent field.
	Name *string `json:"name,omitempty"`
	// Type - The data type of the field. Possible values include: 'FieldDataTypeString', 'FieldDataTypeInt32', 'FieldDataTypeInt64', 'FieldDataTypeDouble', 'FieldDataTypeBoolean', 'FieldDataTypeDateTimeOffset', 'FieldDataTypeGeographyPoint', 'FieldDataTypeComplex', 'FieldDataTypeSingle', 'FieldDataTypeHalf', 'FieldDataTypeInt16', 'FieldDataTypeSByte'
	Type FieldDataType `json:"type,omitempty"`
	// Key - A value indicating whether the field uniquely identifies documents in the index. Exactly one top-level field in each index must be chosen as the key field and it must be of type Edm.String. Key fields can be used to look up documents directly and update or delete specific documents. Default is false for simple fields and null for complex fields.
	Key *bool `json:"key,omitempty"`
	// Retrievable - A value indicating whether the field can be returned in a search result. You can disable this option if you want to use a field (for example, margin) as a filter, sorting, or scoring mechanism but do not want the field to be visible to the end user. This property must be true for key fields, and it must be null for complex fields. This property can be changed on existing fields. Enabling this property does not cause any increase in index storage requirements. Default is true for simple fields, false for vector fields, and null for complex fields.
	Retrievable *bool `json:"retrievable,omitempty"`
	// Stored - An immutable value indicating whether the field will be persisted separately on disk to be returned in a search result. You can disable this option if you don't plan to return the field contents in a search response to save on storage overhead. This can only be set during index creation and only for vector fields. This property cannot be changed for existing fields or set as false for new fields. If this property is set as false, the property 'retrievable' must also be set to false. This property must be true or unset for key fields, for new fields, and for non-vector fields, and it must be null for complex fields. Disabling this property will reduce index storage requirements. The default is true for vector fields.
	Stored *bool `json:"stored,omitempty"`
	// Searchable - A value indicating whether the field is full-text searchable. This means it will undergo analysis such as word-breaking during indexing. If you set a searchable field to a value like "sunny day", internally it will be split into the individual tokens "sunny" and "day". This enables full-text searches for these terms. Fields of type Edm.String or Collection(Edm.String) are searchable by default. This property must be false for simple fields of other non-string data types, and it must be null for complex fields. Note: searchable fields consume extra space in your index to accommodate additional tokenized versions of the field value for full-text searches. If you want to save space in your index and you don't need a field to be included in searches, set searchable to false.
	Searchable *bool `json:"searchable,omitempty"`
	// Filterable - A value indicating whether to enable the field to be referenced in $filter queries. filterable differs from searchable in how strings are handled. Fields of type Edm.String or Collection(Edm.String) that are filterable do not undergo word-breaking, so comparisons are for exact matches only. For example, if you set such a field f to "sunny day", $filter=f eq 'sunny' will find no matches, but $filter=f eq 'sunny day' will. This property must be null for complex fields. Default is true for simple fields and null for complex fields.
	Filterable *bool `json:"filterable,omitempty"`
	// Sortable - A value indicating whether to enable the field to be referenced in $orderby expressions. By default, the search engine sorts results by score, but in many experiences users will want to sort by fields in the documents. A simple field can be sortable only if it is single-valued (it has a single value in the scope of the parent document). Simple collection fields cannot be sortable, since they are multi-valued. Simple sub-fields of complex collections are also multi-valued, and therefore cannot be sortable. This is true whether it's an immediate parent field, or an ancestor field, that's the complex collection. Complex fields cannot be sortable and the sortable property must be null for such fields. The default for sortable is true for single-valued simple fields, false for multi-valued simple fields, and null for complex fields.
	Sortable *bool `json:"sortable,omitempty"`
	// Facetable - A value indicating whether to enable the field to be referenced in facet queries. Typically used in a presentation of search results that includes hit count by category (for example, search for digital cameras and see hits by brand, by megapixels, by price, and so on). This property must be null for complex fields. Fields of type Edm.GeographyPoint or Collection(Edm.GeographyPoint) cannot be facetable. Default is true for all other simple fields.
	Facetable *bool `json:"facetable,omitempty"`
	// Analyzer - The name of the analyzer to use for the field. This option can be used only with searchable fields and it can't be set together with either searchAnalyzer or indexAnalyzer. Once the analyzer is chosen, it cannot be changed for the field. Must be null for complex fields. Possible values include: 'LexicalAnalyzerNameArMicrosoft', 'LexicalAnalyzerNameArLucene', 'LexicalAnalyzerNameHyLucene', 'LexicalAnalyzerNameBnMicrosoft', 'LexicalAnalyzerNameEuLucene', 'LexicalAnalyzerNameBgMicrosoft', 'LexicalAnalyzerNameBgLucene', 'LexicalAnalyzerNameCaMicrosoft', 'LexicalAnalyzerNameCaLucene', 'LexicalAnalyzerNameZhHansMicrosoft', 'LexicalAnalyzerNameZhHansLucene', 'LexicalAnalyzerNameZhHantMicrosoft', 'LexicalAnalyzerNameZhHantLucene', 'LexicalAnalyzerNameHrMicrosoft', 'LexicalAnalyzerNameCsMicrosoft', 'LexicalAnalyzerNameCsLucene', 'LexicalAnalyzerNameDaMicrosoft', 'LexicalAnalyzerNameDaLucene', 'LexicalAnalyzerNameNlMicrosoft', 'LexicalAnalyzerNameNlLucene', 'LexicalAnalyzerNameEnMicrosoft', 'LexicalAnalyzerNameEnLucene', 'LexicalAnalyzerNameEtMicrosoft', 'LexicalAnalyzerNameFiMicrosoft', 'LexicalAnalyzerNameFiLucene', 'LexicalAnalyzerNameFrMicrosoft', 'LexicalAnalyzerNameFrLucene', 'LexicalAnalyzerNameGlLucene', 'LexicalAnalyzerNameDeMicrosoft', 'LexicalAnalyzerNameDeLucene', 'LexicalAnalyzerNameElMicrosoft', 'LexicalAnalyzerNameElLucene', 'LexicalAnalyzerNameGuMicrosoft', 'LexicalAnalyzerNameHeMicrosoft', 'LexicalAnalyzerNameHiMicrosoft', 'LexicalAnalyzerNameHiLucene', 'LexicalAnalyzerNameHuMicrosoft', 'LexicalAnalyzerNameHuLucene', 'LexicalAnalyzerNameIsMicrosoft', 'LexicalAnalyzerNameIDMicrosoft', 'LexicalAnalyzerNameIDLucene', 'LexicalAnalyzerNameGaLucene', 'LexicalAnalyzerNameItMicrosoft', 'LexicalAnalyzerNameItLucene', 'LexicalAnalyzerNameJaMicrosoft', 'LexicalAnalyzerNameJaLucene', 'LexicalAnalyzerNameKnMicrosoft', 'LexicalAnalyzerNameKoMicrosoft', 'LexicalAnalyzerNameKoLucene', 'LexicalAnalyzerNameLvMicrosoft', 'LexicalAnalyzerNameLvLucene', 'LexicalAnalyzerNameLtMicrosoft', 'LexicalAnalyzerNameMlMicrosoft', 'LexicalAnalyzerNameMsMicrosoft', 'LexicalAnalyzerNameMrMicrosoft', 'LexicalAnalyzerNameNbMicrosoft', 'LexicalAnalyzerNameNoLucene', 'LexicalAnalyzerNameFaLucene', 'LexicalAnalyzerNamePlMicrosoft', 'LexicalAnalyzerNamePlLucene', 'LexicalAnalyzerNamePtBrMicrosoft', 'LexicalAnalyzerNamePtBrLucene', 'LexicalAnalyzerNamePtPtMicrosoft', 'LexicalAnalyzerNamePtPtLucene', 'LexicalAnalyzerNamePaMicrosoft', 'LexicalAnalyzerNameRoMicrosoft', 'LexicalAnalyzerNameRoLucene', 'LexicalAnalyzerNameRuMicrosoft', 'LexicalAnalyzerNameRuLucene', 'LexicalAnalyzerNameSrCyrillicMicrosoft', 'LexicalAnalyzerNameSrLatinMicrosoft', 'LexicalAnalyzerNameSkMicrosoft', 'LexicalAnalyzerNameSlMicrosoft', 'LexicalAnalyzerNameEsMicrosoft', 'LexicalAnalyzerNameEsLucene', 'LexicalAnalyzerNameSvMicrosoft', 'LexicalAnalyzerNameSvLucene', 'LexicalAnalyzerNameTaMicrosoft', 'LexicalAnalyzerNameTeMicrosoft', 'LexicalAnalyzerNameThMicrosoft', 'LexicalAnalyzerNameThLucene', 'LexicalAnalyzerNameTrMicrosoft', 'LexicalAnalyzerNameTrLucene', 'LexicalAnalyzerNameUkMicrosoft', 'LexicalAnalyzerNameUrMicrosoft', 'LexicalAnalyzerNameViMicrosoft', 'LexicalAnalyzerNameStandardLucene', 'LexicalAnalyzerNameStandardASCIIFoldingLucene', 'LexicalAnalyzerNameKeyword', 'LexicalAnalyzerNamePattern', 'LexicalAnalyzerNameSimple', 'LexicalAnalyzerNameStop', 'LexicalAnalyzerNameWhitespace'
	Analyzer LexicalAnalyzerName `json:"analyzer,omitempty"`
	// SearchAnalyzer - The name of the analyzer used at search time for the field. This option can be used only with searchable fields. It must be set together with indexAnalyzer and it cannot be set together with the analyzer option. This property cannot be set to the name of a language analyzer; use the analyzer property instead if you need a language analyzer. This analyzer can be updated on an existing field. Must be null for complex fields. Possible values include: 'LexicalAnalyzerNameArMicrosoft', 'LexicalAnalyzerNameArLucene', 'LexicalAnalyzerNameHyLucene', 'LexicalAnalyzerNameBnMicrosoft', 'LexicalAnalyzerNameEuLucene', 'LexicalAnalyzerNameBgMicrosoft', 'LexicalAnalyzerNameBgLucene', 'LexicalAnalyzerNameCaMicrosoft', 'LexicalAnalyzerNameCaLucene', 'LexicalAnalyzerNameZhHansMicrosoft', 'LexicalAnalyzerNameZhHansLucene', 'LexicalAnalyzerNameZhHantMicrosoft', 'LexicalAnalyzerNameZhHantLucene', 'LexicalAnalyzerNameHrMicrosoft', 'LexicalAnalyzerNameCsMicrosoft', 'LexicalAnalyzerNameCsLucene', 'LexicalAnalyzerNameDaMicrosoft', 'LexicalAnalyzerNameDaLucene', 'LexicalAnalyzerNameNlMicrosoft', 'LexicalAnalyzerNameNlLucene', 'LexicalAnalyzerNameEnMicrosoft', 'LexicalAnalyzerNameEnLucene', 'LexicalAnalyzerNameEtMicrosoft', 'LexicalAnalyzerNameFiMicrosoft', 'LexicalAnalyzerNameFiLucene', 'LexicalAnalyzerNameFrMicrosoft', 'LexicalAnalyzerNameFrLucene', 'LexicalAnalyzerNameGlLucene', 'LexicalAnalyzerNameDeMicrosoft', 'LexicalAnalyzerNameDeLucene', 'LexicalAnalyzerNameElMicrosoft', 'LexicalAnalyzerNameElLucene', 'LexicalAnalyzerNameGuMicrosoft', 'LexicalAnalyzerNameHeMicrosoft', 'LexicalAnalyzerNameHiMicrosoft', 'LexicalAnalyzerNameHiLucene', 'LexicalAnalyzerNameHuMicrosoft', 'LexicalAnalyzerNameHuLucene', 'LexicalAnalyzerNameIsMicrosoft', 'LexicalAnalyzerNameIDMicrosoft', 'LexicalAnalyzerNameIDLucene', 'LexicalAnalyzerNameGaLucene', 'LexicalAnalyzerNameItMicrosoft', 'LexicalAnalyzerNameItLucene', 'LexicalAnalyzerNameJaMicrosoft', 'LexicalAnalyzerNameJaLucene', 'LexicalAnalyzerNameKnMicrosoft', 'LexicalAnalyzerNameKoMicrosoft', 'LexicalAnalyzerNameKoLucene', 'LexicalAnalyzerNameLvMicrosoft', 'LexicalAnalyzerNameLvLucene', 'LexicalAnalyzerNameLtMicrosoft', 'LexicalAnalyzerNameMlMicrosoft', 'LexicalAnalyzerNameMsMicrosoft', 'LexicalAnalyzerNameMrMicrosoft', 'LexicalAnalyzerNameNbMicrosoft', 'LexicalAnalyzerNameNoLucene', 'LexicalAnalyzerNameFaLucene', 'LexicalAnalyzerNamePlMicrosoft', 'LexicalAnalyzerNamePlLucene', 'LexicalAnalyzerNamePtBrMicrosoft', 'LexicalAnalyzerNamePtBrLucene', 'LexicalAnalyzerNamePtPtMicrosoft', 'LexicalAnalyzerNamePtPtLucene', 'LexicalAnalyzerNamePaMicrosoft', 'LexicalAnalyzerNameRoMicrosoft', 'LexicalAnalyzerNameRoLucene', 'LexicalAnalyzerNameRuMicrosoft', 'LexicalAnalyzerNameRuLucene', 'LexicalAnalyzerNameSrCyrillicMicrosoft', 'LexicalAnalyzerNameSrLatinMicrosoft', 'LexicalAnalyzerNameSkMicrosoft', 'LexicalAnalyzerNameSlMicrosoft', 'LexicalAnalyzerNameEsMicrosoft', 'LexicalAnalyzerNameEsLucene', 'LexicalAnalyzerNameSvMicrosoft', 'LexicalAnalyzerNameSvLucene', 'LexicalAnalyzerNameTaMicrosoft', 'LexicalAnalyzerNameTeMicrosoft', 'LexicalAnalyzerNameThMicrosoft', 'LexicalAnalyzerNameThLucene', 'LexicalAnalyzerNameTrMicrosoft', 'LexicalAnalyzerNameTrLucene', 'LexicalAnalyzerNameUkMicrosoft', 'LexicalAnalyzerNameUrMicrosoft', 'LexicalAnalyzerNameViMicrosoft', 'LexicalAnalyzerNameStandardLucene', 'LexicalAnalyzerNameStandardASCIIFoldingLucene', 'LexicalAnalyzerNameKeyword', 'LexicalAnalyzerNamePattern', 'LexicalAnalyzerNameSimple', 'LexicalAnalyzerNameStop', 'LexicalAnalyzerNameWhitespace'
	SearchAnalyzer LexicalAnalyzerName `json:"searchAnalyzer,omitempty"`
	// IndexAnalyzer - The name of the analyzer used at indexing time for the field. This option can be used only with searchable fields. It must be set together with searchAnalyzer and it cannot be set together with the analyzer option.  This property cannot be set to the name of a language analyzer; use the analyzer property instead if you need a language analyzer. Once the analyzer is chosen, it cannot be changed for the field. Must be null for complex fields. Possible values include: 'LexicalAnalyzerNameArMicrosoft', 'LexicalAnalyzerNameArLucene', 'LexicalAnalyzerNameHyLucene', 'LexicalAnalyzerNameBnMicrosoft', 'LexicalAnalyzerNameEuLucene', 'LexicalAnalyzerNameBgMicrosoft', 'LexicalAnalyzerNameBgLucene', 'LexicalAnalyzerNameCaMicrosoft', 'LexicalAnalyzerNameCaLucene', 'LexicalAnalyzerNameZhHansMicrosoft', 'LexicalAnalyzerNameZhHansLucene', 'LexicalAnalyzerNameZhHantMicrosoft', 'LexicalAnalyzerNameZhHantLucene', 'LexicalAnalyzerNameHrMicrosoft', 'LexicalAnalyzerNameCsMicrosoft', 'LexicalAnalyzerNameCsLucene', 'LexicalAnalyzerNameDaMicrosoft', 'LexicalAnalyzerNameDaLucene', 'LexicalAnalyzerNameNlMicrosoft', 'LexicalAnalyzerNameNlLucene', 'LexicalAnalyzerNameEnMicrosoft', 'LexicalAnalyzerNameEnLucene', 'LexicalAnalyzerNameEtMicrosoft', 'LexicalAnalyzerNameFiMicrosoft', 'LexicalAnalyzerNameFiLucene', 'LexicalAnalyzerNameFrMicrosoft', 'LexicalAnalyzerNameFrLucene', 'LexicalAnalyzerNameGlLucene', 'LexicalAnalyzerNameDeMicrosoft', 'LexicalAnalyzerNameDeLucene', 'LexicalAnalyzerNameElMicrosoft', 'LexicalAnalyzerNameElLucene', 'LexicalAnalyzerNameGuMicrosoft', 'LexicalAnalyzerNameHeMicrosoft', 'LexicalAnalyzerNameHiMicrosoft', 'LexicalAnalyzerNameHiLucene', 'LexicalAnalyzerNameHuMicrosoft', 'LexicalAnalyzerNameHuLucene', 'LexicalAnalyzerNameIsMicrosoft', 'LexicalAnalyzerNameIDMicrosoft', 'LexicalAnalyzerNameIDLucene', 'LexicalAnalyzerNameGaLucene', 'LexicalAnalyzerNameItMicrosoft', 'LexicalAnalyzerNameItLucene', 'LexicalAnalyzerNameJaMicrosoft', 'LexicalAnalyzerNameJaLucene', 'LexicalAnalyzerNameKnMicrosoft', 'LexicalAnalyzerNameKoMicrosoft', 'LexicalAnalyzerNameKoLucene', 'LexicalAnalyzerNameLvMicrosoft', 'LexicalAnalyzerNameLvLucene', 'LexicalAnalyzerNameLtMicrosoft', 'LexicalAnalyzerNameMlMicrosoft', 'LexicalAnalyzerNameMsMicrosoft', 'LexicalAnalyzerNameMrMicrosoft', 'LexicalAnalyzerNameNbMicrosoft', 'LexicalAnalyzerNameNoLucene', 'LexicalAnalyzerNameFaLucene', 'LexicalAnalyzerNamePlMicrosoft', 'LexicalAnalyzerNamePlLucene', 'LexicalAnalyzerNamePtBrMicrosoft', 'LexicalAnalyzerNamePtBrLucene', 'LexicalAnalyzerNamePtPtMicrosoft', 'LexicalAnalyzerNamePtPtLucene', 'LexicalAnalyzerNamePaMicrosoft', 'LexicalAnalyzerNameRoMicrosoft', 'LexicalAnalyzerNameRoLucene', 'LexicalAnalyzerNameRuMicrosoft', 'LexicalAnalyzerNameRuLucene', 'LexicalAnalyzerNameSrCyrillicMicrosoft', 'LexicalAnalyzerNameSrLatinMicrosoft', 'LexicalAnalyzerNameSkMicrosoft', 'LexicalAnalyzerNameSlMicrosoft', 'LexicalAnalyzerNameEsMicrosoft', 'LexicalAnalyzerNameEsLucene', 'LexicalAnalyzerNameSvMicrosoft', 'LexicalAnalyzerNameSvLucene', 'LexicalAnalyzerNameTaMicrosoft', 'LexicalAnalyzerNameTeMicrosoft', 'LexicalAnalyzerNameThMicrosoft', 'LexicalAnalyzerNameThLucene', 'LexicalAnalyzerNameTrMicrosoft', 'LexicalAnalyzerNameTrLucene', 'LexicalAnalyzerNameUkMicrosoft', 'LexicalAnalyzerNameUrMicrosoft', 'LexicalAnalyzerNameViMicrosoft', 'LexicalAnalyzerNameStandardLucene', 'LexicalAnalyzerNameStandardASCIIFoldingLucene', 'LexicalAnalyzerNameKeyword', 'LexicalAnalyzerNamePattern', 'LexicalAnalyzerNameSimple', 'LexicalAnalyzerNameStop', 'LexicalAnalyzerNameWhitespace'
	IndexAnalyzer LexicalAnalyzerName `json:"indexAnalyzer,omitempty"`
	// Normalizer - The name of the normalizer to use for the field. This option can be used only with fields with filterable, sortable, or facetable enabled. Once the normalizer is chosen, it cannot be changed for the field. Must be null for complex fields. Possible values include: 'LexicalNormalizerNameASCIIFolding', 'LexicalNormalizerNameElision', 'LexicalNormalizerNameLowercase', 'LexicalNormalizerNameStandard', 'LexicalNormalizerNameUppercase'
	Normalizer LexicalNormalizerName `json:"normalizer,omitempty"`
	// VectorSearchDimensions - The dimensionality of the vector field.
	VectorSearchDimensions *int32 `json:"dimensions,omitempty"`
	// VectorSearchProfileName - The name of the vector search profile that specifies the algorithm and vectorizer to use when searching the vector field.
	VectorSearchProfileName *string `json:"vectorSearchProfile,omitempty"`
	// SynonymMaps - A list of the names of synonym maps to associate with this field. This option can be used only with searchable fields. Currently only one synonym map per field is supported. Assigning a synonym map to a field ensures that query terms targeting that field are expanded at query-time using the rules in the synonym map. This attribute can be changed on existing fields. Must be null or an empty collection for complex fields.
	SynonymMaps *[]string `json:"synonymMaps,omitempty"`
	// Fields - A list of sub-fields if this is a field of type Edm.ComplexType or Collection(Edm.ComplexType). Must be null or empty for simple fields.
	Fields *[]Field `json:"fields,omitempty"`
}

// FieldMapping defines a mapping between a field in a data source and a target field in an index.
type FieldMapping struct {
	// SourceFieldName - The name of the field in the data source.
	SourceFieldName *string `json:"sourceFieldName,omitempty"`
	// TargetFieldName - The name of the target field in the index. Same as the source field name by default.
	TargetFieldName *string `json:"targetFieldName,omitempty"`
	// MappingFunction - A function to apply to each source field value before indexing.
	MappingFunction *FieldMappingFunction `json:"mappingFunction,omitempty"`
}

// FieldMappingFunction represents a function that transforms a value from a data source before indexing.
type FieldMappingFunction struct {
	// Name - The name of the field mapping function.
	Name *string `json:"name,omitempty"`
	// Parameters - A dictionary of parameter name/value pairs to pass to the function. Each value must be of a primitive type.
	Parameters interface{} `json:"parameters,omitempty"`
}

// FreshnessScoringFunction defines a function that boosts scores based on the value of a date-time field.
type FreshnessScoringFunction struct {
	// Parameters - Parameter values for the freshness scoring function.
	Parameters *FreshnessScoringParameters `json:"freshness,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'ScoringFunctionInterpolationLinear', 'ScoringFunctionInterpolationConstant', 'ScoringFunctionInterpolationQuadratic', 'ScoringFunctionInterpolationLogarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) MarshalJSON() ([]byte, error) {
	fsf.Type = TypeFreshness
	objectMap := make(map[string]interface{})
	if fsf.Parameters != nil {
		objectMap["freshness"] = fsf.Parameters
	}
	if fsf.FieldName != nil {
		objectMap["fieldName"] = fsf.FieldName
	}
	if fsf.Boost != nil {
		objectMap["boost"] = fsf.Boost
	}
	if fsf.Interpolation != "" {
		objectMap["interpolation"] = fsf.Interpolation
	}
	if fsf.Type != "" {
		objectMap["type"] = fsf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return &fsf, true
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &fsf, true
}

// FreshnessScoringParameters provides parameter values to a freshness scoring function.
type FreshnessScoringParameters struct {
	// BoostingDuration - The expiration period after which boosting will stop for a particular document.
	BoostingDuration *string `json:"boostingDuration,omitempty"`
}

// GetIndexStatisticsResult statistics for a given index. Statistics are collected periodically and are not
// guaranteed to always be up-to-date.
type GetIndexStatisticsResult struct {
	autorest.Response `json:"-"`
	// DocumentCount - READ-ONLY; The number of documents in the index.
	DocumentCount *int64 `json:"documentCount,omitempty"`
	// StorageSize - READ-ONLY; The amount of storage in bytes consumed by the index.
	StorageSize *int64 `json:"storageSize,omitempty"`
	// VectorIndexSize - READ-ONLY; The amount of memory in bytes consumed by vectors in the index.
	VectorIndexSize *int64 `json:"vectorIndexSize,omitempty"`
}

// MarshalJSON is the custom marshaler for GetIndexStatisticsResult.
func (gisr GetIndexStatisticsResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// HighWaterMarkChangeDetectionPolicy defines a data change detection policy that captures changes based on
// the value of a high water mark column.
type HighWaterMarkChangeDetectionPolicy struct {
	// HighWaterMarkColumnName - The name of the high water mark column.
	HighWaterMarkColumnName *string `json:"highWaterMarkColumnName,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeDataChangeDetectionPolicy', 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy', 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy'
	OdataType OdataTypeBasicDataChangeDetectionPolicy `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) MarshalJSON() ([]byte, error) {
	hwmcdp.OdataType = OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy
	objectMap := make(map[string]interface{})
	if hwmcdp.HighWaterMarkColumnName != nil {
		objectMap["highWaterMarkColumnName"] = hwmcdp.HighWaterMarkColumnName
	}
	if hwmcdp.OdataType != "" {
		objectMap["@odata.type"] = hwmcdp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsHighWaterMarkChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool) {
	return &hwmcdp, true
}

// AsSQLIntegratedChangeTrackingPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool) {
	return nil, false
}

// AsDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool) {
	return nil, false
}

// AsBasicDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsBasicDataChangeDetectionPolicy() (BasicDataChangeDetectionPolicy, bool) {
	return &hwmcdp, true
}

// HnswParameters contains the parameters specific to the HNSW algorithm.
type HnswParameters struct {
	// M - The number of bi-directional links created for every new element during construction. Increasing this parameter value may improve recall and reduce retrieval times for datasets with high intrinsic dimensionality at the expense of increased memory consumption and longer indexing time.
	M *int32 `json:"m,omitempty"`
	// EfConstruction - The size of the dynamic list containing the nearest neighbors, which is used during index time. Increasing this parameter may improve index quality, at the expense of increased indexing time. At a certain point, increasing this parameter leads to diminishing returns.
	EfConstruction *int32 `json:"efConstruction,omitempty"`
	// EfSearch - The size of the dynamic list containing the nearest neighbors, which is used during search time. Increasing this parameter may improve search results, at the expense of slower search. At a certain point, increasing this parameter leads to diminishing returns.
	EfSearch *int32 `json:"efSearch,omitempty"`
	// Metric - The similarity metric to use for vector comparisons. Possible values include: 'VectorSearchAlgorithmMetricCosine', 'VectorSearchAlgorithmMetricEuclidean', 'VectorSearchAlgorithmMetricDotProduct'
	Metric VectorSearchAlgorithmMetric `json:"metric,omitempty"`
}

// HnswVectorSearchAlgorithmConfiguration contains configuration options specific to the HNSW approximate
// nearest neighbors algorithm used during indexing and querying. The HNSW algorithm offers a tunable
// trade-off between search speed and accuracy.
type HnswVectorSearchAlgorithmConfiguration struct {
	// Parameters - Contains the parameters specific to HNSW algorithm.
	Parameters *HnswParameters `json:"hnswParameters,omitempty"`
	// Name - The name to associate with this particular configuration.
	Name *string `json:"name,omitempty"`
	// Kind - Possible values include: 'KindVectorSearchAlgorithmConfiguration', 'KindHnsw', 'KindExhaustiveKnn'
	Kind Kind `json:"kind,omitempty"`
}

// MarshalJSON is the custom marshaler for HnswVectorSearchAlgorithmConfiguration.
func (hvsac HnswVectorSearchAlgorithmConfiguration) MarshalJSON() ([]byte, error) {
	hvsac.Kind = KindHnsw
	objectMap := make(map[string]interface{})
	if hvsac.Parameters != nil {
		objectMap["hnswParameters"] = hvsac.Parameters
	}
	if hvsac.Name != nil {
		objectMap["name"] = hvsac.Name
	}
	if hvsac.Kind != "" {
		objectMap["kind"] = hvsac.Kind
	}
	return json.Marshal(objectMap)
}

// AsHnswVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for HnswVectorSearchAlgorithmConfiguration.
func (hvsac HnswVectorSearchAlgorithmConfiguration) AsHnswVectorSearchAlgorithmConfiguration() (*HnswVectorSearchAlgorithmConfiguration, bool) {
	return &hvsac, true
}

// AsExhaustiveKnnVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for HnswVectorSearchAlgorithmConfiguration.
func (hvsac HnswVectorSearchAlgorithmConfiguration) AsExhaustiveKnnVectorSearchAlgorithmConfiguration() (*ExhaustiveKnnVectorSearchAlgorithmConfiguration, bool) {
	return nil, false
}

// AsVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for HnswVectorSearchAlgorithmConfiguration.
func (hvsac HnswVectorSearchAlgorithmConfiguration) AsVectorSearchAlgorithmConfiguration() (*VectorSearchAlgorithmConfiguration, bool) {
	return nil, false
}

// AsBasicVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for HnswVectorSearchAlgorithmConfiguration.
func (hvsac HnswVectorSearchAlgorithmConfiguration) AsBasicVectorSearchAlgorithmConfiguration() (BasicVectorSearchAlgorithmConfiguration, bool) {
	return &hvsac, true
}

// ImageAnalysisSkill a skill that analyzes image files. It extracts a rich set of visual features based on
// the image content.
type ImageAnalysisSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`. Possible values include: 'ImageAnalysisSkillLanguageAr', 'ImageAnalysisSkillLanguageAz', 'ImageAnalysisSkillLanguageBg', 'ImageAnalysisSkillLanguageBs', 'ImageAnalysisSkillLanguageCa', 'ImageAnalysisSkillLanguageCs', 'ImageAnalysisSkillLanguageCy', 'ImageAnalysisSkillLanguageDa', 'ImageAnalysisSkillLanguageDe', 'ImageAnalysisSkillLanguageEl', 'ImageAnalysisSkillLanguageEn', 'ImageAnalysisSkillLanguageEs', 'ImageAnalysisSkillLanguageEt', 'ImageAnalysisSkillLanguageEu', 'ImageAnalysisSkillLanguageFi', 'ImageAnalysisSkillLanguageFr', 'ImageAnalysisSkillLanguageGa', 'ImageAnalysisSkillLanguageGl', 'ImageAnalysisSkillLanguageHe', 'ImageAnalysisSkillLanguageHi', 'ImageAnalysisSkillLanguageHr', 'ImageAnalysisSkillLanguageHu', 'ImageAnalysisSkillLanguageID', 'ImageAnalysisSkillLanguageIt', 'ImageAnalysisSkillLanguageJa', 'ImageAnalysisSkillLanguageKk', 'ImageAnalysisSkillLanguageKo', 'ImageAnalysisSkillLanguageLt', 'ImageAnalysisSkillLanguageLv', 'ImageAnalysisSkillLanguageMk', 'ImageAnalysisSkillLanguageMs', 'ImageAnalysisSkillLanguageNb', 'ImageAnalysisSkillLanguageNl', 'ImageAnalysisSkillLanguagePl', 'ImageAnalysisSkillLanguagePrs', 'ImageAnalysisSkillLanguagePtBR', 'ImageAnalysisSkillLanguagePt', 'ImageAnalysisSkillLanguagePtPT', 'ImageAnalysisSkillLanguageRo', 'ImageAnalysisSkillLanguageRu', 'ImageAnalysisSkillLanguageSk', 'ImageAnalysisSkillLanguageSl', 'ImageAnalysisSkillLanguageSrCyrl', 'ImageAnalysisSkillLanguageSrLatn', 'ImageAnalysisSkillLanguageSv', 'ImageAnalysisSkillLanguageTh', 'ImageAnalysisSkillLanguageTr', 'ImageAnalysisSkillLanguageUk', 'ImageAnalysisSkillLanguageVi', 'ImageAnalysisSkillLanguageZh', 'ImageAnalysisSkillLanguageZhHans', 'ImageAnalysisSkillLanguageZhHant'
	DefaultLanguageCode ImageAnalysisSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// VisualFeatures - A list of visual features.
	VisualFeatures *[]VisualFeature `json:"visualFeatures,omitempty"`
	// Details - A string indicating which domain-specific details to return.
	Details *[]ImageDetail `json:"details,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) MarshalJSON() ([]byte, error) {
	ias.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill
	objectMap := make(map[string]interface{})
	if ias.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ias.DefaultLanguageCode
	}
	if ias.VisualFeatures != nil {
		objectMap["visualFeatures"] = ias.VisualFeatures
	}
	if ias.Details != nil {
		objectMap["details"] = ias.Details
	}
	if ias.Name != nil {
		objectMap["name"] = ias.Name
	}
	if ias.Description != nil {
		objectMap["description"] = ias.Description
	}
	if ias.Context != nil {
		objectMap["context"] = ias.Context
	}
	if ias.Inputs != nil {
		objectMap["inputs"] = ias.Inputs
	}
	if ias.Outputs != nil {
		objectMap["outputs"] = ias.Outputs
	}
	if ias.OdataType != "" {
		objectMap["@odata.type"] = ias.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return &ias, true
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ias, true
}

// Index represents a search index definition, which describes the fields and search behavior of an index.
type Index struct {
	autorest.Response `json:"-"`
	// Name - The name of the index.
	Name *string `json:"name,omitempty"`
	// Fields - The fields of the index.
	Fields *[]Field `json:"fields,omitempty"`
	// ScoringProfiles - The scoring profiles for the index.
	ScoringProfiles *[]ScoringProfile `json:"scoringProfiles,omitempty"`
	// DefaultScoringProfile - The name of the scoring profile to use if none is specified in the query. If this property is not set and no scoring profile is specified in the query, then default scoring (tf-idf) will be used.
	DefaultScoringProfile *string `json:"defaultScoringProfile,omitempty"`
	// CorsOptions - Options to control Cross-Origin Resource Sharing (CORS) for the index.
	CorsOptions *CorsOptions `json:"corsOptions,omitempty"`
	// Suggesters - The suggesters for the index.
	Suggesters *[]Suggester `json:"suggesters,omitempty"`
	// Analyzers - The analyzers for the index.
	Analyzers *[]BasicLexicalAnalyzer `json:"analyzers,omitempty"`
	// Tokenizers - The tokenizers for the index.
	Tokenizers *[]BasicLexicalTokenizer `json:"tokenizers,omitempty"`
	// TokenFilters - The token filters for the index.
	TokenFilters *[]BasicTokenFilter `json:"tokenFilters,omitempty"`
	// CharFilters - The character filters for the index.
	CharFilters *[]BasicCharFilter `json:"charFilters,omitempty"`
	// Normalizers - The normalizers for the index.
	Normalizers *[]BasicLexicalNormalizer `json:"normalizers,omitempty"`
	// EncryptionKey - A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level of encryption-at-rest for your data when you want full assurance that no one, not even Microsoft, can decrypt your data. Once you have encrypted your data, it will always remain encrypted. The search service will ignore attempts to set this property to null. You can change this property as needed if you want to rotate your encryption key; Your data will be unaffected. Encryption with customer-managed keys is not available for free search services, and is only available for paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey `json:"encryptionKey,omitempty"`
	// Similarity - The type of similarity algorithm to be used when scoring and ranking the documents matching a search query. The similarity algorithm can only be defined at index creation time and cannot be modified on existing indexes. If null, the ClassicSimilarity algorithm is used.
	Similarity BasicSimilarity `json:"similarity,omitempty"`
	// SemanticSearch - Defines parameters for a search index that influence semantic capabilities.
	SemanticSearch *SemanticSettings `json:"semantic,omitempty"`
	// VectorSearch - Contains configuration options related to vector search.
	VectorSearch *VectorSearch `json:"vectorSearch,omitempty"`
	// ETag - The ETag of the index.
	ETag *string `json:"@odata.etag,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for Index struct.
func (i *Index) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				i.Name = &name
			}
		case "fields":
			if v != nil {
				var fields []Field
				err = json.Unmarshal(*v, &fields)
				if err != nil {
					return err
				}
				i.Fields = &fields
			}
		case "scoringProfiles":
			if v != nil {
				var scoringProfiles []ScoringProfile
				err = json.Unmarshal(*v, &scoringProfiles)
				if err != nil {
					return err
				}
				i.ScoringProfiles = &scoringProfiles
			}
		case "defaultScoringProfile":
			if v != nil {
				var defaultScoringProfile string
				err = json.Unmarshal(*v, &defaultScoringProfile)
				if err != nil {
					return err
				}
				i.DefaultScoringProfile = &defaultScoringProfile
			}
		case "corsOptions":
			if v != nil {
				var corsOptions CorsOptions
				err = json.Unmarshal(*v, &corsOptions)
				if err != nil {
					return err
				}
				i.CorsOptions = &corsOptions
			}
		case "suggesters":
			if v != nil {
				var suggesters []Suggester
				err = json.Unmarshal(*v, &suggesters)
				if err != nil {
					return err
				}
				i.Suggesters = &suggesters
			}
		case "analyzers":
			if v != nil {
				analyzers, err := unmarshalBasicLexicalAnalyzerArray(*v)
				if err != nil {
					return err
				}
				i.Analyzers = &analyzers
			}
		case "tokenizers":
			if v != nil {
				tokenizers, err := unmarshalBasicLexicalTokenizerArray(*v)
				if err != nil {
					return err
				}
				i.Tokenizers = &tokenizers
			}
		case "tokenFilters":
			if v != nil {
				tokenFilters, err := unmarshalBasicTokenFilterArray(*v)
				if err != nil {
					return err
				}
				i.TokenFilters = &tokenFilters
			}
		case "charFilters":
			if v != nil {
				charFilters, err := unmarshalBasicCharFilterArray(*v)
				if err != nil {
					return err
				}
				i.CharFilters = &charFilters
			}
		case "normalizers":
			if v != nil {
				normalizers, err := unmarshalBasicLexicalNormalizerArray(*v)
				if err != nil {
					return err
				}
				i.Normalizers = &normalizers
			}
		case "encryptionKey":
			if v != nil {
				var encryptionKey ResourceEncryptionKey
				err = json.Unmarshal(*v, &encryptionKey)
				if err != nil {
					return err
				}
				i.EncryptionKey = &encryptionKey
			}
		case "similarity":
			if v != nil {
				similarity, err := unmarshalBasicSimilarity(*v)
				if err != nil {
					return err
				}
				i.Similarity = similarity
			}
		case "semantic":
			if v != nil {
				var semanticSearch SemanticSettings
				err = json.Unmarshal(*v, &semanticSearch)
				if err != nil {
					return err
				}
				i.SemanticSearch = &semanticSearch
			}
		case "vectorSearch":
			if v != nil {
				var vectorSearch VectorSearch
				err = json.Unmarshal(*v, &vectorSearch)
				if err != nil {
					return err
				}
				i.VectorSearch = &vectorSearch
			}
		case "@odata.etag":
			if v != nil {
				var eTag string
				err = json.Unmarshal(*v, &eTag)
				if err != nil {
					return err
				}
				i.ETag = &eTag
			}
		}
	}

	return nil
}

// Indexer represents an indexer.
type Indexer struct {
	autorest.Response `json:"-"`
	// Name - The name of the indexer.
	Name *string `json:"name,omitempty"`
	// Description - The description of the indexer.
	Description *string `json:"description,omitempty"`
	// DataSourceName - The name of the datasource from which this indexer reads data.
	DataSourceName *string `json:"dataSourceName,omitempty"`
	// SkillsetName - The name of the skillset executing with this indexer.
	SkillsetName *string `json:"skillsetName,omitempty"`
	// TargetIndexName - The name of the index to which this indexer writes data.
	TargetIndexName *string `json:"targetIndexName,omitempty"`
	// Schedule - The schedule for this indexer.
	Schedule *IndexingSchedule `json:"schedule,omitempty"`
	// Parameters - Parameters for indexer execution.
	Parameters *IndexingParameters `json:"parameters,omitempty"`
	// FieldMappings - Defines mappings between fields in the data source and corresponding target fields in the index.
	FieldMappings *[]FieldMapping `json:"fieldMappings,omitempty"`
	// OutputFieldMappings - Output field mappings are applied after enrichment and immediately before indexing.
	OutputFieldMappings *[]FieldMapping `json:"outputFieldMappings,omitempty"`
	// IsDisabled - A value indicating whether the indexer is disabled. Default is false.
	IsDisabled *bool `json:"disabled,omitempty"`
	// ETag - The ETag of the indexer.
	ETag *string `json:"@odata.etag,omitempty"`
	// EncryptionKey - A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level of encryption-at-rest for your indexer definition (as well as indexer execution status) when you want full assurance that no one, not even Microsoft, can decrypt them. Once you have encrypted your indexer definition, it will always remain encrypted. The search service will ignore attempts to set this property to null. You can change this property as needed if you want to rotate your encryption key; Your indexer definition (and indexer execution status) will be unaffected. Encryption with customer-managed keys is not available for free search services, and is only available for paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey `json:"encryptionKey,omitempty"`
	// Cache - Adds caching to an enrichment pipeline to allow for incremental modification steps without having to rebuild the index every time.
	Cache *IndexerCache `json:"cache,omitempty"`
}

// IndexerCache ...
type IndexerCache struct {
	// StorageConnectionString - The connection string to the storage account where the cache data will be persisted.
	StorageConnectionString *string `json:"storageConnectionString,omitempty"`
	// EnableReprocessing - Specifies whether incremental reprocessing is enabled.
	EnableReprocessing *bool `json:"enableReprocessing,omitempty"`
	// Identity - The user-assigned managed identity used for connections to the enrichment cache.  If the connection string indicates an identity (ResourceId) and it's not specified, the system-assigned managed identity is used. On updates to the indexer, if the identity is unspecified, the value remains unchanged. If set to "none", the value of this property is cleared.
	Identity BasicIndexerDataIdentity `json:"identity,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for IndexerCache struct.
func (ic *IndexerCache) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "storageConnectionString":
			if v != nil {
				var storageConnectionString string
				err = json.Unmarshal(*v, &storageConnectionString)
				if err != nil {
					return err
				}
				ic.StorageConnectionString = &storageConnectionString
			}
		case "enableReprocessing":
			if v != nil {
				var enableReprocessing bool
				err = json.Unmarshal(*v, &enableReprocessing)
				if err != nil {
					return err
				}
				ic.EnableReprocessing = &enableReprocessing
			}
		case "identity":
			if v != nil {
				identity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				ic.Identity = identity
			}
		}
	}

	return nil
}

// IndexerCurrentState represents all of the state that defines and dictates the indexer's current
// execution.
type IndexerCurrentState struct {
	// Mode - READ-ONLY; The mode the indexer is running in. Possible values include: 'IndexingModeIndexingAllDocs', 'IndexingModeIndexingResetDocs'
	Mode IndexingMode `json:"mode,omitempty"`
	// AllDocsInitialChangeTrackingState - READ-ONLY; Change tracking state used when indexing starts on all documents in the datasource.
	AllDocsInitialChangeTrackingState *string `json:"allDocsInitialChangeTrackingState,omitempty"`
	// AllDocsFinalChangeTrackingState - READ-ONLY; Change tracking state value when indexing finishes on all documents in the datasource.
	AllDocsFinalChangeTrackingState *string `json:"allDocsFinalChangeTrackingState,omitempty"`
	// ResetDocsInitialChangeTrackingState - READ-ONLY; Change tracking state used when indexing starts on select, reset documents in the datasource.
	ResetDocsInitialChangeTrackingState *string `json:"resetDocsInitialChangeTrackingState,omitempty"`
	// ResetDocsFinalChangeTrackingState - READ-ONLY; Change tracking state value when indexing finishes on select, reset documents in the datasource.
	ResetDocsFinalChangeTrackingState *string `json:"resetDocsFinalChangeTrackingState,omitempty"`
	// ResetDocumentKeys - READ-ONLY; The list of document keys that have been reset. The document key is the document's unique identifier for the data in the search index. The indexer will prioritize selectively re-ingesting these keys.
	ResetDocumentKeys *[]string `json:"resetDocumentKeys,omitempty"`
	// ResetDatasourceDocumentIds - READ-ONLY; The list of datasource document ids that have been reset. The datasource document id is the unique identifier for the data in the datasource. The indexer will prioritize selectively re-ingesting these ids.
	ResetDatasourceDocumentIds *[]string `json:"resetDatasourceDocumentIds,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerCurrentState.
func (ics IndexerCurrentState) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexerDataContainer represents information about the entity (such as Azure SQL table or CosmosDB
// collection) that will be indexed.
type IndexerDataContainer struct {
	// Name - The name of the table or view (for Azure SQL data source) or collection (for CosmosDB data source) that will be indexed.
	Name *string `json:"name,omitempty"`
	// Query - A query that is applied to this data container. The syntax and meaning of this parameter is datasource-specific. Not supported by Azure SQL datasources.
	Query *string `json:"query,omitempty"`
}

// BasicIndexerDataIdentity abstract base type for data identities.
type BasicIndexerDataIdentity interface {
	AsIndexerDataNoneIdentity() (*IndexerDataNoneIdentity, bool)
	AsIndexerDataUserAssignedIdentity() (*IndexerDataUserAssignedIdentity, bool)
	AsIndexerDataIdentity() (*IndexerDataIdentity, bool)
}

// IndexerDataIdentity abstract base type for data identities.
type IndexerDataIdentity struct {
	// OdataType - Possible values include: 'OdataTypeBasicIndexerDataIdentityOdataTypeSearchIndexerDataIdentity', 'OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataNoneIdentity', 'OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataUserAssignedIdentity'
	OdataType OdataTypeBasicIndexerDataIdentity `json:"@odata.type,omitempty"`
}

func unmarshalBasicIndexerDataIdentity(body []byte) (BasicIndexerDataIdentity, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataNoneIdentity):
		var idni IndexerDataNoneIdentity
		err := json.Unmarshal(body, &idni)
		return idni, err
	case string(OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataUserAssignedIdentity):
		var iduai IndexerDataUserAssignedIdentity
		err := json.Unmarshal(body, &iduai)
		return iduai, err
	default:
		var idi IndexerDataIdentity
		err := json.Unmarshal(body, &idi)
		return idi, err
	}
}
func unmarshalBasicIndexerDataIdentityArray(body []byte) ([]BasicIndexerDataIdentity, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	idiArray := make([]BasicIndexerDataIdentity, len(rawMessages))

	for index, rawMessage := range rawMessages {
		idi, err := unmarshalBasicIndexerDataIdentity(*rawMessage)
		if err != nil {
			return nil, err
		}
		idiArray[index] = idi
	}
	return idiArray, nil
}

// MarshalJSON is the custom marshaler for IndexerDataIdentity.
func (idi IndexerDataIdentity) MarshalJSON() ([]byte, error) {
	idi.OdataType = OdataTypeBasicIndexerDataIdentityOdataTypeSearchIndexerDataIdentity
	objectMap := make(map[string]interface{})
	if idi.OdataType != "" {
		objectMap["@odata.type"] = idi.OdataType
	}
	return json.Marshal(objectMap)
}

// AsIndexerDataNoneIdentity is the BasicIndexerDataIdentity implementation for IndexerDataIdentity.
func (idi IndexerDataIdentity) AsIndexerDataNoneIdentity() (*IndexerDataNoneIdentity, bool) {
	return nil, false
}

// AsIndexerDataUserAssignedIdentity is the BasicIndexerDataIdentity implementation for IndexerDataIdentity.
func (idi IndexerDataIdentity) AsIndexerDataUserAssignedIdentity() (*IndexerDataUserAssignedIdentity, bool) {
	return nil, false
}

// AsIndexerDataIdentity is the BasicIndexerDataIdentity implementation for IndexerDataIdentity.
func (idi IndexerDataIdentity) AsIndexerDataIdentity() (*IndexerDataIdentity, bool) {
	return &idi, true
}

// AsBasicIndexerDataIdentity is the BasicIndexerDataIdentity implementation for IndexerDataIdentity.
func (idi IndexerDataIdentity) AsBasicIndexerDataIdentity() (BasicIndexerDataIdentity, bool) {
	return &idi, true
}

// IndexerDataNoneIdentity clears the identity property of a datasource.
type IndexerDataNoneIdentity struct {
	// OdataType - Possible values include: 'OdataTypeBasicIndexerDataIdentityOdataTypeSearchIndexerDataIdentity', 'OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataNoneIdentity', 'OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataUserAssignedIdentity'
	OdataType OdataTypeBasicIndexerDataIdentity `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerDataNoneIdentity.
func (idni IndexerDataNoneIdentity) MarshalJSON() ([]byte, error) {
	idni.OdataType = OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataNoneIdentity
	objectMap := make(map[string]interface{})
	if idni.OdataType != "" {
		objectMap["@odata.type"] = idni.OdataType
	}
	return json.Marshal(objectMap)
}

// AsIndexerDataNoneIdentity is the BasicIndexerDataIdentity implementation for IndexerDataNoneIdentity.
func (idni IndexerDataNoneIdentity) AsIndexerDataNoneIdentity() (*IndexerDataNoneIdentity, bool) {
	return &idni, true
}

// AsIndexerDataUserAssignedIdentity is the BasicIndexerDataIdentity implementation for IndexerDataNoneIdentity.
func (idni IndexerDataNoneIdentity) AsIndexerDataUserAssignedIdentity() (*IndexerDataUserAssignedIdentity, bool) {
	return nil, false
}

// AsIndexerDataIdentity is the BasicIndexerDataIdentity implementation for IndexerDataNoneIdentity.
func (idni IndexerDataNoneIdentity) AsIndexerDataIdentity() (*IndexerDataIdentity, bool) {
	return nil, false
}

// AsBasicIndexerDataIdentity is the BasicIndexerDataIdentity implementation for IndexerDataNoneIdentity.
func (idni IndexerDataNoneIdentity) AsBasicIndexerDataIdentity() (BasicIndexerDataIdentity, bool) {
	return &idni, true
}

// IndexerDataSource represents a datasource definition, which can be used to configure an indexer.
type IndexerDataSource struct {
	autorest.Response `json:"-"`
	// Name - The name of the datasource.
	Name *string `json:"name,omitempty"`
	// Description - The description of the datasource.
	Description *string `json:"description,omitempty"`
	// Type - The type of the datasource. Possible values include: 'IndexerDataSourceTypeAzureSQL', 'IndexerDataSourceTypeCosmosDb', 'IndexerDataSourceTypeAzureBlob', 'IndexerDataSourceTypeAzureTable', 'IndexerDataSourceTypeMySQL', 'IndexerDataSourceTypeAdlsGen2'
	Type IndexerDataSourceType `json:"type,omitempty"`
	// Credentials - Credentials for the datasource.
	Credentials *DataSourceCredentials `json:"credentials,omitempty"`
	// Container - The data container for the datasource.
	Container *IndexerDataContainer `json:"container,omitempty"`
	// Identity - An explicit managed identity to use for this datasource. If not specified and the connection string is a managed identity, the system-assigned managed identity is used. If not specified, the value remains unchanged. If "none" is specified, the value of this property is cleared.
	Identity BasicIndexerDataIdentity `json:"identity,omitempty"`
	// DataChangeDetectionPolicy - The data change detection policy for the datasource.
	DataChangeDetectionPolicy BasicDataChangeDetectionPolicy `json:"dataChangeDetectionPolicy,omitempty"`
	// DataDeletionDetectionPolicy - The data deletion detection policy for the datasource.
	DataDeletionDetectionPolicy BasicDataDeletionDetectionPolicy `json:"dataDeletionDetectionPolicy,omitempty"`
	// ETag - The ETag of the data source.
	ETag *string `json:"@odata.etag,omitempty"`
	// EncryptionKey - A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level of encryption-at-rest for your datasource definition when you want full assurance that no one, not even Microsoft, can decrypt your data source definition. Once you have encrypted your data source definition, it will always remain encrypted. The search service will ignore attempts to set this property to null. You can change this property as needed if you want to rotate your encryption key; Your datasource definition will be unaffected. Encryption with customer-managed keys is not available for free search services, and is only available for paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey `json:"encryptionKey,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for IndexerDataSource struct.
func (ids *IndexerDataSource) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				ids.Name = &name
			}
		case "description":
			if v != nil {
				var description string
				err = json.Unmarshal(*v, &description)
				if err != nil {
					return err
				}
				ids.Description = &description
			}
		case "type":
			if v != nil {
				var typeVar IndexerDataSourceType
				err = json.Unmarshal(*v, &typeVar)
				if err != nil {
					return err
				}
				ids.Type = typeVar
			}
		case "credentials":
			if v != nil {
				var credentials DataSourceCredentials
				err = json.Unmarshal(*v, &credentials)
				if err != nil {
					return err
				}
				ids.Credentials = &credentials
			}
		case "container":
			if v != nil {
				var containerVar IndexerDataContainer
				err = json.Unmarshal(*v, &containerVar)
				if err != nil {
					return err
				}
				ids.Container = &containerVar
			}
		case "identity":
			if v != nil {
				identity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				ids.Identity = identity
			}
		case "dataChangeDetectionPolicy":
			if v != nil {
				dataChangeDetectionPolicy, err := unmarshalBasicDataChangeDetectionPolicy(*v)
				if err != nil {
					return err
				}
				ids.DataChangeDetectionPolicy = dataChangeDetectionPolicy
			}
		case "dataDeletionDetectionPolicy":
			if v != nil {
				dataDeletionDetectionPolicy, err := unmarshalBasicDataDeletionDetectionPolicy(*v)
				if err != nil {
					return err
				}
				ids.DataDeletionDetectionPolicy = dataDeletionDetectionPolicy
			}
		case "@odata.etag":
			if v != nil {
				var eTag string
				err = json.Unmarshal(*v, &eTag)
				if err != nil {
					return err
				}
				ids.ETag = &eTag
			}
		case "encryptionKey":
			if v != nil {
				var encryptionKey ResourceEncryptionKey
				err = json.Unmarshal(*v, &encryptionKey)
				if err != nil {
					return err
				}
				ids.EncryptionKey = &encryptionKey
			}
		}
	}

	return nil
}

// IndexerDataUserAssignedIdentity specifies the identity for a datasource to use.
type IndexerDataUserAssignedIdentity struct {
	// UserAssignedIdentity - The fully qualified Azure resource Id of a user assigned managed identity typically in the form "/subscriptions/12345678-1234-1234-1234-1234567890ab/resourceGroups/rg/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myId" that should have been assigned to the search service.
	UserAssignedIdentity *string `json:"userAssignedIdentity,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerDataIdentityOdataTypeSearchIndexerDataIdentity', 'OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataNoneIdentity', 'OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataUserAssignedIdentity'
	OdataType OdataTypeBasicIndexerDataIdentity `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerDataUserAssignedIdentity.
func (iduai IndexerDataUserAssignedIdentity) MarshalJSON() ([]byte, error) {
	iduai.OdataType = OdataTypeBasicIndexerDataIdentityOdataTypeMicrosoftAzureSearchDataUserAssignedIdentity
	objectMap := make(map[string]interface{})
	if iduai.UserAssignedIdentity != nil {
		objectMap["userAssignedIdentity"] = iduai.UserAssignedIdentity
	}
	if iduai.OdataType != "" {
		objectMap["@odata.type"] = iduai.OdataType
	}
	return json.Marshal(objectMap)
}

// AsIndexerDataNoneIdentity is the BasicIndexerDataIdentity implementation for IndexerDataUserAssignedIdentity.
func (iduai IndexerDataUserAssignedIdentity) AsIndexerDataNoneIdentity() (*IndexerDataNoneIdentity, bool) {
	return nil, false
}

// AsIndexerDataUserAssignedIdentity is the BasicIndexerDataIdentity implementation for IndexerDataUserAssignedIdentity.
func (iduai IndexerDataUserAssignedIdentity) AsIndexerDataUserAssignedIdentity() (*IndexerDataUserAssignedIdentity, bool) {
	return &iduai, true
}

// AsIndexerDataIdentity is the BasicIndexerDataIdentity implementation for IndexerDataUserAssignedIdentity.
func (iduai IndexerDataUserAssignedIdentity) AsIndexerDataIdentity() (*IndexerDataIdentity, bool) {
	return nil, false
}

// AsBasicIndexerDataIdentity is the BasicIndexerDataIdentity implementation for IndexerDataUserAssignedIdentity.
func (iduai IndexerDataUserAssignedIdentity) AsBasicIndexerDataIdentity() (BasicIndexerDataIdentity, bool) {
	return &iduai, true
}

// IndexerError represents an item- or document-level indexing error.
type IndexerError struct {
	// Key - READ-ONLY; The key of the item for which indexing failed.
	Key *string `json:"key,omitempty"`
	// ErrorMessage - READ-ONLY; The message describing the error that occurred while processing the item.
	ErrorMessage *string `json:"errorMessage,omitempty"`
	// StatusCode - READ-ONLY; The status code indicating why the indexing operation failed. Possible values include: 400 for a malformed input document, 404 for document not found, 409 for a version conflict, 422 when the index is temporarily unavailable, or 503 for when the service is too busy.
	StatusCode *int32 `json:"statusCode,omitempty"`
	// Name - READ-ONLY; The name of the source at which the error originated. For example, this could refer to a particular skill in the attached skillset. This may not be always available.
	Name *string `json:"name,omitempty"`
	// Details - READ-ONLY; Additional, verbose details about the error to assist in debugging the indexer. This may not be always available.
	Details *string `json:"details,omitempty"`
	// DocumentationLink - READ-ONLY; A link to a troubleshooting guide for these classes of errors. This may not be always available.
	DocumentationLink *string `json:"documentationLink,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerError.
func (ie IndexerError) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexerExecutionResult represents the result of an individual indexer execution.
type IndexerExecutionResult struct {
	// Status - READ-ONLY; The outcome of this indexer execution. Possible values include: 'IndexerExecutionStatusTransientFailure', 'IndexerExecutionStatusSuccess', 'IndexerExecutionStatusInProgress', 'IndexerExecutionStatusReset'
	Status IndexerExecutionStatus `json:"status,omitempty"`
	// StatusDetail - READ-ONLY; The outcome of this indexer execution. Possible values include: 'IndexerExecutionStatusDetailResetDocs'
	StatusDetail IndexerExecutionStatusDetail `json:"statusDetail,omitempty"`
	// CurrentState - READ-ONLY; All of the state that defines and dictates the indexer's current execution.
	CurrentState *IndexerCurrentState `json:"currentState,omitempty"`
	// ErrorMessage - READ-ONLY; The error message indicating the top-level error, if any.
	ErrorMessage *string `json:"errorMessage,omitempty"`
	// StartTime - READ-ONLY; The start time of this indexer execution.
	StartTime *date.Time `json:"startTime,omitempty"`
	// EndTime - READ-ONLY; The end time of this indexer execution, if the execution has already completed.
	EndTime *date.Time `json:"endTime,omitempty"`
	// Errors - READ-ONLY; The item-level indexing errors.
	Errors *[]IndexerError `json:"errors,omitempty"`
	// Warnings - READ-ONLY; The item-level indexing warnings.
	Warnings *[]IndexerWarning `json:"warnings,omitempty"`
	// ItemCount - READ-ONLY; The number of items that were processed during this indexer execution. This includes both successfully processed items and items where indexing was attempted but failed.
	ItemCount *int32 `json:"itemsProcessed,omitempty"`
	// FailedItemCount - READ-ONLY; The number of items that failed to be indexed during this indexer execution.
	FailedItemCount *int32 `json:"itemsFailed,omitempty"`
	// InitialTrackingState - READ-ONLY; Change tracking state with which an indexer execution started.
	InitialTrackingState *string `json:"initialTrackingState,omitempty"`
	// FinalTrackingState - READ-ONLY; Change tracking state with which an indexer execution finished.
	FinalTrackingState *string `json:"finalTrackingState,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerExecutionResult.
func (ier IndexerExecutionResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexerIndexProjectionSelector description for what data to store in the designated search index.
type IndexerIndexProjectionSelector struct {
	// TargetIndexName - Name of the search index to project to. Must have a key field with the 'keyword' analyzer set.
	TargetIndexName *string `json:"targetIndexName,omitempty"`
	// ParentKeyFieldName - Name of the field in the search index to map the parent document's key value to. Must be a string field that is filterable and not the key field.
	ParentKeyFieldName *string `json:"parentKeyFieldName,omitempty"`
	// SourceContext - Source context for the projections. Represents the cardinality at which the document will be split into multiple sub documents.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Mappings - Mappings for the projection, or which source should be mapped to which field in the target index.
	Mappings *[]InputFieldMappingEntry `json:"mappings,omitempty"`
}

// IndexerIndexProjections definition of additional projections to secondary search indexes.
type IndexerIndexProjections struct {
	// Selectors - A list of projections to be performed to secondary search indexes.
	Selectors  *[]IndexerIndexProjectionSelector  `json:"selectors,omitempty"`
	Parameters *IndexerIndexProjectionsParameters `json:"parameters,omitempty"`
}

// IndexerIndexProjectionsParameters a dictionary of index projection-specific configuration properties.
// Each name is the name of a specific property. Each value must be of a primitive type.
type IndexerIndexProjectionsParameters struct {
	// AdditionalProperties - Unmatched properties from the message are deserialized this collection
	AdditionalProperties map[string]interface{} `json:""`
	// ProjectionMode - Defines behavior of the index projections in relation to the rest of the indexer. Possible values include: 'IndexProjectionModeSkipIndexingParentDocuments', 'IndexProjectionModeIncludeIndexingParentDocuments'
	ProjectionMode IndexProjectionMode `json:"projectionMode,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerIndexProjectionsParameters.
func (iipp IndexerIndexProjectionsParameters) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if iipp.ProjectionMode != "" {
		objectMap["projectionMode"] = iipp.ProjectionMode
	}
	for k, v := range iipp.AdditionalProperties {
		objectMap[k] = v
	}
	return json.Marshal(objectMap)
}

// UnmarshalJSON is the custom unmarshaler for IndexerIndexProjectionsParameters struct.
func (iipp *IndexerIndexProjectionsParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		default:
			if v != nil {
				var additionalProperties interface{}
				err = json.Unmarshal(*v, &additionalProperties)
				if err != nil {
					return err
				}
				if iipp.AdditionalProperties == nil {
					iipp.AdditionalProperties = make(map[string]interface{})
				}
				iipp.AdditionalProperties[k] = additionalProperties
			}
		case "projectionMode":
			if v != nil {
				var projectionMode IndexProjectionMode
				err = json.Unmarshal(*v, &projectionMode)
				if err != nil {
					return err
				}
				iipp.ProjectionMode = projectionMode
			}
		}
	}

	return nil
}

// IndexerKnowledgeStore definition of additional projections to azure blob, table, or files, of enriched
// data.
type IndexerKnowledgeStore struct {
	// StorageConnectionString - The connection string to the storage account projections will be stored in.
	StorageConnectionString *string `json:"storageConnectionString,omitempty"`
	// Projections - A list of additional projections to perform during indexing.
	Projections *[]IndexerKnowledgeStoreProjection `json:"projections,omitempty"`
	// Identity - The user-assigned managed identity used for connections to Azure Storage when writing knowledge store projections. If the connection string indicates an identity (ResourceId) and it's not specified, the system-assigned managed identity is used. On updates to the indexer, if the identity is unspecified, the value remains unchanged. If set to "none", the value of this property is cleared.
	Identity   BasicIndexerDataIdentity         `json:"identity,omitempty"`
	Parameters *IndexerKnowledgeStoreParameters `json:"parameters,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for IndexerKnowledgeStore struct.
func (iks *IndexerKnowledgeStore) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "storageConnectionString":
			if v != nil {
				var storageConnectionString string
				err = json.Unmarshal(*v, &storageConnectionString)
				if err != nil {
					return err
				}
				iks.StorageConnectionString = &storageConnectionString
			}
		case "projections":
			if v != nil {
				var projections []IndexerKnowledgeStoreProjection
				err = json.Unmarshal(*v, &projections)
				if err != nil {
					return err
				}
				iks.Projections = &projections
			}
		case "identity":
			if v != nil {
				identity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				iks.Identity = identity
			}
		case "parameters":
			if v != nil {
				var parameters IndexerKnowledgeStoreParameters
				err = json.Unmarshal(*v, &parameters)
				if err != nil {
					return err
				}
				iks.Parameters = &parameters
			}
		}
	}

	return nil
}

// IndexerKnowledgeStoreBlobProjectionSelector abstract class to share properties between concrete
// selectors.
type IndexerKnowledgeStoreBlobProjectionSelector struct {
	// StorageContainer - Blob container to store projections in.
	StorageContainer *string `json:"storageContainer,omitempty"`
	// ReferenceKeyName - Name of reference key to different projection.
	ReferenceKeyName *string `json:"referenceKeyName,omitempty"`
	// GeneratedKeyName - Name of generated key to store projection under.
	GeneratedKeyName *string `json:"generatedKeyName,omitempty"`
	// Source - Source data to project.
	Source *string `json:"source,omitempty"`
	// SourceContext - Source context for complex projections.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Inputs - Nested inputs for complex projections.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
}

// IndexerKnowledgeStoreFileProjectionSelector projection definition for what data to store in Azure Files.
type IndexerKnowledgeStoreFileProjectionSelector struct {
	// StorageContainer - Blob container to store projections in.
	StorageContainer *string `json:"storageContainer,omitempty"`
	// ReferenceKeyName - Name of reference key to different projection.
	ReferenceKeyName *string `json:"referenceKeyName,omitempty"`
	// GeneratedKeyName - Name of generated key to store projection under.
	GeneratedKeyName *string `json:"generatedKeyName,omitempty"`
	// Source - Source data to project.
	Source *string `json:"source,omitempty"`
	// SourceContext - Source context for complex projections.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Inputs - Nested inputs for complex projections.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
}

// IndexerKnowledgeStoreObjectProjectionSelector projection definition for what data to store in Azure
// Blob.
type IndexerKnowledgeStoreObjectProjectionSelector struct {
	// StorageContainer - Blob container to store projections in.
	StorageContainer *string `json:"storageContainer,omitempty"`
	// ReferenceKeyName - Name of reference key to different projection.
	ReferenceKeyName *string `json:"referenceKeyName,omitempty"`
	// GeneratedKeyName - Name of generated key to store projection under.
	GeneratedKeyName *string `json:"generatedKeyName,omitempty"`
	// Source - Source data to project.
	Source *string `json:"source,omitempty"`
	// SourceContext - Source context for complex projections.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Inputs - Nested inputs for complex projections.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
}

// IndexerKnowledgeStoreParameters a dictionary of knowledge store-specific configuration properties. Each
// name is the name of a specific property. Each value must be of a primitive type.
type IndexerKnowledgeStoreParameters struct {
	// AdditionalProperties - Unmatched properties from the message are deserialized this collection
	AdditionalProperties map[string]interface{} `json:""`
	// SynthesizeGeneratedKeyName - Whether or not projections should synthesize a generated key name if one isn't already present.
	SynthesizeGeneratedKeyName *bool `json:"synthesizeGeneratedKeyName,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerKnowledgeStoreParameters.
func (iksp IndexerKnowledgeStoreParameters) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if iksp.SynthesizeGeneratedKeyName != nil {
		objectMap["synthesizeGeneratedKeyName"] = iksp.SynthesizeGeneratedKeyName
	}
	for k, v := range iksp.AdditionalProperties {
		objectMap[k] = v
	}
	return json.Marshal(objectMap)
}

// UnmarshalJSON is the custom unmarshaler for IndexerKnowledgeStoreParameters struct.
func (iksp *IndexerKnowledgeStoreParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		default:
			if v != nil {
				var additionalProperties interface{}
				err = json.Unmarshal(*v, &additionalProperties)
				if err != nil {
					return err
				}
				if iksp.AdditionalProperties == nil {
					iksp.AdditionalProperties = make(map[string]interface{})
				}
				iksp.AdditionalProperties[k] = additionalProperties
			}
		case "synthesizeGeneratedKeyName":
			if v != nil {
				var synthesizeGeneratedKeyName bool
				err = json.Unmarshal(*v, &synthesizeGeneratedKeyName)
				if err != nil {
					return err
				}
				iksp.SynthesizeGeneratedKeyName = &synthesizeGeneratedKeyName
			}
		}
	}

	return nil
}

// IndexerKnowledgeStoreProjection container object for various projection selectors.
type IndexerKnowledgeStoreProjection struct {
	// Tables - Projections to Azure Table storage.
	Tables *[]IndexerKnowledgeStoreTableProjectionSelector `json:"tables,omitempty"`
	// Objects - Projections to Azure Blob storage.
	Objects *[]IndexerKnowledgeStoreObjectProjectionSelector `json:"objects,omitempty"`
	// Files - Projections to Azure File storage.
	Files *[]IndexerKnowledgeStoreFileProjectionSelector `json:"files,omitempty"`
}

// IndexerKnowledgeStoreProjectionSelector abstract class to share properties between concrete selectors.
type IndexerKnowledgeStoreProjectionSelector struct {
	// ReferenceKeyName - Name of reference key to different projection.
	ReferenceKeyName *string `json:"referenceKeyName,omitempty"`
	// GeneratedKeyName - Name of generated key to store projection under.
	GeneratedKeyName *string `json:"generatedKeyName,omitempty"`
	// Source - Source data to project.
	Source *string `json:"source,omitempty"`
	// SourceContext - Source context for complex projections.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Inputs - Nested inputs for complex projections.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
}

// IndexerKnowledgeStoreTableProjectionSelector description for what data to store in Azure Tables.
type IndexerKnowledgeStoreTableProjectionSelector struct {
	// TableName - Name of the Azure table to store projected data in.
	TableName *string `json:"tableName,omitempty"`
	// ReferenceKeyName - Name of reference key to different projection.
	ReferenceKeyName *string `json:"referenceKeyName,omitempty"`
	// GeneratedKeyName - Name of generated key to store projection under.
	GeneratedKeyName *string `json:"generatedKeyName,omitempty"`
	// Source - Source data to project.
	Source *string `json:"source,omitempty"`
	// SourceContext - Source context for complex projections.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Inputs - Nested inputs for complex projections.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
}

// IndexerLimits ...
type IndexerLimits struct {
	// MaxRunTime - READ-ONLY; The maximum duration that the indexer is permitted to run for one execution.
	MaxRunTime *string `json:"maxRunTime,omitempty"`
	// MaxDocumentExtractionSize - READ-ONLY; The maximum size of a document, in bytes, which will be considered valid for indexing.
	MaxDocumentExtractionSize *float64 `json:"maxDocumentExtractionSize,omitempty"`
	// MaxDocumentContentCharactersToExtract - READ-ONLY; The maximum number of characters that will be extracted from a document picked up for indexing.
	MaxDocumentContentCharactersToExtract *float64 `json:"maxDocumentContentCharactersToExtract,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerLimits.
func (il IndexerLimits) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// BasicIndexerSkill base type for skills.
type BasicIndexerSkill interface {
	AsConditionalSkill() (*ConditionalSkill, bool)
	AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool)
	AsOcrSkill() (*OcrSkill, bool)
	AsImageAnalysisSkill() (*ImageAnalysisSkill, bool)
	AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool)
	AsShaperSkill() (*ShaperSkill, bool)
	AsMergeSkill() (*MergeSkill, bool)
	AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool)
	AsSentimentSkill() (*SentimentSkill, bool)
	AsSentimentSkillV3() (*SentimentSkillV3, bool)
	AsEntityLinkingSkill() (*EntityLinkingSkill, bool)
	AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool)
	AsPIIDetectionSkill() (*PIIDetectionSkill, bool)
	AsSplitSkill() (*SplitSkill, bool)
	AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool)
	AsTextTranslationSkill() (*TextTranslationSkill, bool)
	AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool)
	AsWebAPISkill() (*WebAPISkill, bool)
	AsAmlSkill() (*AmlSkill, bool)
	AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool)
	AsIndexerSkill() (*IndexerSkill, bool)
}

// IndexerSkill base type for skills.
type IndexerSkill struct {
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

func unmarshalBasicIndexerSkill(body []byte) (BasicIndexerSkill, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill):
		var cs ConditionalSkill
		err := json.Unmarshal(body, &cs)
		return cs, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill):
		var kpes KeyPhraseExtractionSkill
		err := json.Unmarshal(body, &kpes)
		return kpes, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill):
		var osVar OcrSkill
		err := json.Unmarshal(body, &osVar)
		return osVar, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill):
		var ias ImageAnalysisSkill
		err := json.Unmarshal(body, &ias)
		return ias, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill):
		var lds LanguageDetectionSkill
		err := json.Unmarshal(body, &lds)
		return lds, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill):
		var ss ShaperSkill
		err := json.Unmarshal(body, &ss)
		return ss, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill):
		var ms MergeSkill
		err := json.Unmarshal(body, &ms)
		return ms, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill):
		var ers EntityRecognitionSkill
		err := json.Unmarshal(body, &ers)
		return ers, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill):
		var ss SentimentSkill
		err := json.Unmarshal(body, &ss)
		return ss, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill):
		var ssv SentimentSkillV3
		err := json.Unmarshal(body, &ssv)
		return ssv, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill):
		var els EntityLinkingSkill
		err := json.Unmarshal(body, &els)
		return els, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill):
		var ersv EntityRecognitionSkillV3
		err := json.Unmarshal(body, &ersv)
		return ersv, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill):
		var pds PIIDetectionSkill
		err := json.Unmarshal(body, &pds)
		return pds, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill):
		var ss SplitSkill
		err := json.Unmarshal(body, &ss)
		return ss, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill):
		var cels CustomEntityLookupSkill
		err := json.Unmarshal(body, &cels)
		return cels, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill):
		var tts TextTranslationSkill
		err := json.Unmarshal(body, &tts)
		return tts, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill):
		var desVar DocumentExtractionSkill
		err := json.Unmarshal(body, &desVar)
		return desVar, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill):
		var was WebAPISkill
		err := json.Unmarshal(body, &was)
		return was, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill):
		var as AmlSkill
		err := json.Unmarshal(body, &as)
		return as, err
	case string(OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill):
		var aoaes AzureOpenAIEmbeddingSkill
		err := json.Unmarshal(body, &aoaes)
		return aoaes, err
	default:
		var is IndexerSkill
		err := json.Unmarshal(body, &is)
		return is, err
	}
}
func unmarshalBasicIndexerSkillArray(body []byte) ([]BasicIndexerSkill, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	isArray := make([]BasicIndexerSkill, len(rawMessages))

	for index, rawMessage := range rawMessages {
		is, err := unmarshalBasicIndexerSkill(*rawMessage)
		if err != nil {
			return nil, err
		}
		isArray[index] = is
	}
	return isArray, nil
}

// MarshalJSON is the custom marshaler for IndexerSkill.
func (is IndexerSkill) MarshalJSON() ([]byte, error) {
	is.OdataType = OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill
	objectMap := make(map[string]interface{})
	if is.Name != nil {
		objectMap["name"] = is.Name
	}
	if is.Description != nil {
		objectMap["description"] = is.Description
	}
	if is.Context != nil {
		objectMap["context"] = is.Context
	}
	if is.Inputs != nil {
		objectMap["inputs"] = is.Inputs
	}
	if is.Outputs != nil {
		objectMap["outputs"] = is.Outputs
	}
	if is.OdataType != "" {
		objectMap["@odata.type"] = is.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return &is, true
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for IndexerSkill.
func (is IndexerSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &is, true
}

// IndexerSkillset a list of skills.
type IndexerSkillset struct {
	autorest.Response `json:"-"`
	// Name - The name of the skillset.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skillset.
	Description *string `json:"description,omitempty"`
	// Skills - A list of skills in the skillset.
	Skills *[]BasicIndexerSkill `json:"skills,omitempty"`
	// CognitiveServicesAccount - Details about the Azure AI service to be used when running skills.
	CognitiveServicesAccount BasicCognitiveServicesAccount `json:"cognitiveServices,omitempty"`
	// KnowledgeStore - Definition of additional projections to Azure blob, table, or files, of enriched data.
	KnowledgeStore *IndexerKnowledgeStore `json:"knowledgeStore,omitempty"`
	// IndexProjections - Definition of additional projections to secondary search index(es).
	IndexProjections *IndexerIndexProjections `json:"indexProjections,omitempty"`
	// ETag - The ETag of the skillset.
	ETag *string `json:"@odata.etag,omitempty"`
	// EncryptionKey - A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level of encryption-at-rest for your skillset definition when you want full assurance that no one, not even Microsoft, can decrypt your skillset definition. Once you have encrypted your skillset definition, it will always remain encrypted. The search service will ignore attempts to set this property to null. You can change this property as needed if you want to rotate your encryption key; Your skillset definition will be unaffected. Encryption with customer-managed keys is not available for free search services, and is only available for paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey `json:"encryptionKey,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for IndexerSkillset struct.
func (is *IndexerSkillset) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				is.Name = &name
			}
		case "description":
			if v != nil {
				var description string
				err = json.Unmarshal(*v, &description)
				if err != nil {
					return err
				}
				is.Description = &description
			}
		case "skills":
			if v != nil {
				skills, err := unmarshalBasicIndexerSkillArray(*v)
				if err != nil {
					return err
				}
				is.Skills = &skills
			}
		case "cognitiveServices":
			if v != nil {
				cognitiveServicesAccount, err := unmarshalBasicCognitiveServicesAccount(*v)
				if err != nil {
					return err
				}
				is.CognitiveServicesAccount = cognitiveServicesAccount
			}
		case "knowledgeStore":
			if v != nil {
				var knowledgeStore IndexerKnowledgeStore
				err = json.Unmarshal(*v, &knowledgeStore)
				if err != nil {
					return err
				}
				is.KnowledgeStore = &knowledgeStore
			}
		case "indexProjections":
			if v != nil {
				var indexProjections IndexerIndexProjections
				err = json.Unmarshal(*v, &indexProjections)
				if err != nil {
					return err
				}
				is.IndexProjections = &indexProjections
			}
		case "@odata.etag":
			if v != nil {
				var eTag string
				err = json.Unmarshal(*v, &eTag)
				if err != nil {
					return err
				}
				is.ETag = &eTag
			}
		case "encryptionKey":
			if v != nil {
				var encryptionKey ResourceEncryptionKey
				err = json.Unmarshal(*v, &encryptionKey)
				if err != nil {
					return err
				}
				is.EncryptionKey = &encryptionKey
			}
		}
	}

	return nil
}

// IndexerStatus represents the current status and execution history of an indexer.
type IndexerStatus struct {
	autorest.Response `json:"-"`
	// Status - READ-ONLY; Overall indexer status. Possible values include: 'IndexerStatusUnknown', 'IndexerStatusError', 'IndexerStatusRunning'
	Status IndexerStatus `json:"status,omitempty"`
	// LastResult - READ-ONLY; The result of the most recent or an in-progress indexer execution.
	LastResult *IndexerExecutionResult `json:"lastResult,omitempty"`
	// ExecutionHistory - READ-ONLY; History of the recent indexer executions, sorted in reverse chronological order.
	ExecutionHistory *[]IndexerExecutionResult `json:"executionHistory,omitempty"`
	// Limits - READ-ONLY; The execution limits for the indexer.
	Limits *IndexerLimits `json:"limits,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerStatus.
func (is IndexerStatus) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexerWarning represents an item-level warning.
type IndexerWarning struct {
	// Key - READ-ONLY; The key of the item which generated a warning.
	Key *string `json:"key,omitempty"`
	// Message - READ-ONLY; The message describing the warning that occurred while processing the item.
	Message *string `json:"message,omitempty"`
	// Name - READ-ONLY; The name of the source at which the warning originated. For example, this could refer to a particular skill in the attached skillset. This may not be always available.
	Name *string `json:"name,omitempty"`
	// Details - READ-ONLY; Additional, verbose details about the warning to assist in debugging the indexer. This may not be always available.
	Details *string `json:"details,omitempty"`
	// DocumentationLink - READ-ONLY; A link to a troubleshooting guide for these classes of warnings. This may not be always available.
	DocumentationLink *string `json:"documentationLink,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerWarning.
func (iw IndexerWarning) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexingParameters represents parameters for indexer execution.
type IndexingParameters struct {
	// BatchSize - The number of items that are read from the data source and indexed as a single batch in order to improve performance. The default depends on the data source type.
	BatchSize *int32 `json:"batchSize,omitempty"`
	// MaxFailedItems - The maximum number of items that can fail indexing for indexer execution to still be considered successful. -1 means no limit. Default is 0.
	MaxFailedItems *int32 `json:"maxFailedItems,omitempty"`
	// MaxFailedItemsPerBatch - The maximum number of items in a single batch that can fail indexing for the batch to still be considered successful. -1 means no limit. Default is 0.
	MaxFailedItemsPerBatch *int32                           `json:"maxFailedItemsPerBatch,omitempty"`
	Configuration          *IndexingParametersConfiguration `json:"configuration,omitempty"`
}

// IndexingParametersConfiguration a dictionary of indexer-specific configuration properties. Each name is
// the name of a specific property. Each value must be of a primitive type.
type IndexingParametersConfiguration struct {
	// AdditionalProperties - Unmatched properties from the message are deserialized this collection
	AdditionalProperties map[string]interface{} `json:""`
	// ParsingMode - Possible values include: 'BlobIndexerParsingModeDefault', 'BlobIndexerParsingModeText', 'BlobIndexerParsingModeDelimitedText', 'BlobIndexerParsingModeJSON', 'BlobIndexerParsingModeJSONArray', 'BlobIndexerParsingModeJSONLines'
	ParsingMode BlobIndexerParsingMode `json:"parsingMode,omitempty"`
	// ExcludedFileNameExtensions - Comma-delimited list of filename extensions to ignore when processing from Azure blob storage.  For example, you could exclude ".png, .mp4" to skip over those files during indexing.
	ExcludedFileNameExtensions *string `json:"excludedFileNameExtensions,omitempty"`
	// IndexedFileNameExtensions - Comma-delimited list of filename extensions to select when processing from Azure blob storage.  For example, you could focus indexing on specific application files ".docx, .pptx, .msg" to specifically include those file types.
	IndexedFileNameExtensions *string `json:"indexedFileNameExtensions,omitempty"`
	// FailOnUnsupportedContentType - For Azure blobs, set to false if you want to continue indexing when an unsupported content type is encountered, and you don't know all the content types (file extensions) in advance.
	FailOnUnsupportedContentType *bool `json:"failOnUnsupportedContentType,omitempty"`
	// FailOnUnprocessableDocument - For Azure blobs, set to false if you want to continue indexing if a document fails indexing.
	FailOnUnprocessableDocument *bool `json:"failOnUnprocessableDocument,omitempty"`
	// IndexStorageMetadataOnlyForOversizedDocuments - For Azure blobs, set this property to true to still index storage metadata for blob content that is too large to process. Oversized blobs are treated as errors by default. For limits on blob size, see https://docs.microsoft.com/azure/search/search-limits-quotas-capacity.
	IndexStorageMetadataOnlyForOversizedDocuments *bool `json:"indexStorageMetadataOnlyForOversizedDocuments,omitempty"`
	// DelimitedTextHeaders - For CSV blobs, specifies a comma-delimited list of column headers, useful for mapping source fields to destination fields in an index.
	DelimitedTextHeaders *string `json:"delimitedTextHeaders,omitempty"`
	// DelimitedTextDelimiter - For CSV blobs, specifies the end-of-line single-character delimiter for CSV files where each line starts a new document (for example, "|").
	DelimitedTextDelimiter *string `json:"delimitedTextDelimiter,omitempty"`
	// FirstLineContainsHeaders - For CSV blobs, indicates that the first (non-blank) line of each blob contains headers.
	FirstLineContainsHeaders *bool `json:"firstLineContainsHeaders,omitempty"`
	// DocumentRoot - For JSON arrays, given a structured or semi-structured document, you can specify a path to the array using this property.
	DocumentRoot *string `json:"documentRoot,omitempty"`
	// DataToExtract - Possible values include: 'BlobIndexerDataToExtractStorageMetadata', 'BlobIndexerDataToExtractAllMetadata', 'BlobIndexerDataToExtractContentAndMetadata'
	DataToExtract BlobIndexerDataToExtract `json:"dataToExtract,omitempty"`
	// ImageAction - Possible values include: 'BlobIndexerImageActionNone', 'BlobIndexerImageActionGenerateNormalizedImages', 'BlobIndexerImageActionGenerateNormalizedImagePerPage'
	ImageAction BlobIndexerImageAction `json:"imageAction,omitempty"`
	// AllowSkillsetToReadFileData - If true, will create a path //document//file_data that is an object representing the original file data downloaded from your blob data source.  This allows you to pass the original file data to a custom skill for processing within the enrichment pipeline, or to the Document Extraction skill.
	AllowSkillsetToReadFileData *bool `json:"allowSkillsetToReadFileData,omitempty"`
	// PdfTextRotationAlgorithm - Possible values include: 'BlobIndexerPDFTextRotationAlgorithmNone', 'BlobIndexerPDFTextRotationAlgorithmDetectAngles'
	PdfTextRotationAlgorithm BlobIndexerPDFTextRotationAlgorithm `json:"pdfTextRotationAlgorithm,omitempty"`
	// ExecutionEnvironment - Possible values include: 'IndexerExecutionEnvironmentStandard', 'IndexerExecutionEnvironmentPrivate'
	ExecutionEnvironment IndexerExecutionEnvironment `json:"executionEnvironment,omitempty"`
	// QueryTimeout - Increases the timeout beyond the 5-minute default for Azure SQL database data sources, specified in the format "hh:mm:ss".
	QueryTimeout *string `json:"queryTimeout,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexingParametersConfiguration.
func (ipc IndexingParametersConfiguration) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if ipc.ParsingMode != "" {
		objectMap["parsingMode"] = ipc.ParsingMode
	}
	if ipc.ExcludedFileNameExtensions != nil {
		objectMap["excludedFileNameExtensions"] = ipc.ExcludedFileNameExtensions
	}
	if ipc.IndexedFileNameExtensions != nil {
		objectMap["indexedFileNameExtensions"] = ipc.IndexedFileNameExtensions
	}
	if ipc.FailOnUnsupportedContentType != nil {
		objectMap["failOnUnsupportedContentType"] = ipc.FailOnUnsupportedContentType
	}
	if ipc.FailOnUnprocessableDocument != nil {
		objectMap["failOnUnprocessableDocument"] = ipc.FailOnUnprocessableDocument
	}
	if ipc.IndexStorageMetadataOnlyForOversizedDocuments != nil {
		objectMap["indexStorageMetadataOnlyForOversizedDocuments"] = ipc.IndexStorageMetadataOnlyForOversizedDocuments
	}
	if ipc.DelimitedTextHeaders != nil {
		objectMap["delimitedTextHeaders"] = ipc.DelimitedTextHeaders
	}
	if ipc.DelimitedTextDelimiter != nil {
		objectMap["delimitedTextDelimiter"] = ipc.DelimitedTextDelimiter
	}
	if ipc.FirstLineContainsHeaders != nil {
		objectMap["firstLineContainsHeaders"] = ipc.FirstLineContainsHeaders
	}
	if ipc.DocumentRoot != nil {
		objectMap["documentRoot"] = ipc.DocumentRoot
	}
	if ipc.DataToExtract != "" {
		objectMap["dataToExtract"] = ipc.DataToExtract
	}
	if ipc.ImageAction != "" {
		objectMap["imageAction"] = ipc.ImageAction
	}
	if ipc.AllowSkillsetToReadFileData != nil {
		objectMap["allowSkillsetToReadFileData"] = ipc.AllowSkillsetToReadFileData
	}
	if ipc.PdfTextRotationAlgorithm != "" {
		objectMap["pdfTextRotationAlgorithm"] = ipc.PdfTextRotationAlgorithm
	}
	if ipc.ExecutionEnvironment != "" {
		objectMap["executionEnvironment"] = ipc.ExecutionEnvironment
	}
	if ipc.QueryTimeout != nil {
		objectMap["queryTimeout"] = ipc.QueryTimeout
	}
	for k, v := range ipc.AdditionalProperties {
		objectMap[k] = v
	}
	return json.Marshal(objectMap)
}

// UnmarshalJSON is the custom unmarshaler for IndexingParametersConfiguration struct.
func (ipc *IndexingParametersConfiguration) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		default:
			if v != nil {
				var additionalProperties interface{}
				err = json.Unmarshal(*v, &additionalProperties)
				if err != nil {
					return err
				}
				if ipc.AdditionalProperties == nil {
					ipc.AdditionalProperties = make(map[string]interface{})
				}
				ipc.AdditionalProperties[k] = additionalProperties
			}
		case "parsingMode":
			if v != nil {
				var parsingMode BlobIndexerParsingMode
				err = json.Unmarshal(*v, &parsingMode)
				if err != nil {
					return err
				}
				ipc.ParsingMode = parsingMode
			}
		case "excludedFileNameExtensions":
			if v != nil {
				var excludedFileNameExtensions string
				err = json.Unmarshal(*v, &excludedFileNameExtensions)
				if err != nil {
					return err
				}
				ipc.ExcludedFileNameExtensions = &excludedFileNameExtensions
			}
		case "indexedFileNameExtensions":
			if v != nil {
				var indexedFileNameExtensions string
				err = json.Unmarshal(*v, &indexedFileNameExtensions)
				if err != nil {
					return err
				}
				ipc.IndexedFileNameExtensions = &indexedFileNameExtensions
			}
		case "failOnUnsupportedContentType":
			if v != nil {
				var failOnUnsupportedContentType bool
				err = json.Unmarshal(*v, &failOnUnsupportedContentType)
				if err != nil {
					return err
				}
				ipc.FailOnUnsupportedContentType = &failOnUnsupportedContentType
			}
		case "failOnUnprocessableDocument":
			if v != nil {
				var failOnUnprocessableDocument bool
				err = json.Unmarshal(*v, &failOnUnprocessableDocument)
				if err != nil {
					return err
				}
				ipc.FailOnUnprocessableDocument = &failOnUnprocessableDocument
			}
		case "indexStorageMetadataOnlyForOversizedDocuments":
			if v != nil {
				var indexStorageMetadataOnlyForOversizedDocuments bool
				err = json.Unmarshal(*v, &indexStorageMetadataOnlyForOversizedDocuments)
				if err != nil {
					return err
				}
				ipc.IndexStorageMetadataOnlyForOversizedDocuments = &indexStorageMetadataOnlyForOversizedDocuments
			}
		case "delimitedTextHeaders":
			if v != nil {
				var delimitedTextHeaders string
				err = json.Unmarshal(*v, &delimitedTextHeaders)
				if err != nil {
					return err
				}
				ipc.DelimitedTextHeaders = &delimitedTextHeaders
			}
		case "delimitedTextDelimiter":
			if v != nil {
				var delimitedTextDelimiter string
				err = json.Unmarshal(*v, &delimitedTextDelimiter)
				if err != nil {
					return err
				}
				ipc.DelimitedTextDelimiter = &delimitedTextDelimiter
			}
		case "firstLineContainsHeaders":
			if v != nil {
				var firstLineContainsHeaders bool
				err = json.Unmarshal(*v, &firstLineContainsHeaders)
				if err != nil {
					return err
				}
				ipc.FirstLineContainsHeaders = &firstLineContainsHeaders
			}
		case "documentRoot":
			if v != nil {
				var documentRoot string
				err = json.Unmarshal(*v, &documentRoot)
				if err != nil {
					return err
				}
				ipc.DocumentRoot = &documentRoot
			}
		case "dataToExtract":
			if v != nil {
				var dataToExtract BlobIndexerDataToExtract
				err = json.Unmarshal(*v, &dataToExtract)
				if err != nil {
					return err
				}
				ipc.DataToExtract = dataToExtract
			}
		case "imageAction":
			if v != nil {
				var imageAction BlobIndexerImageAction
				err = json.Unmarshal(*v, &imageAction)
				if err != nil {
					return err
				}
				ipc.ImageAction = imageAction
			}
		case "allowSkillsetToReadFileData":
			if v != nil {
				var allowSkillsetToReadFileData bool
				err = json.Unmarshal(*v, &allowSkillsetToReadFileData)
				if err != nil {
					return err
				}
				ipc.AllowSkillsetToReadFileData = &allowSkillsetToReadFileData
			}
		case "pdfTextRotationAlgorithm":
			if v != nil {
				var pdfTextRotationAlgorithm BlobIndexerPDFTextRotationAlgorithm
				err = json.Unmarshal(*v, &pdfTextRotationAlgorithm)
				if err != nil {
					return err
				}
				ipc.PdfTextRotationAlgorithm = pdfTextRotationAlgorithm
			}
		case "executionEnvironment":
			if v != nil {
				var executionEnvironment IndexerExecutionEnvironment
				err = json.Unmarshal(*v, &executionEnvironment)
				if err != nil {
					return err
				}
				ipc.ExecutionEnvironment = executionEnvironment
			}
		case "queryTimeout":
			if v != nil {
				var queryTimeout string
				err = json.Unmarshal(*v, &queryTimeout)
				if err != nil {
					return err
				}
				ipc.QueryTimeout = &queryTimeout
			}
		}
	}

	return nil
}

// IndexingSchedule represents a schedule for indexer execution.
type IndexingSchedule struct {
	// Interval - The interval of time between indexer executions.
	Interval *string `json:"interval,omitempty"`
	// StartTime - The time when an indexer should start running.
	StartTime *date.Time `json:"startTime,omitempty"`
}

// InputFieldMappingEntry input field mapping for a skill.
type InputFieldMappingEntry struct {
	// Name - The name of the input.
	Name *string `json:"name,omitempty"`
	// Source - The source of the input.
	Source *string `json:"source,omitempty"`
	// SourceContext - The source context used for selecting recursive inputs.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Inputs - The recursive inputs used when creating a complex type.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
}

// KeepTokenFilter a token filter that only keeps tokens with text contained in a specified list of words.
// This token filter is implemented using Apache Lucene.
type KeepTokenFilter struct {
	// KeepWords - The list of words to keep.
	KeepWords *[]string `json:"keepWords,omitempty"`
	// LowerCaseKeepWords - A value indicating whether to lower case all words first. Default is false.
	LowerCaseKeepWords *bool `json:"keepWordsCase,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeepTokenFilter.
func (ktf KeepTokenFilter) MarshalJSON() ([]byte, error) {
	ktf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter
	objectMap := make(map[string]interface{})
	if ktf.KeepWords != nil {
		objectMap["keepWords"] = ktf.KeepWords
	}
	if ktf.LowerCaseKeepWords != nil {
		objectMap["keepWordsCase"] = ktf.LowerCaseKeepWords
	}
	if ktf.Name != nil {
		objectMap["name"] = ktf.Name
	}
	if ktf.OdataType != "" {
		objectMap["@odata.type"] = ktf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return &ktf, true
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ktf, true
}

// KeyPhraseExtractionSkill a skill that uses text analytics for key phrase extraction.
type KeyPhraseExtractionSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`. Possible values include: 'KeyPhraseExtractionSkillLanguageDa', 'KeyPhraseExtractionSkillLanguageNl', 'KeyPhraseExtractionSkillLanguageEn', 'KeyPhraseExtractionSkillLanguageFi', 'KeyPhraseExtractionSkillLanguageFr', 'KeyPhraseExtractionSkillLanguageDe', 'KeyPhraseExtractionSkillLanguageIt', 'KeyPhraseExtractionSkillLanguageJa', 'KeyPhraseExtractionSkillLanguageKo', 'KeyPhraseExtractionSkillLanguageNo', 'KeyPhraseExtractionSkillLanguagePl', 'KeyPhraseExtractionSkillLanguagePtPT', 'KeyPhraseExtractionSkillLanguagePtBR', 'KeyPhraseExtractionSkillLanguageRu', 'KeyPhraseExtractionSkillLanguageEs', 'KeyPhraseExtractionSkillLanguageSv'
	DefaultLanguageCode KeyPhraseExtractionSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// MaxKeyPhraseCount - A number indicating how many key phrases to return. If absent, all identified key phrases will be returned.
	MaxKeyPhraseCount *int32 `json:"maxKeyPhraseCount,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics service. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) MarshalJSON() ([]byte, error) {
	kpes.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill
	objectMap := make(map[string]interface{})
	if kpes.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = kpes.DefaultLanguageCode
	}
	if kpes.MaxKeyPhraseCount != nil {
		objectMap["maxKeyPhraseCount"] = kpes.MaxKeyPhraseCount
	}
	if kpes.ModelVersion != nil {
		objectMap["modelVersion"] = kpes.ModelVersion
	}
	if kpes.Name != nil {
		objectMap["name"] = kpes.Name
	}
	if kpes.Description != nil {
		objectMap["description"] = kpes.Description
	}
	if kpes.Context != nil {
		objectMap["context"] = kpes.Context
	}
	if kpes.Inputs != nil {
		objectMap["inputs"] = kpes.Inputs
	}
	if kpes.Outputs != nil {
		objectMap["outputs"] = kpes.Outputs
	}
	if kpes.OdataType != "" {
		objectMap["@odata.type"] = kpes.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return &kpes, true
}

// AsOcrSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &kpes, true
}

// KeysOrIds ...
type KeysOrIds struct {
	// DocumentKeys - document keys to be reset
	DocumentKeys *[]string `json:"documentKeys,omitempty"`
	// DatasourceDocumentIds - datasource document identifiers to be reset
	DatasourceDocumentIds *[]string `json:"datasourceDocumentIds,omitempty"`
}

// KeywordMarkerTokenFilter marks terms as keywords. This token filter is implemented using Apache Lucene.
type KeywordMarkerTokenFilter struct {
	// Keywords - A list of words to mark as keywords.
	Keywords *[]string `json:"keywords,omitempty"`
	// IgnoreCase - A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) MarshalJSON() ([]byte, error) {
	kmtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter
	objectMap := make(map[string]interface{})
	if kmtf.Keywords != nil {
		objectMap["keywords"] = kmtf.Keywords
	}
	if kmtf.IgnoreCase != nil {
		objectMap["ignoreCase"] = kmtf.IgnoreCase
	}
	if kmtf.Name != nil {
		objectMap["name"] = kmtf.Name
	}
	if kmtf.OdataType != "" {
		objectMap["@odata.type"] = kmtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return &kmtf, true
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &kmtf, true
}

// KeywordTokenizer emits the entire input as a single token. This tokenizer is implemented using Apache
// Lucene.
type KeywordTokenizer struct {
	// BufferSize - The read buffer size in bytes. Default is 256.
	BufferSize *int32 `json:"bufferSize,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeywordTokenizer.
func (kt KeywordTokenizer) MarshalJSON() ([]byte, error) {
	kt.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer
	objectMap := make(map[string]interface{})
	if kt.BufferSize != nil {
		objectMap["bufferSize"] = kt.BufferSize
	}
	if kt.Name != nil {
		objectMap["name"] = kt.Name
	}
	if kt.OdataType != "" {
		objectMap["@odata.type"] = kt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return &kt, true
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &kt, true
}

// KeywordTokenizerV2 emits the entire input as a single token. This tokenizer is implemented using Apache
// Lucene.
type KeywordTokenizerV2 struct {
	// MaxTokenLength - The maximum token length. Default is 256. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) MarshalJSON() ([]byte, error) {
	ktv.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2
	objectMap := make(map[string]interface{})
	if ktv.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = ktv.MaxTokenLength
	}
	if ktv.Name != nil {
		objectMap["name"] = ktv.Name
	}
	if ktv.OdataType != "" {
		objectMap["@odata.type"] = ktv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return &ktv, true
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &ktv, true
}

// LanguageDetectionSkill a skill that detects the language of input text and reports a single language
// code for every document submitted on the request. The language code is paired with a score indicating
// the confidence of the analysis.
type LanguageDetectionSkill struct {
	// DefaultCountryHint - A country code to use as a hint to the language detection model if it cannot disambiguate the language.
	DefaultCountryHint *string `json:"defaultCountryHint,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics service. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) MarshalJSON() ([]byte, error) {
	lds.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill
	objectMap := make(map[string]interface{})
	if lds.DefaultCountryHint != nil {
		objectMap["defaultCountryHint"] = lds.DefaultCountryHint
	}
	if lds.ModelVersion != nil {
		objectMap["modelVersion"] = lds.ModelVersion
	}
	if lds.Name != nil {
		objectMap["name"] = lds.Name
	}
	if lds.Description != nil {
		objectMap["description"] = lds.Description
	}
	if lds.Context != nil {
		objectMap["context"] = lds.Context
	}
	if lds.Inputs != nil {
		objectMap["inputs"] = lds.Inputs
	}
	if lds.Outputs != nil {
		objectMap["outputs"] = lds.Outputs
	}
	if lds.OdataType != "" {
		objectMap["@odata.type"] = lds.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return &lds, true
}

// AsShaperSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &lds, true
}

// LengthTokenFilter removes words that are too long or too short. This token filter is implemented using
// Apache Lucene.
type LengthTokenFilter struct {
	// MinLength - The minimum length in characters. Default is 0. Maximum is 300. Must be less than the value of max.
	MinLength *int32 `json:"min,omitempty"`
	// MaxLength - The maximum length in characters. Default and maximum is 300.
	MaxLength *int32 `json:"max,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LengthTokenFilter.
func (ltf LengthTokenFilter) MarshalJSON() ([]byte, error) {
	ltf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter
	objectMap := make(map[string]interface{})
	if ltf.MinLength != nil {
		objectMap["min"] = ltf.MinLength
	}
	if ltf.MaxLength != nil {
		objectMap["max"] = ltf.MaxLength
	}
	if ltf.Name != nil {
		objectMap["name"] = ltf.Name
	}
	if ltf.OdataType != "" {
		objectMap["@odata.type"] = ltf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return &ltf, true
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ltf, true
}

// BasicLexicalAnalyzer base type for analyzers.
type BasicLexicalAnalyzer interface {
	AsCustomAnalyzer() (*CustomAnalyzer, bool)
	AsPatternAnalyzer() (*PatternAnalyzer, bool)
	AsLuceneStandardAnalyzer() (*LuceneStandardAnalyzer, bool)
	AsStopAnalyzer() (*StopAnalyzer, bool)
	AsLexicalAnalyzer() (*LexicalAnalyzer, bool)
}

// LexicalAnalyzer base type for analyzers.
type LexicalAnalyzer struct {
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeLexicalAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

func unmarshalBasicLexicalAnalyzer(body []byte) (BasicLexicalAnalyzer, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchCustomAnalyzer):
		var ca CustomAnalyzer
		err := json.Unmarshal(body, &ca)
		return ca, err
	case string(OdataTypeMicrosoftAzureSearchPatternAnalyzer):
		var pa PatternAnalyzer
		err := json.Unmarshal(body, &pa)
		return pa, err
	case string(OdataTypeMicrosoftAzureSearchStandardAnalyzer):
		var lsa LuceneStandardAnalyzer
		err := json.Unmarshal(body, &lsa)
		return lsa, err
	case string(OdataTypeMicrosoftAzureSearchStopAnalyzer):
		var sa StopAnalyzer
		err := json.Unmarshal(body, &sa)
		return sa, err
	default:
		var la LexicalAnalyzer
		err := json.Unmarshal(body, &la)
		return la, err
	}
}
func unmarshalBasicLexicalAnalyzerArray(body []byte) ([]BasicLexicalAnalyzer, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	laArray := make([]BasicLexicalAnalyzer, len(rawMessages))

	for index, rawMessage := range rawMessages {
		la, err := unmarshalBasicLexicalAnalyzer(*rawMessage)
		if err != nil {
			return nil, err
		}
		laArray[index] = la
	}
	return laArray, nil
}

// MarshalJSON is the custom marshaler for LexicalAnalyzer.
func (la LexicalAnalyzer) MarshalJSON() ([]byte, error) {
	la.OdataType = OdataTypeLexicalAnalyzer
	objectMap := make(map[string]interface{})
	if la.Name != nil {
		objectMap["name"] = la.Name
	}
	if la.OdataType != "" {
		objectMap["@odata.type"] = la.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicLexicalAnalyzer implementation for LexicalAnalyzer.
func (la LexicalAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicLexicalAnalyzer implementation for LexicalAnalyzer.
func (la LexicalAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsLuceneStandardAnalyzer is the BasicLexicalAnalyzer implementation for LexicalAnalyzer.
func (la LexicalAnalyzer) AsLuceneStandardAnalyzer() (*LuceneStandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicLexicalAnalyzer implementation for LexicalAnalyzer.
func (la LexicalAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsLexicalAnalyzer is the BasicLexicalAnalyzer implementation for LexicalAnalyzer.
func (la LexicalAnalyzer) AsLexicalAnalyzer() (*LexicalAnalyzer, bool) {
	return &la, true
}

// AsBasicLexicalAnalyzer is the BasicLexicalAnalyzer implementation for LexicalAnalyzer.
func (la LexicalAnalyzer) AsBasicLexicalAnalyzer() (BasicLexicalAnalyzer, bool) {
	return &la, true
}

// BasicLexicalNormalizer base type for normalizers.
type BasicLexicalNormalizer interface {
	AsCustomNormalizer() (*CustomNormalizer, bool)
	AsLexicalNormalizer() (*LexicalNormalizer, bool)
}

// LexicalNormalizer base type for normalizers.
type LexicalNormalizer struct {
	// Name - The name of the normalizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters. It cannot end in '.microsoft' nor '.lucene', nor be named 'asciifolding', 'standard', 'lowercase', 'uppercase', or 'elision'.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalNormalizerOdataTypeLexicalNormalizer', 'OdataTypeBasicLexicalNormalizerOdataTypeMicrosoftAzureSearchCustomNormalizer'
	OdataType OdataTypeBasicLexicalNormalizer `json:"@odata.type,omitempty"`
}

func unmarshalBasicLexicalNormalizer(body []byte) (BasicLexicalNormalizer, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicLexicalNormalizerOdataTypeMicrosoftAzureSearchCustomNormalizer):
		var cn CustomNormalizer
		err := json.Unmarshal(body, &cn)
		return cn, err
	default:
		var ln LexicalNormalizer
		err := json.Unmarshal(body, &ln)
		return ln, err
	}
}
func unmarshalBasicLexicalNormalizerArray(body []byte) ([]BasicLexicalNormalizer, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	lnArray := make([]BasicLexicalNormalizer, len(rawMessages))

	for index, rawMessage := range rawMessages {
		ln, err := unmarshalBasicLexicalNormalizer(*rawMessage)
		if err != nil {
			return nil, err
		}
		lnArray[index] = ln
	}
	return lnArray, nil
}

// MarshalJSON is the custom marshaler for LexicalNormalizer.
func (ln LexicalNormalizer) MarshalJSON() ([]byte, error) {
	ln.OdataType = OdataTypeBasicLexicalNormalizerOdataTypeLexicalNormalizer
	objectMap := make(map[string]interface{})
	if ln.Name != nil {
		objectMap["name"] = ln.Name
	}
	if ln.OdataType != "" {
		objectMap["@odata.type"] = ln.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomNormalizer is the BasicLexicalNormalizer implementation for LexicalNormalizer.
func (ln LexicalNormalizer) AsCustomNormalizer() (*CustomNormalizer, bool) {
	return nil, false
}

// AsLexicalNormalizer is the BasicLexicalNormalizer implementation for LexicalNormalizer.
func (ln LexicalNormalizer) AsLexicalNormalizer() (*LexicalNormalizer, bool) {
	return &ln, true
}

// AsBasicLexicalNormalizer is the BasicLexicalNormalizer implementation for LexicalNormalizer.
func (ln LexicalNormalizer) AsBasicLexicalNormalizer() (BasicLexicalNormalizer, bool) {
	return &ln, true
}

// BasicLexicalTokenizer base type for tokenizers.
type BasicLexicalTokenizer interface {
	AsClassicTokenizer() (*ClassicTokenizer, bool)
	AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool)
	AsKeywordTokenizer() (*KeywordTokenizer, bool)
	AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool)
	AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool)
	AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool)
	AsNGramTokenizer() (*NGramTokenizer, bool)
	AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool)
	AsPatternTokenizer() (*PatternTokenizer, bool)
	AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool)
	AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool)
	AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool)
	AsLexicalTokenizer() (*LexicalTokenizer, bool)
}

// LexicalTokenizer base type for tokenizers.
type LexicalTokenizer struct {
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

func unmarshalBasicLexicalTokenizer(body []byte) (BasicLexicalTokenizer, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer):
		var ct ClassicTokenizer
		err := json.Unmarshal(body, &ct)
		return ct, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer):
		var engt EdgeNGramTokenizer
		err := json.Unmarshal(body, &engt)
		return engt, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer):
		var kt KeywordTokenizer
		err := json.Unmarshal(body, &kt)
		return kt, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2):
		var ktv KeywordTokenizerV2
		err := json.Unmarshal(body, &ktv)
		return ktv, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer):
		var mlt MicrosoftLanguageTokenizer
		err := json.Unmarshal(body, &mlt)
		return mlt, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer):
		var mlst MicrosoftLanguageStemmingTokenizer
		err := json.Unmarshal(body, &mlst)
		return mlst, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer):
		var ngt NGramTokenizer
		err := json.Unmarshal(body, &ngt)
		return ngt, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2):
		var phtv PathHierarchyTokenizerV2
		err := json.Unmarshal(body, &phtv)
		return phtv, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer):
		var pt PatternTokenizer
		err := json.Unmarshal(body, &pt)
		return pt, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer):
		var lst LuceneStandardTokenizer
		err := json.Unmarshal(body, &lst)
		return lst, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2):
		var lstv LuceneStandardTokenizerV2
		err := json.Unmarshal(body, &lstv)
		return lstv, err
	case string(OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer):
		var uuet UaxURLEmailTokenizer
		err := json.Unmarshal(body, &uuet)
		return uuet, err
	default:
		var lt LexicalTokenizer
		err := json.Unmarshal(body, &lt)
		return lt, err
	}
}
func unmarshalBasicLexicalTokenizerArray(body []byte) ([]BasicLexicalTokenizer, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	ltArray := make([]BasicLexicalTokenizer, len(rawMessages))

	for index, rawMessage := range rawMessages {
		lt, err := unmarshalBasicLexicalTokenizer(*rawMessage)
		if err != nil {
			return nil, err
		}
		ltArray[index] = lt
	}
	return ltArray, nil
}

// MarshalJSON is the custom marshaler for LexicalTokenizer.
func (lt LexicalTokenizer) MarshalJSON() ([]byte, error) {
	lt.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer
	objectMap := make(map[string]interface{})
	if lt.Name != nil {
		objectMap["name"] = lt.Name
	}
	if lt.OdataType != "" {
		objectMap["@odata.type"] = lt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return &lt, true
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for LexicalTokenizer.
func (lt LexicalTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &lt, true
}

// LimitTokenFilter limits the number of tokens while indexing. This token filter is implemented using
// Apache Lucene.
type LimitTokenFilter struct {
	// MaxTokenCount - The maximum number of tokens to produce. Default is 1.
	MaxTokenCount *int32 `json:"maxTokenCount,omitempty"`
	// ConsumeAllTokens - A value indicating whether all tokens from the input must be consumed even if maxTokenCount is reached. Default is false.
	ConsumeAllTokens *bool `json:"consumeAllTokens,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LimitTokenFilter.
func (ltf LimitTokenFilter) MarshalJSON() ([]byte, error) {
	ltf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter
	objectMap := make(map[string]interface{})
	if ltf.MaxTokenCount != nil {
		objectMap["maxTokenCount"] = ltf.MaxTokenCount
	}
	if ltf.ConsumeAllTokens != nil {
		objectMap["consumeAllTokens"] = ltf.ConsumeAllTokens
	}
	if ltf.Name != nil {
		objectMap["name"] = ltf.Name
	}
	if ltf.OdataType != "" {
		objectMap["@odata.type"] = ltf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return &ltf, true
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ltf, true
}

// ListAliasesResult response from a List Aliases request. If successful, it includes the associated index
// mappings for all aliases.
type ListAliasesResult struct {
	autorest.Response `json:"-"`
	// Aliases - READ-ONLY; The aliases in the Search service.
	Aliases *[]Alias `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListAliasesResult.
func (lar ListAliasesResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListDataSourcesResult response from a List Datasources request. If successful, it includes the full
// definitions of all datasources.
type ListDataSourcesResult struct {
	autorest.Response `json:"-"`
	// DataSources - READ-ONLY; The datasources in the Search service.
	DataSources *[]IndexerDataSource `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListDataSourcesResult.
func (ldsr ListDataSourcesResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListIndexersResult response from a List Indexers request. If successful, it includes the full
// definitions of all indexers.
type ListIndexersResult struct {
	autorest.Response `json:"-"`
	// Indexers - READ-ONLY; The indexers in the Search service.
	Indexers *[]Indexer `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListIndexersResult.
func (lir ListIndexersResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListIndexesResult response from a List Indexes request. If successful, it includes the full definitions
// of all indexes.
type ListIndexesResult struct {
	autorest.Response `json:"-"`
	// Indexes - READ-ONLY; The indexes in the Search service.
	Indexes *[]Index `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListIndexesResult.
func (lir ListIndexesResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListSkillsetsResult response from a list skillset request. If successful, it includes the full
// definitions of all skillsets.
type ListSkillsetsResult struct {
	autorest.Response `json:"-"`
	// Skillsets - READ-ONLY; The skillsets defined in the Search service.
	Skillsets *[]IndexerSkillset `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListSkillsetsResult.
func (lsr ListSkillsetsResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListSynonymMapsResult response from a List SynonymMaps request. If successful, it includes the full
// definitions of all synonym maps.
type ListSynonymMapsResult struct {
	autorest.Response `json:"-"`
	// SynonymMaps - READ-ONLY; The synonym maps in the Search service.
	SynonymMaps *[]SynonymMap `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListSynonymMapsResult.
func (lsmr ListSynonymMapsResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// LuceneStandardAnalyzer standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase
// filter and stop filter.
type LuceneStandardAnalyzer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Stopwords - A list of stopwords.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeLexicalAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LuceneStandardAnalyzer.
func (lsa LuceneStandardAnalyzer) MarshalJSON() ([]byte, error) {
	lsa.OdataType = OdataTypeMicrosoftAzureSearchStandardAnalyzer
	objectMap := make(map[string]interface{})
	if lsa.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = lsa.MaxTokenLength
	}
	if lsa.Stopwords != nil {
		objectMap["stopwords"] = lsa.Stopwords
	}
	if lsa.Name != nil {
		objectMap["name"] = lsa.Name
	}
	if lsa.OdataType != "" {
		objectMap["@odata.type"] = lsa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicLexicalAnalyzer implementation for LuceneStandardAnalyzer.
func (lsa LuceneStandardAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicLexicalAnalyzer implementation for LuceneStandardAnalyzer.
func (lsa LuceneStandardAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsLuceneStandardAnalyzer is the BasicLexicalAnalyzer implementation for LuceneStandardAnalyzer.
func (lsa LuceneStandardAnalyzer) AsLuceneStandardAnalyzer() (*LuceneStandardAnalyzer, bool) {
	return &lsa, true
}

// AsStopAnalyzer is the BasicLexicalAnalyzer implementation for LuceneStandardAnalyzer.
func (lsa LuceneStandardAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsLexicalAnalyzer is the BasicLexicalAnalyzer implementation for LuceneStandardAnalyzer.
func (lsa LuceneStandardAnalyzer) AsLexicalAnalyzer() (*LexicalAnalyzer, bool) {
	return nil, false
}

// AsBasicLexicalAnalyzer is the BasicLexicalAnalyzer implementation for LuceneStandardAnalyzer.
func (lsa LuceneStandardAnalyzer) AsBasicLexicalAnalyzer() (BasicLexicalAnalyzer, bool) {
	return &lsa, true
}

// LuceneStandardTokenizer breaks text following the Unicode Text Segmentation rules. This tokenizer is
// implemented using Apache Lucene.
type LuceneStandardTokenizer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) MarshalJSON() ([]byte, error) {
	lst.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer
	objectMap := make(map[string]interface{})
	if lst.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = lst.MaxTokenLength
	}
	if lst.Name != nil {
		objectMap["name"] = lst.Name
	}
	if lst.OdataType != "" {
		objectMap["@odata.type"] = lst.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return &lst, true
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizer.
func (lst LuceneStandardTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &lst, true
}

// LuceneStandardTokenizerV2 breaks text following the Unicode Text Segmentation rules. This tokenizer is
// implemented using Apache Lucene.
type LuceneStandardTokenizerV2 struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) MarshalJSON() ([]byte, error) {
	lstv.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2
	objectMap := make(map[string]interface{})
	if lstv.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = lstv.MaxTokenLength
	}
	if lstv.Name != nil {
		objectMap["name"] = lstv.Name
	}
	if lstv.OdataType != "" {
		objectMap["@odata.type"] = lstv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return &lstv, true
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for LuceneStandardTokenizerV2.
func (lstv LuceneStandardTokenizerV2) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &lstv, true
}

// MagnitudeScoringFunction defines a function that boosts scores based on the magnitude of a numeric
// field.
type MagnitudeScoringFunction struct {
	// Parameters - Parameter values for the magnitude scoring function.
	Parameters *MagnitudeScoringParameters `json:"magnitude,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'ScoringFunctionInterpolationLinear', 'ScoringFunctionInterpolationConstant', 'ScoringFunctionInterpolationQuadratic', 'ScoringFunctionInterpolationLogarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) MarshalJSON() ([]byte, error) {
	msf.Type = TypeMagnitude
	objectMap := make(map[string]interface{})
	if msf.Parameters != nil {
		objectMap["magnitude"] = msf.Parameters
	}
	if msf.FieldName != nil {
		objectMap["fieldName"] = msf.FieldName
	}
	if msf.Boost != nil {
		objectMap["boost"] = msf.Boost
	}
	if msf.Interpolation != "" {
		objectMap["interpolation"] = msf.Interpolation
	}
	if msf.Type != "" {
		objectMap["type"] = msf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return &msf, true
}

// AsTagScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &msf, true
}

// MagnitudeScoringParameters provides parameter values to a magnitude scoring function.
type MagnitudeScoringParameters struct {
	// BoostingRangeStart - The field value at which boosting starts.
	BoostingRangeStart *float64 `json:"boostingRangeStart,omitempty"`
	// BoostingRangeEnd - The field value at which boosting ends.
	BoostingRangeEnd *float64 `json:"boostingRangeEnd,omitempty"`
	// ShouldBoostBeyondRangeByConstant - A value indicating whether to apply a constant boost for field values beyond the range end value; default is false.
	ShouldBoostBeyondRangeByConstant *bool `json:"constantBoostBeyondRange,omitempty"`
}

// MappingCharFilter a character filter that applies mappings defined with the mappings option. Matching is
// greedy (longest pattern matching at a given point wins). Replacement is allowed to be the empty string.
// This character filter is implemented using Apache Lucene.
type MappingCharFilter struct {
	// Mappings - A list of mappings of the following format: "a=>b" (all occurrences of the character "a" will be replaced with character "b").
	Mappings *[]string `json:"mappings,omitempty"`
	// Name - The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicCharFilterOdataTypeCharFilter', 'OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchMappingCharFilter', 'OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchPatternReplaceCharFilter'
	OdataType OdataTypeBasicCharFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MappingCharFilter.
func (mcf MappingCharFilter) MarshalJSON() ([]byte, error) {
	mcf.OdataType = OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchMappingCharFilter
	objectMap := make(map[string]interface{})
	if mcf.Mappings != nil {
		objectMap["mappings"] = mcf.Mappings
	}
	if mcf.Name != nil {
		objectMap["name"] = mcf.Name
	}
	if mcf.OdataType != "" {
		objectMap["@odata.type"] = mcf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsMappingCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsMappingCharFilter() (*MappingCharFilter, bool) {
	return &mcf, true
}

// AsPatternReplaceCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool) {
	return nil, false
}

// AsCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsCharFilter() (*CharFilter, bool) {
	return nil, false
}

// AsBasicCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsBasicCharFilter() (BasicCharFilter, bool) {
	return &mcf, true
}

// MergeSkill a skill for merging two or more strings into a single unified string, with an optional
// user-defined delimiter separating each component part.
type MergeSkill struct {
	// InsertPreTag - The tag indicates the start of the merged text. By default, the tag is an empty space.
	InsertPreTag *string `json:"insertPreTag,omitempty"`
	// InsertPostTag - The tag indicates the end of the merged text. By default, the tag is an empty space.
	InsertPostTag *string `json:"insertPostTag,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MergeSkill.
func (ms MergeSkill) MarshalJSON() ([]byte, error) {
	ms.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill
	objectMap := make(map[string]interface{})
	if ms.InsertPreTag != nil {
		objectMap["insertPreTag"] = ms.InsertPreTag
	}
	if ms.InsertPostTag != nil {
		objectMap["insertPostTag"] = ms.InsertPostTag
	}
	if ms.Name != nil {
		objectMap["name"] = ms.Name
	}
	if ms.Description != nil {
		objectMap["description"] = ms.Description
	}
	if ms.Context != nil {
		objectMap["context"] = ms.Context
	}
	if ms.Inputs != nil {
		objectMap["inputs"] = ms.Inputs
	}
	if ms.Outputs != nil {
		objectMap["outputs"] = ms.Outputs
	}
	if ms.OdataType != "" {
		objectMap["@odata.type"] = ms.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsMergeSkill() (*MergeSkill, bool) {
	return &ms, true
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for MergeSkill.
func (ms MergeSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ms, true
}

// MicrosoftLanguageStemmingTokenizer divides text using language-specific rules and reduces words to their
// base forms.
type MicrosoftLanguageStemmingTokenizer struct {
	// MaxTokenLength - The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300 characters. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the max token length set. Default is 255.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// IsSearchTokenizer - A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. Default is false.
	IsSearchTokenizer *bool `json:"isSearchTokenizer,omitempty"`
	// Language - The language to use. The default is English. Possible values include: 'MicrosoftStemmingTokenizerLanguageArabic', 'MicrosoftStemmingTokenizerLanguageBangla', 'MicrosoftStemmingTokenizerLanguageBulgarian', 'MicrosoftStemmingTokenizerLanguageCatalan', 'MicrosoftStemmingTokenizerLanguageCroatian', 'MicrosoftStemmingTokenizerLanguageCzech', 'MicrosoftStemmingTokenizerLanguageDanish', 'MicrosoftStemmingTokenizerLanguageDutch', 'MicrosoftStemmingTokenizerLanguageEnglish', 'MicrosoftStemmingTokenizerLanguageEstonian', 'MicrosoftStemmingTokenizerLanguageFinnish', 'MicrosoftStemmingTokenizerLanguageFrench', 'MicrosoftStemmingTokenizerLanguageGerman', 'MicrosoftStemmingTokenizerLanguageGreek', 'MicrosoftStemmingTokenizerLanguageGujarati', 'MicrosoftStemmingTokenizerLanguageHebrew', 'MicrosoftStemmingTokenizerLanguageHindi', 'MicrosoftStemmingTokenizerLanguageHungarian', 'MicrosoftStemmingTokenizerLanguageIcelandic', 'MicrosoftStemmingTokenizerLanguageIndonesian', 'MicrosoftStemmingTokenizerLanguageItalian', 'MicrosoftStemmingTokenizerLanguageKannada', 'MicrosoftStemmingTokenizerLanguageLatvian', 'MicrosoftStemmingTokenizerLanguageLithuanian', 'MicrosoftStemmingTokenizerLanguageMalay', 'MicrosoftStemmingTokenizerLanguageMalayalam', 'MicrosoftStemmingTokenizerLanguageMarathi', 'MicrosoftStemmingTokenizerLanguageNorwegianBokmaal', 'MicrosoftStemmingTokenizerLanguagePolish', 'MicrosoftStemmingTokenizerLanguagePortuguese', 'MicrosoftStemmingTokenizerLanguagePortugueseBrazilian', 'MicrosoftStemmingTokenizerLanguagePunjabi', 'MicrosoftStemmingTokenizerLanguageRomanian', 'MicrosoftStemmingTokenizerLanguageRussian', 'MicrosoftStemmingTokenizerLanguageSerbianCyrillic', 'MicrosoftStemmingTokenizerLanguageSerbianLatin', 'MicrosoftStemmingTokenizerLanguageSlovak', 'MicrosoftStemmingTokenizerLanguageSlovenian', 'MicrosoftStemmingTokenizerLanguageSpanish', 'MicrosoftStemmingTokenizerLanguageSwedish', 'MicrosoftStemmingTokenizerLanguageTamil', 'MicrosoftStemmingTokenizerLanguageTelugu', 'MicrosoftStemmingTokenizerLanguageTurkish', 'MicrosoftStemmingTokenizerLanguageUkrainian', 'MicrosoftStemmingTokenizerLanguageUrdu'
	Language MicrosoftStemmingTokenizerLanguage `json:"language,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) MarshalJSON() ([]byte, error) {
	mlst.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer
	objectMap := make(map[string]interface{})
	if mlst.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = mlst.MaxTokenLength
	}
	if mlst.IsSearchTokenizer != nil {
		objectMap["isSearchTokenizer"] = mlst.IsSearchTokenizer
	}
	if mlst.Language != "" {
		objectMap["language"] = mlst.Language
	}
	if mlst.Name != nil {
		objectMap["name"] = mlst.Name
	}
	if mlst.OdataType != "" {
		objectMap["@odata.type"] = mlst.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return &mlst, true
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &mlst, true
}

// MicrosoftLanguageTokenizer divides text using language-specific rules.
type MicrosoftLanguageTokenizer struct {
	// MaxTokenLength - The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300 characters. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the max token length set. Default is 255.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// IsSearchTokenizer - A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. Default is false.
	IsSearchTokenizer *bool `json:"isSearchTokenizer,omitempty"`
	// Language - The language to use. The default is English. Possible values include: 'MicrosoftTokenizerLanguageBangla', 'MicrosoftTokenizerLanguageBulgarian', 'MicrosoftTokenizerLanguageCatalan', 'MicrosoftTokenizerLanguageChineseSimplified', 'MicrosoftTokenizerLanguageChineseTraditional', 'MicrosoftTokenizerLanguageCroatian', 'MicrosoftTokenizerLanguageCzech', 'MicrosoftTokenizerLanguageDanish', 'MicrosoftTokenizerLanguageDutch', 'MicrosoftTokenizerLanguageEnglish', 'MicrosoftTokenizerLanguageFrench', 'MicrosoftTokenizerLanguageGerman', 'MicrosoftTokenizerLanguageGreek', 'MicrosoftTokenizerLanguageGujarati', 'MicrosoftTokenizerLanguageHindi', 'MicrosoftTokenizerLanguageIcelandic', 'MicrosoftTokenizerLanguageIndonesian', 'MicrosoftTokenizerLanguageItalian', 'MicrosoftTokenizerLanguageJapanese', 'MicrosoftTokenizerLanguageKannada', 'MicrosoftTokenizerLanguageKorean', 'MicrosoftTokenizerLanguageMalay', 'MicrosoftTokenizerLanguageMalayalam', 'MicrosoftTokenizerLanguageMarathi', 'MicrosoftTokenizerLanguageNorwegianBokmaal', 'MicrosoftTokenizerLanguagePolish', 'MicrosoftTokenizerLanguagePortuguese', 'MicrosoftTokenizerLanguagePortugueseBrazilian', 'MicrosoftTokenizerLanguagePunjabi', 'MicrosoftTokenizerLanguageRomanian', 'MicrosoftTokenizerLanguageRussian', 'MicrosoftTokenizerLanguageSerbianCyrillic', 'MicrosoftTokenizerLanguageSerbianLatin', 'MicrosoftTokenizerLanguageSlovenian', 'MicrosoftTokenizerLanguageSpanish', 'MicrosoftTokenizerLanguageSwedish', 'MicrosoftTokenizerLanguageTamil', 'MicrosoftTokenizerLanguageTelugu', 'MicrosoftTokenizerLanguageThai', 'MicrosoftTokenizerLanguageUkrainian', 'MicrosoftTokenizerLanguageUrdu', 'MicrosoftTokenizerLanguageVietnamese'
	Language MicrosoftTokenizerLanguage `json:"language,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) MarshalJSON() ([]byte, error) {
	mlt.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer
	objectMap := make(map[string]interface{})
	if mlt.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = mlt.MaxTokenLength
	}
	if mlt.IsSearchTokenizer != nil {
		objectMap["isSearchTokenizer"] = mlt.IsSearchTokenizer
	}
	if mlt.Language != "" {
		objectMap["language"] = mlt.Language
	}
	if mlt.Name != nil {
		objectMap["name"] = mlt.Name
	}
	if mlt.OdataType != "" {
		objectMap["@odata.type"] = mlt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return &mlt, true
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &mlt, true
}

// NGramTokenFilter generates n-grams of the given size(s). This token filter is implemented using Apache
// Lucene.
type NGramTokenFilter struct {
	// MinGram - The minimum n-gram length. Default is 1. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for NGramTokenFilter.
func (ngtf NGramTokenFilter) MarshalJSON() ([]byte, error) {
	ngtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter
	objectMap := make(map[string]interface{})
	if ngtf.MinGram != nil {
		objectMap["minGram"] = ngtf.MinGram
	}
	if ngtf.MaxGram != nil {
		objectMap["maxGram"] = ngtf.MaxGram
	}
	if ngtf.Name != nil {
		objectMap["name"] = ngtf.Name
	}
	if ngtf.OdataType != "" {
		objectMap["@odata.type"] = ngtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return &ngtf, true
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ngtf, true
}

// NGramTokenFilterV2 generates n-grams of the given size(s). This token filter is implemented using Apache
// Lucene.
type NGramTokenFilterV2 struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) MarshalJSON() ([]byte, error) {
	ngtfv.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2
	objectMap := make(map[string]interface{})
	if ngtfv.MinGram != nil {
		objectMap["minGram"] = ngtfv.MinGram
	}
	if ngtfv.MaxGram != nil {
		objectMap["maxGram"] = ngtfv.MaxGram
	}
	if ngtfv.Name != nil {
		objectMap["name"] = ngtfv.Name
	}
	if ngtfv.OdataType != "" {
		objectMap["@odata.type"] = ngtfv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return &ngtfv, true
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ngtfv, true
}

// NGramTokenizer tokenizes the input into n-grams of the given size(s). This tokenizer is implemented
// using Apache Lucene.
type NGramTokenizer struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// TokenChars - Character classes to keep in the tokens.
	TokenChars *[]TokenCharacterKind `json:"tokenChars,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for NGramTokenizer.
func (ngt NGramTokenizer) MarshalJSON() ([]byte, error) {
	ngt.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer
	objectMap := make(map[string]interface{})
	if ngt.MinGram != nil {
		objectMap["minGram"] = ngt.MinGram
	}
	if ngt.MaxGram != nil {
		objectMap["maxGram"] = ngt.MaxGram
	}
	if ngt.TokenChars != nil {
		objectMap["tokenChars"] = ngt.TokenChars
	}
	if ngt.Name != nil {
		objectMap["name"] = ngt.Name
	}
	if ngt.OdataType != "" {
		objectMap["@odata.type"] = ngt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return &ngt, true
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &ngt, true
}

// NativeBlobSoftDeleteDeletionDetectionPolicy defines a data deletion detection policy utilizing Azure
// Blob Storage's native soft delete feature for deletion detection.
type NativeBlobSoftDeleteDeletionDetectionPolicy struct {
	// OdataType - Possible values include: 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeDataDeletionDetectionPolicy', 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy', 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchNativeBlobSoftDeleteDeletionDetectionPolicy'
	OdataType OdataTypeBasicDataDeletionDetectionPolicy `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for NativeBlobSoftDeleteDeletionDetectionPolicy.
func (nbsdddp NativeBlobSoftDeleteDeletionDetectionPolicy) MarshalJSON() ([]byte, error) {
	nbsdddp.OdataType = OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchNativeBlobSoftDeleteDeletionDetectionPolicy
	objectMap := make(map[string]interface{})
	if nbsdddp.OdataType != "" {
		objectMap["@odata.type"] = nbsdddp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsSoftDeleteColumnDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for NativeBlobSoftDeleteDeletionDetectionPolicy.
func (nbsdddp NativeBlobSoftDeleteDeletionDetectionPolicy) AsSoftDeleteColumnDeletionDetectionPolicy() (*SoftDeleteColumnDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsNativeBlobSoftDeleteDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for NativeBlobSoftDeleteDeletionDetectionPolicy.
func (nbsdddp NativeBlobSoftDeleteDeletionDetectionPolicy) AsNativeBlobSoftDeleteDeletionDetectionPolicy() (*NativeBlobSoftDeleteDeletionDetectionPolicy, bool) {
	return &nbsdddp, true
}

// AsDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for NativeBlobSoftDeleteDeletionDetectionPolicy.
func (nbsdddp NativeBlobSoftDeleteDeletionDetectionPolicy) AsDataDeletionDetectionPolicy() (*DataDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsBasicDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for NativeBlobSoftDeleteDeletionDetectionPolicy.
func (nbsdddp NativeBlobSoftDeleteDeletionDetectionPolicy) AsBasicDataDeletionDetectionPolicy() (BasicDataDeletionDetectionPolicy, bool) {
	return &nbsdddp, true
}

// OcrSkill a skill that extracts text from image files.
type OcrSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`. Possible values include: 'OcrSkillLanguageAf', 'OcrSkillLanguageSq', 'OcrSkillLanguageAnp', 'OcrSkillLanguageAr', 'OcrSkillLanguageAst', 'OcrSkillLanguageAwa', 'OcrSkillLanguageAz', 'OcrSkillLanguageBfy', 'OcrSkillLanguageEu', 'OcrSkillLanguageBe', 'OcrSkillLanguageBeCyrl', 'OcrSkillLanguageBeLatn', 'OcrSkillLanguageBho', 'OcrSkillLanguageBi', 'OcrSkillLanguageBrx', 'OcrSkillLanguageBs', 'OcrSkillLanguageBra', 'OcrSkillLanguageBr', 'OcrSkillLanguageBg', 'OcrSkillLanguageBns', 'OcrSkillLanguageBua', 'OcrSkillLanguageCa', 'OcrSkillLanguageCeb', 'OcrSkillLanguageRab', 'OcrSkillLanguageCh', 'OcrSkillLanguageHne', 'OcrSkillLanguageZhHans', 'OcrSkillLanguageZhHant', 'OcrSkillLanguageKw', 'OcrSkillLanguageCo', 'OcrSkillLanguageCrh', 'OcrSkillLanguageHr', 'OcrSkillLanguageCs', 'OcrSkillLanguageDa', 'OcrSkillLanguagePrs', 'OcrSkillLanguageDhi', 'OcrSkillLanguageDoi', 'OcrSkillLanguageNl', 'OcrSkillLanguageEn', 'OcrSkillLanguageMyv', 'OcrSkillLanguageEt', 'OcrSkillLanguageFo', 'OcrSkillLanguageFj', 'OcrSkillLanguageFil', 'OcrSkillLanguageFi', 'OcrSkillLanguageFr', 'OcrSkillLanguageFur', 'OcrSkillLanguageGag', 'OcrSkillLanguageGl', 'OcrSkillLanguageDe', 'OcrSkillLanguageGil', 'OcrSkillLanguageGon', 'OcrSkillLanguageEl', 'OcrSkillLanguageKl', 'OcrSkillLanguageGvr', 'OcrSkillLanguageHt', 'OcrSkillLanguageHlb', 'OcrSkillLanguageHni', 'OcrSkillLanguageBgc', 'OcrSkillLanguageHaw', 'OcrSkillLanguageHi', 'OcrSkillLanguageMww', 'OcrSkillLanguageHoc', 'OcrSkillLanguageHu', 'OcrSkillLanguageIs', 'OcrSkillLanguageSmn', 'OcrSkillLanguageID', 'OcrSkillLanguageIa', 'OcrSkillLanguageIu', 'OcrSkillLanguageGa', 'OcrSkillLanguageIt', 'OcrSkillLanguageJa', 'OcrSkillLanguageJns', 'OcrSkillLanguageJv', 'OcrSkillLanguageKea', 'OcrSkillLanguageKac', 'OcrSkillLanguageXnr', 'OcrSkillLanguageKrc', 'OcrSkillLanguageKaaCyrl', 'OcrSkillLanguageKaa', 'OcrSkillLanguageCsb', 'OcrSkillLanguageKkCyrl', 'OcrSkillLanguageKkLatn', 'OcrSkillLanguageKlr', 'OcrSkillLanguageKha', 'OcrSkillLanguageQuc', 'OcrSkillLanguageKo', 'OcrSkillLanguageKfq', 'OcrSkillLanguageKpy', 'OcrSkillLanguageKos', 'OcrSkillLanguageKum', 'OcrSkillLanguageKuArab', 'OcrSkillLanguageKuLatn', 'OcrSkillLanguageKru', 'OcrSkillLanguageKy', 'OcrSkillLanguageLkt', 'OcrSkillLanguageLa', 'OcrSkillLanguageLt', 'OcrSkillLanguageDsb', 'OcrSkillLanguageSmj', 'OcrSkillLanguageLb', 'OcrSkillLanguageBfz', 'OcrSkillLanguageMs', 'OcrSkillLanguageMt', 'OcrSkillLanguageKmj', 'OcrSkillLanguageGv', 'OcrSkillLanguageMi', 'OcrSkillLanguageMr', 'OcrSkillLanguageMn', 'OcrSkillLanguageCnrCyrl', 'OcrSkillLanguageCnrLatn', 'OcrSkillLanguageNap', 'OcrSkillLanguageNe', 'OcrSkillLanguageNiu', 'OcrSkillLanguageNog', 'OcrSkillLanguageSme', 'OcrSkillLanguageNb', 'OcrSkillLanguageNo', 'OcrSkillLanguageOc', 'OcrSkillLanguageOs', 'OcrSkillLanguagePs', 'OcrSkillLanguageFa', 'OcrSkillLanguagePl', 'OcrSkillLanguagePt', 'OcrSkillLanguagePa', 'OcrSkillLanguageKsh', 'OcrSkillLanguageRo', 'OcrSkillLanguageRm', 'OcrSkillLanguageRu', 'OcrSkillLanguageSck', 'OcrSkillLanguageSm', 'OcrSkillLanguageSa', 'OcrSkillLanguageSat', 'OcrSkillLanguageSco', 'OcrSkillLanguageGd', 'OcrSkillLanguageSr', 'OcrSkillLanguageSrCyrl', 'OcrSkillLanguageSrLatn', 'OcrSkillLanguageXsr', 'OcrSkillLanguageSrx', 'OcrSkillLanguageSms', 'OcrSkillLanguageSk', 'OcrSkillLanguageSl', 'OcrSkillLanguageSo', 'OcrSkillLanguageSma', 'OcrSkillLanguageEs', 'OcrSkillLanguageSw', 'OcrSkillLanguageSv', 'OcrSkillLanguageTg', 'OcrSkillLanguageTt', 'OcrSkillLanguageTet', 'OcrSkillLanguageThf', 'OcrSkillLanguageTo', 'OcrSkillLanguageTr', 'OcrSkillLanguageTk', 'OcrSkillLanguageTyv', 'OcrSkillLanguageHsb', 'OcrSkillLanguageUr', 'OcrSkillLanguageUg', 'OcrSkillLanguageUzArab', 'OcrSkillLanguageUzCyrl', 'OcrSkillLanguageUz', 'OcrSkillLanguageVo', 'OcrSkillLanguageWae', 'OcrSkillLanguageCy', 'OcrSkillLanguageFy', 'OcrSkillLanguageYua', 'OcrSkillLanguageZa', 'OcrSkillLanguageZu', 'OcrSkillLanguageUnk'
	DefaultLanguageCode OcrSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// ShouldDetectOrientation - A value indicating to turn orientation detection on or not. Default is false.
	ShouldDetectOrientation *bool `json:"detectOrientation,omitempty"`
	// LineEnding - Defines the sequence of characters to use between the lines of text recognized by the OCR skill. The default value is "space". Possible values include: 'LineEndingSpace', 'LineEndingCarriageReturn', 'LineEndingLineFeed', 'LineEndingCarriageReturnLineFeed'
	LineEnding LineEnding `json:"lineEnding,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for OcrSkill.
func (osVar OcrSkill) MarshalJSON() ([]byte, error) {
	osVar.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill
	objectMap := make(map[string]interface{})
	if osVar.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = osVar.DefaultLanguageCode
	}
	if osVar.ShouldDetectOrientation != nil {
		objectMap["detectOrientation"] = osVar.ShouldDetectOrientation
	}
	if osVar.LineEnding != "" {
		objectMap["lineEnding"] = osVar.LineEnding
	}
	if osVar.Name != nil {
		objectMap["name"] = osVar.Name
	}
	if osVar.Description != nil {
		objectMap["description"] = osVar.Description
	}
	if osVar.Context != nil {
		objectMap["context"] = osVar.Context
	}
	if osVar.Inputs != nil {
		objectMap["inputs"] = osVar.Inputs
	}
	if osVar.Outputs != nil {
		objectMap["outputs"] = osVar.Outputs
	}
	if osVar.OdataType != "" {
		objectMap["@odata.type"] = osVar.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsOcrSkill() (*OcrSkill, bool) {
	return &osVar, true
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for OcrSkill.
func (osVar OcrSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &osVar, true
}

// OutputFieldMappingEntry output field mapping for a skill.
type OutputFieldMappingEntry struct {
	// Name - The name of the output defined by the skill.
	Name *string `json:"name,omitempty"`
	// TargetName - The target name of the output. It is optional and default to name.
	TargetName *string `json:"targetName,omitempty"`
}

// PIIDetectionSkill using the Text Analytics API, extracts personal information from an input text and
// gives you the option of masking it.
type PIIDetectionSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`.
	DefaultLanguageCode *string `json:"defaultLanguageCode,omitempty"`
	// MinimumPrecision - A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified. If not set (default), or if explicitly set to null, all entities will be included.
	MinimumPrecision *float64 `json:"minimumPrecision,omitempty"`
	// MaskingMode - A parameter that provides various ways to mask the personal information detected in the input text. Default is 'none'. Possible values include: 'PIIDetectionSkillMaskingModeNone', 'PIIDetectionSkillMaskingModeReplace'
	MaskingMode PIIDetectionSkillMaskingMode `json:"maskingMode,omitempty"`
	// Mask - The character used to mask the text if the maskingMode parameter is set to replace. Default is '*'.
	Mask *string `json:"maskingCharacter,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics service. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// PiiCategories - A list of PII entity categories that should be extracted and masked.
	PiiCategories *[]string `json:"piiCategories,omitempty"`
	// Domain - If specified, will set the PII domain to include only a subset of the entity categories. Possible values include: 'phi', 'none'. Default is 'none'.
	Domain *string `json:"domain,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PIIDetectionSkill.
func (pds PIIDetectionSkill) MarshalJSON() ([]byte, error) {
	pds.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill
	objectMap := make(map[string]interface{})
	if pds.DefaultLanguageCode != nil {
		objectMap["defaultLanguageCode"] = pds.DefaultLanguageCode
	}
	if pds.MinimumPrecision != nil {
		objectMap["minimumPrecision"] = pds.MinimumPrecision
	}
	if pds.MaskingMode != "" {
		objectMap["maskingMode"] = pds.MaskingMode
	}
	if pds.Mask != nil {
		objectMap["maskingCharacter"] = pds.Mask
	}
	if pds.ModelVersion != nil {
		objectMap["modelVersion"] = pds.ModelVersion
	}
	if pds.PiiCategories != nil {
		objectMap["piiCategories"] = pds.PiiCategories
	}
	if pds.Domain != nil {
		objectMap["domain"] = pds.Domain
	}
	if pds.Name != nil {
		objectMap["name"] = pds.Name
	}
	if pds.Description != nil {
		objectMap["description"] = pds.Description
	}
	if pds.Context != nil {
		objectMap["context"] = pds.Context
	}
	if pds.Inputs != nil {
		objectMap["inputs"] = pds.Inputs
	}
	if pds.Outputs != nil {
		objectMap["outputs"] = pds.Outputs
	}
	if pds.OdataType != "" {
		objectMap["@odata.type"] = pds.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return &pds, true
}

// AsSplitSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for PIIDetectionSkill.
func (pds PIIDetectionSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &pds, true
}

// PathHierarchyTokenizerV2 tokenizer for path-like hierarchies. This tokenizer is implemented using Apache
// Lucene.
type PathHierarchyTokenizerV2 struct {
	// Delimiter - The delimiter character to use. Default is "/".
	Delimiter *string `json:"delimiter,omitempty"`
	// Replacement - A value that, if set, replaces the delimiter character. Default is "/".
	Replacement *string `json:"replacement,omitempty"`
	// MaxTokenLength - The maximum token length. Default and maximum is 300.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// ReverseTokenOrder - A value indicating whether to generate tokens in reverse order. Default is false.
	ReverseTokenOrder *bool `json:"reverse,omitempty"`
	// NumberOfTokensToSkip - The number of initial tokens to skip. Default is 0.
	NumberOfTokensToSkip *int32 `json:"skip,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) MarshalJSON() ([]byte, error) {
	phtv.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2
	objectMap := make(map[string]interface{})
	if phtv.Delimiter != nil {
		objectMap["delimiter"] = phtv.Delimiter
	}
	if phtv.Replacement != nil {
		objectMap["replacement"] = phtv.Replacement
	}
	if phtv.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = phtv.MaxTokenLength
	}
	if phtv.ReverseTokenOrder != nil {
		objectMap["reverse"] = phtv.ReverseTokenOrder
	}
	if phtv.NumberOfTokensToSkip != nil {
		objectMap["skip"] = phtv.NumberOfTokensToSkip
	}
	if phtv.Name != nil {
		objectMap["name"] = phtv.Name
	}
	if phtv.OdataType != "" {
		objectMap["@odata.type"] = phtv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return &phtv, true
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &phtv, true
}

// PatternAnalyzer flexibly separates text into terms via a regular expression pattern. This analyzer is
// implemented using Apache Lucene.
type PatternAnalyzer struct {
	// LowerCaseTerms - A value indicating whether terms should be lower-cased. Default is true.
	LowerCaseTerms *bool `json:"lowercase,omitempty"`
	// Pattern - A regular expression pattern to match token separators. Default is an expression that matches one or more non-word characters.
	Pattern *string `json:"pattern,omitempty"`
	// Flags - Regular expression flags. Possible values include: 'RegexFlagsCanonEq', 'RegexFlagsCaseInsensitive', 'RegexFlagsComments', 'RegexFlagsDotAll', 'RegexFlagsLiteral', 'RegexFlagsMultiline', 'RegexFlagsUnicodeCase', 'RegexFlagsUnixLines'
	Flags RegexFlags `json:"flags,omitempty"`
	// Stopwords - A list of stopwords.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeLexicalAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternAnalyzer.
func (pa PatternAnalyzer) MarshalJSON() ([]byte, error) {
	pa.OdataType = OdataTypeMicrosoftAzureSearchPatternAnalyzer
	objectMap := make(map[string]interface{})
	if pa.LowerCaseTerms != nil {
		objectMap["lowercase"] = pa.LowerCaseTerms
	}
	if pa.Pattern != nil {
		objectMap["pattern"] = pa.Pattern
	}
	if pa.Flags != "" {
		objectMap["flags"] = pa.Flags
	}
	if pa.Stopwords != nil {
		objectMap["stopwords"] = pa.Stopwords
	}
	if pa.Name != nil {
		objectMap["name"] = pa.Name
	}
	if pa.OdataType != "" {
		objectMap["@odata.type"] = pa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicLexicalAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicLexicalAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return &pa, true
}

// AsLuceneStandardAnalyzer is the BasicLexicalAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsLuceneStandardAnalyzer() (*LuceneStandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicLexicalAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsLexicalAnalyzer is the BasicLexicalAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsLexicalAnalyzer() (*LexicalAnalyzer, bool) {
	return nil, false
}

// AsBasicLexicalAnalyzer is the BasicLexicalAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsBasicLexicalAnalyzer() (BasicLexicalAnalyzer, bool) {
	return &pa, true
}

// PatternCaptureTokenFilter uses Java regexes to emit multiple tokens - one for each capture group in one
// or more patterns. This token filter is implemented using Apache Lucene.
type PatternCaptureTokenFilter struct {
	// Patterns - A list of patterns to match against each token.
	Patterns *[]string `json:"patterns,omitempty"`
	// PreserveOriginal - A value indicating whether to return the original token even if one of the patterns matches. Default is true.
	PreserveOriginal *bool `json:"preserveOriginal,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) MarshalJSON() ([]byte, error) {
	pctf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter
	objectMap := make(map[string]interface{})
	if pctf.Patterns != nil {
		objectMap["patterns"] = pctf.Patterns
	}
	if pctf.PreserveOriginal != nil {
		objectMap["preserveOriginal"] = pctf.PreserveOriginal
	}
	if pctf.Name != nil {
		objectMap["name"] = pctf.Name
	}
	if pctf.OdataType != "" {
		objectMap["@odata.type"] = pctf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return &pctf, true
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &pctf, true
}

// PatternReplaceCharFilter a character filter that replaces characters in the input string. It uses a
// regular expression to identify character sequences to preserve and a replacement pattern to identify
// characters to replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and
// replacement "$1#$2", the result would be "aa#bb aa#bb". This character filter is implemented using
// Apache Lucene.
type PatternReplaceCharFilter struct {
	// Pattern - A regular expression pattern.
	Pattern *string `json:"pattern,omitempty"`
	// Replacement - The replacement text.
	Replacement *string `json:"replacement,omitempty"`
	// Name - The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicCharFilterOdataTypeCharFilter', 'OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchMappingCharFilter', 'OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchPatternReplaceCharFilter'
	OdataType OdataTypeBasicCharFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) MarshalJSON() ([]byte, error) {
	prcf.OdataType = OdataTypeBasicCharFilterOdataTypeMicrosoftAzureSearchPatternReplaceCharFilter
	objectMap := make(map[string]interface{})
	if prcf.Pattern != nil {
		objectMap["pattern"] = prcf.Pattern
	}
	if prcf.Replacement != nil {
		objectMap["replacement"] = prcf.Replacement
	}
	if prcf.Name != nil {
		objectMap["name"] = prcf.Name
	}
	if prcf.OdataType != "" {
		objectMap["@odata.type"] = prcf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsMappingCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsMappingCharFilter() (*MappingCharFilter, bool) {
	return nil, false
}

// AsPatternReplaceCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool) {
	return &prcf, true
}

// AsCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsCharFilter() (*CharFilter, bool) {
	return nil, false
}

// AsBasicCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsBasicCharFilter() (BasicCharFilter, bool) {
	return &prcf, true
}

// PatternReplaceTokenFilter a character filter that replaces characters in the input string. It uses a
// regular expression to identify character sequences to preserve and a replacement pattern to identify
// characters to replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and
// replacement "$1#$2", the result would be "aa#bb aa#bb". This token filter is implemented using Apache
// Lucene.
type PatternReplaceTokenFilter struct {
	// Pattern - A regular expression pattern.
	Pattern *string `json:"pattern,omitempty"`
	// Replacement - The replacement text.
	Replacement *string `json:"replacement,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) MarshalJSON() ([]byte, error) {
	prtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter
	objectMap := make(map[string]interface{})
	if prtf.Pattern != nil {
		objectMap["pattern"] = prtf.Pattern
	}
	if prtf.Replacement != nil {
		objectMap["replacement"] = prtf.Replacement
	}
	if prtf.Name != nil {
		objectMap["name"] = prtf.Name
	}
	if prtf.OdataType != "" {
		objectMap["@odata.type"] = prtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return &prtf, true
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &prtf, true
}

// PatternTokenizer tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer
// is implemented using Apache Lucene.
type PatternTokenizer struct {
	// Pattern - A regular expression pattern to match token separators. Default is an expression that matches one or more non-word characters.
	Pattern *string `json:"pattern,omitempty"`
	// Flags - Regular expression flags. Possible values include: 'RegexFlagsCanonEq', 'RegexFlagsCaseInsensitive', 'RegexFlagsComments', 'RegexFlagsDotAll', 'RegexFlagsLiteral', 'RegexFlagsMultiline', 'RegexFlagsUnicodeCase', 'RegexFlagsUnixLines'
	Flags RegexFlags `json:"flags,omitempty"`
	// Group - The zero-based ordinal of the matching group in the regular expression pattern to extract into tokens. Use -1 if you want to use the entire pattern to split the input into tokens, irrespective of matching groups. Default is -1.
	Group *int32 `json:"group,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternTokenizer.
func (pt PatternTokenizer) MarshalJSON() ([]byte, error) {
	pt.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer
	objectMap := make(map[string]interface{})
	if pt.Pattern != nil {
		objectMap["pattern"] = pt.Pattern
	}
	if pt.Flags != "" {
		objectMap["flags"] = pt.Flags
	}
	if pt.Group != nil {
		objectMap["group"] = pt.Group
	}
	if pt.Name != nil {
		objectMap["name"] = pt.Name
	}
	if pt.OdataType != "" {
		objectMap["@odata.type"] = pt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return &pt, true
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &pt, true
}

// PhoneticTokenFilter create tokens for phonetic matches. This token filter is implemented using Apache
// Lucene.
type PhoneticTokenFilter struct {
	// Encoder - The phonetic encoder to use. Default is "metaphone". Possible values include: 'PhoneticEncoderMetaphone', 'PhoneticEncoderDoubleMetaphone', 'PhoneticEncoderSoundex', 'PhoneticEncoderRefinedSoundex', 'PhoneticEncoderCaverphone1', 'PhoneticEncoderCaverphone2', 'PhoneticEncoderCologne', 'PhoneticEncoderNysiis', 'PhoneticEncoderKoelnerPhonetik', 'PhoneticEncoderHaasePhonetik', 'PhoneticEncoderBeiderMorse'
	Encoder PhoneticEncoder `json:"encoder,omitempty"`
	// ReplaceOriginalTokens - A value indicating whether encoded tokens should replace original tokens. If false, encoded tokens are added as synonyms. Default is true.
	ReplaceOriginalTokens *bool `json:"replace,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) MarshalJSON() ([]byte, error) {
	ptf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter
	objectMap := make(map[string]interface{})
	if ptf.Encoder != "" {
		objectMap["encoder"] = ptf.Encoder
	}
	if ptf.ReplaceOriginalTokens != nil {
		objectMap["replace"] = ptf.ReplaceOriginalTokens
	}
	if ptf.Name != nil {
		objectMap["name"] = ptf.Name
	}
	if ptf.OdataType != "" {
		objectMap["@odata.type"] = ptf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return &ptf, true
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ptf, true
}

// PrioritizedFields describes the title, content, and keywords fields to be used for semantic ranking,
// captions, highlights, and answers.
type PrioritizedFields struct {
	// TitleField - Defines the title field to be used for semantic ranking, captions, highlights, and answers. If you don't have a title field in your index, leave this blank.
	TitleField *SemanticField `json:"titleField,omitempty"`
	// ContentFields - Defines the content fields to be used for semantic ranking, captions, highlights, and answers. For the best result, the selected fields should contain text in natural language form. The order of the fields in the array represents their priority. Fields with lower priority may get truncated if the content is long.
	ContentFields *[]SemanticField `json:"prioritizedContentFields,omitempty"`
	// KeywordsFields - Defines the keyword fields to be used for semantic ranking, captions, highlights, and answers. For the best result, the selected fields should contain a list of keywords. The order of the fields in the array represents their priority. Fields with lower priority may get truncated if the content is long.
	KeywordsFields *[]SemanticField `json:"prioritizedKeywordsFields,omitempty"`
}

// ResourceCounter represents a resource's usage and quota.
type ResourceCounter struct {
	// Usage - The resource usage amount.
	Usage *int64 `json:"usage,omitempty"`
	// Quota - The resource amount quota.
	Quota *int64 `json:"quota,omitempty"`
}

// ResourceEncryptionKey a customer-managed encryption key in Azure Key Vault. Keys that you create and
// manage can be used to encrypt or decrypt data-at-rest, such as indexes and synonym maps.
type ResourceEncryptionKey struct {
	// KeyName - The name of your Azure Key Vault key to be used to encrypt your data at rest.
	KeyName *string `json:"keyVaultKeyName,omitempty"`
	// KeyVersion - The version of your Azure Key Vault key to be used to encrypt your data at rest.
	KeyVersion *string `json:"keyVaultKeyVersion,omitempty"`
	// VaultURI - The URI of your Azure Key Vault, also referred to as DNS name, that contains the key to be used to encrypt your data at rest. An example URI might be `https://my-keyvault-name.vault.azure.net`.
	VaultURI *string `json:"keyVaultUri,omitempty"`
	// AccessCredentials - Optional Azure Active Directory credentials used for accessing your Azure Key Vault. Not required if using managed identity instead.
	AccessCredentials *AzureActiveDirectoryApplicationCredentials `json:"accessCredentials,omitempty"`
	// Identity - An explicit managed identity to use for this encryption key. If not specified and the access credentials property is null, the system-assigned managed identity is used. On update to the resource, if the explicit identity is unspecified, it remains unchanged. If "none" is specified, the value of this property is cleared.
	Identity BasicIndexerDataIdentity `json:"identity,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for ResourceEncryptionKey struct.
func (rek *ResourceEncryptionKey) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "keyVaultKeyName":
			if v != nil {
				var keyName string
				err = json.Unmarshal(*v, &keyName)
				if err != nil {
					return err
				}
				rek.KeyName = &keyName
			}
		case "keyVaultKeyVersion":
			if v != nil {
				var keyVersion string
				err = json.Unmarshal(*v, &keyVersion)
				if err != nil {
					return err
				}
				rek.KeyVersion = &keyVersion
			}
		case "keyVaultUri":
			if v != nil {
				var vaultURI string
				err = json.Unmarshal(*v, &vaultURI)
				if err != nil {
					return err
				}
				rek.VaultURI = &vaultURI
			}
		case "accessCredentials":
			if v != nil {
				var accessCredentials AzureActiveDirectoryApplicationCredentials
				err = json.Unmarshal(*v, &accessCredentials)
				if err != nil {
					return err
				}
				rek.AccessCredentials = &accessCredentials
			}
		case "identity":
			if v != nil {
				identity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				rek.Identity = identity
			}
		}
	}

	return nil
}

// SQLIntegratedChangeTrackingPolicy defines a data change detection policy that captures changes using the
// Integrated Change Tracking feature of Azure SQL Database.
type SQLIntegratedChangeTrackingPolicy struct {
	// OdataType - Possible values include: 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeDataChangeDetectionPolicy', 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy', 'OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy'
	OdataType OdataTypeBasicDataChangeDetectionPolicy `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) MarshalJSON() ([]byte, error) {
	sictp.OdataType = OdataTypeBasicDataChangeDetectionPolicyOdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy
	objectMap := make(map[string]interface{})
	if sictp.OdataType != "" {
		objectMap["@odata.type"] = sictp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsHighWaterMarkChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool) {
	return nil, false
}

// AsSQLIntegratedChangeTrackingPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool) {
	return &sictp, true
}

// AsDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool) {
	return nil, false
}

// AsBasicDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsBasicDataChangeDetectionPolicy() (BasicDataChangeDetectionPolicy, bool) {
	return &sictp, true
}

// ScalarQuantizationParameters contains the parameters specific to Scalar Quantization.
type ScalarQuantizationParameters struct {
	// QuantizedDataType - The quantized data type of compressed vector values. Possible values include: 'VectorSearchCompressionTargetDataTypeInt8'
	QuantizedDataType VectorSearchCompressionTargetDataType `json:"quantizedDataType,omitempty"`
}

// ScalarQuantizationVectorSearchCompressionConfiguration contains configuration options specific to the
// scalar quantization compression method used during indexing and querying.
type ScalarQuantizationVectorSearchCompressionConfiguration struct {
	// Parameters - Contains the parameters specific to Scalar Quantization.
	Parameters *ScalarQuantizationParameters `json:"scalarQuantizationParameters,omitempty"`
	// Name - The name to associate with this particular configuration.
	Name *string `json:"name,omitempty"`
	// RerankWithOriginalVectors - If set to true, once the ordered set of results calculated using compressed vectors are obtained, they will be reranked again by recalculating the full-precision similarity scores. This will improve recall at the expense of latency.
	RerankWithOriginalVectors *bool `json:"rerankWithOriginalVectors,omitempty"`
	// DefaultOversampling - Default oversampling factor. Oversampling will internally request more documents (specified by this multiplier) in the initial search. This increases the set of results that will be reranked using recomputed similarity scores from full-precision vectors. Minimum value is 1, meaning no oversampling (1x). This parameter can only be set when rerankWithOriginalVectors is true. Higher values improve recall at the expense of latency.
	DefaultOversampling *float64 `json:"defaultOversampling,omitempty"`
	// Kind - Possible values include: 'KindBasicVectorSearchCompressionConfigurationKindVectorSearchCompressionConfiguration', 'KindBasicVectorSearchCompressionConfigurationKindScalarQuantization'
	Kind KindBasicVectorSearchCompressionConfiguration `json:"kind,omitempty"`
}

// MarshalJSON is the custom marshaler for ScalarQuantizationVectorSearchCompressionConfiguration.
func (sqvscc ScalarQuantizationVectorSearchCompressionConfiguration) MarshalJSON() ([]byte, error) {
	sqvscc.Kind = KindBasicVectorSearchCompressionConfigurationKindScalarQuantization
	objectMap := make(map[string]interface{})
	if sqvscc.Parameters != nil {
		objectMap["scalarQuantizationParameters"] = sqvscc.Parameters
	}
	if sqvscc.Name != nil {
		objectMap["name"] = sqvscc.Name
	}
	if sqvscc.RerankWithOriginalVectors != nil {
		objectMap["rerankWithOriginalVectors"] = sqvscc.RerankWithOriginalVectors
	}
	if sqvscc.DefaultOversampling != nil {
		objectMap["defaultOversampling"] = sqvscc.DefaultOversampling
	}
	if sqvscc.Kind != "" {
		objectMap["kind"] = sqvscc.Kind
	}
	return json.Marshal(objectMap)
}

// AsScalarQuantizationVectorSearchCompressionConfiguration is the BasicVectorSearchCompressionConfiguration implementation for ScalarQuantizationVectorSearchCompressionConfiguration.
func (sqvscc ScalarQuantizationVectorSearchCompressionConfiguration) AsScalarQuantizationVectorSearchCompressionConfiguration() (*ScalarQuantizationVectorSearchCompressionConfiguration, bool) {
	return &sqvscc, true
}

// AsVectorSearchCompressionConfiguration is the BasicVectorSearchCompressionConfiguration implementation for ScalarQuantizationVectorSearchCompressionConfiguration.
func (sqvscc ScalarQuantizationVectorSearchCompressionConfiguration) AsVectorSearchCompressionConfiguration() (*VectorSearchCompressionConfiguration, bool) {
	return nil, false
}

// AsBasicVectorSearchCompressionConfiguration is the BasicVectorSearchCompressionConfiguration implementation for ScalarQuantizationVectorSearchCompressionConfiguration.
func (sqvscc ScalarQuantizationVectorSearchCompressionConfiguration) AsBasicVectorSearchCompressionConfiguration() (BasicVectorSearchCompressionConfiguration, bool) {
	return &sqvscc, true
}

// BasicScoringFunction base type for functions that can modify document scores during ranking.
type BasicScoringFunction interface {
	AsDistanceScoringFunction() (*DistanceScoringFunction, bool)
	AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool)
	AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool)
	AsTagScoringFunction() (*TagScoringFunction, bool)
	AsScoringFunction() (*ScoringFunction, bool)
}

// ScoringFunction base type for functions that can modify document scores during ranking.
type ScoringFunction struct {
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'ScoringFunctionInterpolationLinear', 'ScoringFunctionInterpolationConstant', 'ScoringFunctionInterpolationQuadratic', 'ScoringFunctionInterpolationLogarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

func unmarshalBasicScoringFunction(body []byte) (BasicScoringFunction, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["type"] {
	case string(TypeDistance):
		var dsf DistanceScoringFunction
		err := json.Unmarshal(body, &dsf)
		return dsf, err
	case string(TypeFreshness):
		var fsf FreshnessScoringFunction
		err := json.Unmarshal(body, &fsf)
		return fsf, err
	case string(TypeMagnitude):
		var msf MagnitudeScoringFunction
		err := json.Unmarshal(body, &msf)
		return msf, err
	case string(TypeTag):
		var tsf TagScoringFunction
		err := json.Unmarshal(body, &tsf)
		return tsf, err
	default:
		var sf ScoringFunction
		err := json.Unmarshal(body, &sf)
		return sf, err
	}
}
func unmarshalBasicScoringFunctionArray(body []byte) ([]BasicScoringFunction, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	sfArray := make([]BasicScoringFunction, len(rawMessages))

	for index, rawMessage := range rawMessages {
		sf, err := unmarshalBasicScoringFunction(*rawMessage)
		if err != nil {
			return nil, err
		}
		sfArray[index] = sf
	}
	return sfArray, nil
}

// MarshalJSON is the custom marshaler for ScoringFunction.
func (sf ScoringFunction) MarshalJSON() ([]byte, error) {
	sf.Type = TypeScoringFunction
	objectMap := make(map[string]interface{})
	if sf.FieldName != nil {
		objectMap["fieldName"] = sf.FieldName
	}
	if sf.Boost != nil {
		objectMap["boost"] = sf.Boost
	}
	if sf.Interpolation != "" {
		objectMap["interpolation"] = sf.Interpolation
	}
	if sf.Type != "" {
		objectMap["type"] = sf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return &sf, true
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &sf, true
}

// ScoringProfile defines parameters for a search index that influence scoring in search queries.
type ScoringProfile struct {
	// Name - The name of the scoring profile.
	Name *string `json:"name,omitempty"`
	// TextWeights - Parameters that boost scoring based on text matches in certain index fields.
	TextWeights *TextWeights `json:"text,omitempty"`
	// Functions - The collection of functions that influence the scoring of documents.
	Functions *[]BasicScoringFunction `json:"functions,omitempty"`
	// FunctionAggregation - A value indicating how the results of individual scoring functions should be combined. Defaults to "Sum". Ignored if there are no scoring functions. Possible values include: 'ScoringFunctionAggregationSum', 'ScoringFunctionAggregationAverage', 'ScoringFunctionAggregationMinimum', 'ScoringFunctionAggregationMaximum', 'ScoringFunctionAggregationFirstMatching'
	FunctionAggregation ScoringFunctionAggregation `json:"functionAggregation,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for ScoringProfile struct.
func (sp *ScoringProfile) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				sp.Name = &name
			}
		case "text":
			if v != nil {
				var textWeights TextWeights
				err = json.Unmarshal(*v, &textWeights)
				if err != nil {
					return err
				}
				sp.TextWeights = &textWeights
			}
		case "functions":
			if v != nil {
				functions, err := unmarshalBasicScoringFunctionArray(*v)
				if err != nil {
					return err
				}
				sp.Functions = &functions
			}
		case "functionAggregation":
			if v != nil {
				var functionAggregation ScoringFunctionAggregation
				err = json.Unmarshal(*v, &functionAggregation)
				if err != nil {
					return err
				}
				sp.FunctionAggregation = functionAggregation
			}
		}
	}

	return nil
}

// SemanticConfiguration defines a specific configuration to be used in the context of semantic
// capabilities.
type SemanticConfiguration struct {
	// Name - The name of the semantic configuration.
	Name *string `json:"name,omitempty"`
	// PrioritizedFields - Describes the title, content, and keyword fields to be used for semantic ranking, captions, highlights, and answers. At least one of the three sub properties (titleField, prioritizedKeywordsFields and prioritizedContentFields) need to be set.
	PrioritizedFields *PrioritizedFields `json:"prioritizedFields,omitempty"`
}

// SemanticField a field that is used as part of the semantic configuration.
type SemanticField struct {
	FieldName *string `json:"fieldName,omitempty"`
}

// SemanticSettings defines parameters for a search index that influence semantic capabilities.
type SemanticSettings struct {
	// DefaultConfigurationName - Allows you to set the name of a default semantic configuration in your index, making it optional to pass it on as a query parameter every time.
	DefaultConfigurationName *string `json:"defaultConfiguration,omitempty"`
	// Configurations - The semantic configurations for the index.
	Configurations *[]SemanticConfiguration `json:"configurations,omitempty"`
}

// SentimentSkill this skill is deprecated. Use the V3.SentimentSkill instead.
type SentimentSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`. Possible values include: 'SentimentSkillLanguageDa', 'SentimentSkillLanguageNl', 'SentimentSkillLanguageEn', 'SentimentSkillLanguageFi', 'SentimentSkillLanguageFr', 'SentimentSkillLanguageDe', 'SentimentSkillLanguageEl', 'SentimentSkillLanguageIt', 'SentimentSkillLanguageNo', 'SentimentSkillLanguagePl', 'SentimentSkillLanguagePtPT', 'SentimentSkillLanguageRu', 'SentimentSkillLanguageEs', 'SentimentSkillLanguageSv', 'SentimentSkillLanguageTr'
	DefaultLanguageCode SentimentSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SentimentSkill.
func (ss SentimentSkill) MarshalJSON() ([]byte, error) {
	ss.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill
	objectMap := make(map[string]interface{})
	if ss.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ss.DefaultLanguageCode
	}
	if ss.Name != nil {
		objectMap["name"] = ss.Name
	}
	if ss.Description != nil {
		objectMap["description"] = ss.Description
	}
	if ss.Context != nil {
		objectMap["context"] = ss.Context
	}
	if ss.Inputs != nil {
		objectMap["inputs"] = ss.Inputs
	}
	if ss.Outputs != nil {
		objectMap["outputs"] = ss.Outputs
	}
	if ss.OdataType != "" {
		objectMap["@odata.type"] = ss.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return &ss, true
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ss, true
}

// SentimentSkillV3 using the Text Analytics API, evaluates unstructured text and for each record, provides
// sentiment labels (such as "negative", "neutral" and "positive") based on the highest confidence score
// found by the service at a sentence and document-level.
type SentimentSkillV3 struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`.
	DefaultLanguageCode *string `json:"defaultLanguageCode,omitempty"`
	// IncludeOpinionMining - If set to true, the skill output will include information from Text Analytics for opinion mining, namely targets (nouns or verbs) and their associated assessment (adjective) in the text. Default is false.
	IncludeOpinionMining *bool `json:"includeOpinionMining,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics service. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SentimentSkillV3.
func (ssv SentimentSkillV3) MarshalJSON() ([]byte, error) {
	ssv.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill
	objectMap := make(map[string]interface{})
	if ssv.DefaultLanguageCode != nil {
		objectMap["defaultLanguageCode"] = ssv.DefaultLanguageCode
	}
	if ssv.IncludeOpinionMining != nil {
		objectMap["includeOpinionMining"] = ssv.IncludeOpinionMining
	}
	if ssv.ModelVersion != nil {
		objectMap["modelVersion"] = ssv.ModelVersion
	}
	if ssv.Name != nil {
		objectMap["name"] = ssv.Name
	}
	if ssv.Description != nil {
		objectMap["description"] = ssv.Description
	}
	if ssv.Context != nil {
		objectMap["context"] = ssv.Context
	}
	if ssv.Inputs != nil {
		objectMap["inputs"] = ssv.Inputs
	}
	if ssv.Outputs != nil {
		objectMap["outputs"] = ssv.Outputs
	}
	if ssv.OdataType != "" {
		objectMap["@odata.type"] = ssv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return &ssv, true
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for SentimentSkillV3.
func (ssv SentimentSkillV3) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ssv, true
}

// ServiceCounters represents service-level resource counters and quotas.
type ServiceCounters struct {
	// AliasCounter - Total number of aliases.
	AliasCounter *ResourceCounter `json:"aliasesCount,omitempty"`
	// DocumentCounter - Total number of documents across all indexes in the service.
	DocumentCounter *ResourceCounter `json:"documentCount,omitempty"`
	// IndexCounter - Total number of indexes.
	IndexCounter *ResourceCounter `json:"indexesCount,omitempty"`
	// IndexerCounter - Total number of indexers.
	IndexerCounter *ResourceCounter `json:"indexersCount,omitempty"`
	// DataSourceCounter - Total number of data sources.
	DataSourceCounter *ResourceCounter `json:"dataSourcesCount,omitempty"`
	// StorageSizeCounter - Total size of used storage in bytes.
	StorageSizeCounter *ResourceCounter `json:"storageSize,omitempty"`
	// SynonymMapCounter - Total number of synonym maps.
	SynonymMapCounter *ResourceCounter `json:"synonymMaps,omitempty"`
	// SkillsetCounter - Total number of skillsets.
	SkillsetCounter *ResourceCounter `json:"skillsetCount,omitempty"`
	// VectorIndexSizeCounter - Total memory consumption of all vector indexes within the service, in bytes.
	VectorIndexSizeCounter *ResourceCounter `json:"vectorIndexSize,omitempty"`
}

// ServiceLimits represents various service level limits.
type ServiceLimits struct {
	// MaxFieldsPerIndex - The maximum allowed fields per index.
	MaxFieldsPerIndex *int32 `json:"maxFieldsPerIndex,omitempty"`
	// MaxFieldNestingDepthPerIndex - The maximum depth which you can nest sub-fields in an index, including the top-level complex field. For example, a/b/c has a nesting depth of 3.
	MaxFieldNestingDepthPerIndex *int32 `json:"maxFieldNestingDepthPerIndex,omitempty"`
	// MaxComplexCollectionFieldsPerIndex - The maximum number of fields of type Collection(Edm.ComplexType) allowed in an index.
	MaxComplexCollectionFieldsPerIndex *int32 `json:"maxComplexCollectionFieldsPerIndex,omitempty"`
	// MaxComplexObjectsInCollectionsPerDocument - The maximum number of objects in complex collections allowed per document.
	MaxComplexObjectsInCollectionsPerDocument *int32 `json:"maxComplexObjectsInCollectionsPerDocument,omitempty"`
}

// ServiceStatistics response from a get service statistics request. If successful, it includes service
// level counters and limits.
type ServiceStatistics struct {
	autorest.Response `json:"-"`
	// Counters - Service level resource counters.
	Counters *ServiceCounters `json:"counters,omitempty"`
	// Limits - Service level general limits.
	Limits *ServiceLimits `json:"limits,omitempty"`
}

// ShaperSkill a skill for reshaping the outputs. It creates a complex type to support composite fields
// (also known as multipart fields).
type ShaperSkill struct {
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ShaperSkill.
func (ss ShaperSkill) MarshalJSON() ([]byte, error) {
	ss.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill
	objectMap := make(map[string]interface{})
	if ss.Name != nil {
		objectMap["name"] = ss.Name
	}
	if ss.Description != nil {
		objectMap["description"] = ss.Description
	}
	if ss.Context != nil {
		objectMap["context"] = ss.Context
	}
	if ss.Inputs != nil {
		objectMap["inputs"] = ss.Inputs
	}
	if ss.Outputs != nil {
		objectMap["outputs"] = ss.Outputs
	}
	if ss.OdataType != "" {
		objectMap["@odata.type"] = ss.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return &ss, true
}

// AsMergeSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ss, true
}

// ShingleTokenFilter creates combinations of tokens as a single token. This token filter is implemented
// using Apache Lucene.
type ShingleTokenFilter struct {
	// MaxShingleSize - The maximum shingle size. Default and minimum value is 2.
	MaxShingleSize *int32 `json:"maxShingleSize,omitempty"`
	// MinShingleSize - The minimum shingle size. Default and minimum value is 2. Must be less than the value of maxShingleSize.
	MinShingleSize *int32 `json:"minShingleSize,omitempty"`
	// OutputUnigrams - A value indicating whether the output stream will contain the input tokens (unigrams) as well as shingles. Default is true.
	OutputUnigrams *bool `json:"outputUnigrams,omitempty"`
	// OutputUnigramsIfNoShingles - A value indicating whether to output unigrams for those times when no shingles are available. This property takes precedence when outputUnigrams is set to false. Default is false.
	OutputUnigramsIfNoShingles *bool `json:"outputUnigramsIfNoShingles,omitempty"`
	// TokenSeparator - The string to use when joining adjacent tokens to form a shingle. Default is a single space (" ").
	TokenSeparator *string `json:"tokenSeparator,omitempty"`
	// FilterToken - The string to insert for each position at which there is no token. Default is an underscore ("_").
	FilterToken *string `json:"filterToken,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ShingleTokenFilter.
func (stf ShingleTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter
	objectMap := make(map[string]interface{})
	if stf.MaxShingleSize != nil {
		objectMap["maxShingleSize"] = stf.MaxShingleSize
	}
	if stf.MinShingleSize != nil {
		objectMap["minShingleSize"] = stf.MinShingleSize
	}
	if stf.OutputUnigrams != nil {
		objectMap["outputUnigrams"] = stf.OutputUnigrams
	}
	if stf.OutputUnigramsIfNoShingles != nil {
		objectMap["outputUnigramsIfNoShingles"] = stf.OutputUnigramsIfNoShingles
	}
	if stf.TokenSeparator != nil {
		objectMap["tokenSeparator"] = stf.TokenSeparator
	}
	if stf.FilterToken != nil {
		objectMap["filterToken"] = stf.FilterToken
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return &stf, true
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// BasicSimilarity base type for similarity algorithms. Similarity algorithms are used to calculate scores that tie
// queries to documents. The higher the score, the more relevant the document is to that specific query. Those scores
// are used to rank the search results.
type BasicSimilarity interface {
	AsClassicSimilarity() (*ClassicSimilarity, bool)
	AsBM25Similarity() (*BM25Similarity, bool)
	AsSimilarity() (*Similarity, bool)
}

// Similarity base type for similarity algorithms. Similarity algorithms are used to calculate scores that tie
// queries to documents. The higher the score, the more relevant the document is to that specific query. Those
// scores are used to rank the search results.
type Similarity struct {
	// OdataType - Possible values include: 'OdataTypeBasicSimilarityOdataTypeSimilarity', 'OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchClassicSimilarity', 'OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchBM25Similarity'
	OdataType OdataTypeBasicSimilarity `json:"@odata.type,omitempty"`
}

func unmarshalBasicSimilarity(body []byte) (BasicSimilarity, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchClassicSimilarity):
		var cs ClassicSimilarity
		err := json.Unmarshal(body, &cs)
		return cs, err
	case string(OdataTypeBasicSimilarityOdataTypeMicrosoftAzureSearchBM25Similarity):
		var bs BM25Similarity
		err := json.Unmarshal(body, &bs)
		return bs, err
	default:
		var s Similarity
		err := json.Unmarshal(body, &s)
		return s, err
	}
}
func unmarshalBasicSimilarityArray(body []byte) ([]BasicSimilarity, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	sArray := make([]BasicSimilarity, len(rawMessages))

	for index, rawMessage := range rawMessages {
		s, err := unmarshalBasicSimilarity(*rawMessage)
		if err != nil {
			return nil, err
		}
		sArray[index] = s
	}
	return sArray, nil
}

// MarshalJSON is the custom marshaler for Similarity.
func (s Similarity) MarshalJSON() ([]byte, error) {
	s.OdataType = OdataTypeBasicSimilarityOdataTypeSimilarity
	objectMap := make(map[string]interface{})
	if s.OdataType != "" {
		objectMap["@odata.type"] = s.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicSimilarity is the BasicSimilarity implementation for Similarity.
func (s Similarity) AsClassicSimilarity() (*ClassicSimilarity, bool) {
	return nil, false
}

// AsBM25Similarity is the BasicSimilarity implementation for Similarity.
func (s Similarity) AsBM25Similarity() (*BM25Similarity, bool) {
	return nil, false
}

// AsSimilarity is the BasicSimilarity implementation for Similarity.
func (s Similarity) AsSimilarity() (*Similarity, bool) {
	return &s, true
}

// AsBasicSimilarity is the BasicSimilarity implementation for Similarity.
func (s Similarity) AsBasicSimilarity() (BasicSimilarity, bool) {
	return &s, true
}

// SkillNames ...
type SkillNames struct {
	// SkillNames - the names of skills to be reset.
	SkillNames *[]string `json:"skillNames,omitempty"`
}

// SnowballTokenFilter a filter that stems words using a Snowball-generated stemmer. This token filter is
// implemented using Apache Lucene.
type SnowballTokenFilter struct {
	// Language - The language to use. Possible values include: 'SnowballTokenFilterLanguageArmenian', 'SnowballTokenFilterLanguageBasque', 'SnowballTokenFilterLanguageCatalan', 'SnowballTokenFilterLanguageDanish', 'SnowballTokenFilterLanguageDutch', 'SnowballTokenFilterLanguageEnglish', 'SnowballTokenFilterLanguageFinnish', 'SnowballTokenFilterLanguageFrench', 'SnowballTokenFilterLanguageGerman', 'SnowballTokenFilterLanguageGerman2', 'SnowballTokenFilterLanguageHungarian', 'SnowballTokenFilterLanguageItalian', 'SnowballTokenFilterLanguageKp', 'SnowballTokenFilterLanguageLovins', 'SnowballTokenFilterLanguageNorwegian', 'SnowballTokenFilterLanguagePorter', 'SnowballTokenFilterLanguagePortuguese', 'SnowballTokenFilterLanguageRomanian', 'SnowballTokenFilterLanguageRussian', 'SnowballTokenFilterLanguageSpanish', 'SnowballTokenFilterLanguageSwedish', 'SnowballTokenFilterLanguageTurkish'
	Language SnowballTokenFilterLanguage `json:"language,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SnowballTokenFilter.
func (stf SnowballTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Language != "" {
		objectMap["language"] = stf.Language
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return &stf, true
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// SoftDeleteColumnDeletionDetectionPolicy defines a data deletion detection policy that implements a
// soft-deletion strategy. It determines whether an item should be deleted based on the value of a
// designated 'soft delete' column.
type SoftDeleteColumnDeletionDetectionPolicy struct {
	// SoftDeleteColumnName - The name of the column to use for soft-deletion detection.
	SoftDeleteColumnName *string `json:"softDeleteColumnName,omitempty"`
	// SoftDeleteMarkerValue - The marker value that identifies an item as deleted.
	SoftDeleteMarkerValue *string `json:"softDeleteMarkerValue,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeDataDeletionDetectionPolicy', 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy', 'OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchNativeBlobSoftDeleteDeletionDetectionPolicy'
	OdataType OdataTypeBasicDataDeletionDetectionPolicy `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) MarshalJSON() ([]byte, error) {
	sdcddp.OdataType = OdataTypeBasicDataDeletionDetectionPolicyOdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy
	objectMap := make(map[string]interface{})
	if sdcddp.SoftDeleteColumnName != nil {
		objectMap["softDeleteColumnName"] = sdcddp.SoftDeleteColumnName
	}
	if sdcddp.SoftDeleteMarkerValue != nil {
		objectMap["softDeleteMarkerValue"] = sdcddp.SoftDeleteMarkerValue
	}
	if sdcddp.OdataType != "" {
		objectMap["@odata.type"] = sdcddp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsSoftDeleteColumnDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) AsSoftDeleteColumnDeletionDetectionPolicy() (*SoftDeleteColumnDeletionDetectionPolicy, bool) {
	return &sdcddp, true
}

// AsNativeBlobSoftDeleteDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) AsNativeBlobSoftDeleteDeletionDetectionPolicy() (*NativeBlobSoftDeleteDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) AsDataDeletionDetectionPolicy() (*DataDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsBasicDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) AsBasicDataDeletionDetectionPolicy() (BasicDataDeletionDetectionPolicy, bool) {
	return &sdcddp, true
}

// SplitSkill a skill to split a string into chunks of text.
type SplitSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is `en`. Possible values include: 'SplitSkillLanguageAm', 'SplitSkillLanguageBs', 'SplitSkillLanguageCs', 'SplitSkillLanguageDa', 'SplitSkillLanguageDe', 'SplitSkillLanguageEn', 'SplitSkillLanguageEs', 'SplitSkillLanguageEt', 'SplitSkillLanguageFi', 'SplitSkillLanguageFr', 'SplitSkillLanguageHe', 'SplitSkillLanguageHi', 'SplitSkillLanguageHr', 'SplitSkillLanguageHu', 'SplitSkillLanguageID', 'SplitSkillLanguageIs', 'SplitSkillLanguageIt', 'SplitSkillLanguageJa', 'SplitSkillLanguageKo', 'SplitSkillLanguageLv', 'SplitSkillLanguageNb', 'SplitSkillLanguageNl', 'SplitSkillLanguagePl', 'SplitSkillLanguagePt', 'SplitSkillLanguagePtBr', 'SplitSkillLanguageRu', 'SplitSkillLanguageSk', 'SplitSkillLanguageSl', 'SplitSkillLanguageSr', 'SplitSkillLanguageSv', 'SplitSkillLanguageTr', 'SplitSkillLanguageUr', 'SplitSkillLanguageZh'
	DefaultLanguageCode SplitSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// TextSplitMode - A value indicating which split mode to perform. Possible values include: 'TextSplitModePages', 'TextSplitModeSentences'
	TextSplitMode TextSplitMode `json:"textSplitMode,omitempty"`
	// MaximumPageLength - The desired maximum page length. Default is 10000.
	MaximumPageLength *int32 `json:"maximumPageLength,omitempty"`
	// PageOverlapLength - Only applicable when textSplitMode is set to 'pages'. If specified, n+1th chunk will start with this number of characters/tokens from the end of the nth chunk.
	PageOverlapLength *int32 `json:"pageOverlapLength,omitempty"`
	// MaximumPagesToTake - Only applicable when textSplitMode is set to 'pages'. If specified, the SplitSkill will discontinue splitting after processing the first 'maximumPagesToTake' pages, in order to improve performance when only a few initial pages are needed from each document.
	MaximumPagesToTake *int32 `json:"maximumPagesToTake,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SplitSkill.
func (ss SplitSkill) MarshalJSON() ([]byte, error) {
	ss.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill
	objectMap := make(map[string]interface{})
	if ss.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ss.DefaultLanguageCode
	}
	if ss.TextSplitMode != "" {
		objectMap["textSplitMode"] = ss.TextSplitMode
	}
	if ss.MaximumPageLength != nil {
		objectMap["maximumPageLength"] = ss.MaximumPageLength
	}
	if ss.PageOverlapLength != nil {
		objectMap["pageOverlapLength"] = ss.PageOverlapLength
	}
	if ss.MaximumPagesToTake != nil {
		objectMap["maximumPagesToTake"] = ss.MaximumPagesToTake
	}
	if ss.Name != nil {
		objectMap["name"] = ss.Name
	}
	if ss.Description != nil {
		objectMap["description"] = ss.Description
	}
	if ss.Context != nil {
		objectMap["context"] = ss.Context
	}
	if ss.Inputs != nil {
		objectMap["inputs"] = ss.Inputs
	}
	if ss.Outputs != nil {
		objectMap["outputs"] = ss.Outputs
	}
	if ss.OdataType != "" {
		objectMap["@odata.type"] = ss.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsSplitSkill() (*SplitSkill, bool) {
	return &ss, true
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for SplitSkill.
func (ss SplitSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &ss, true
}

// StemmerOverrideTokenFilter provides the ability to override other stemming filters with custom
// dictionary-based stemming. Any dictionary-stemmed terms will be marked as keywords so that they will not
// be stemmed with stemmers down the chain. Must be placed before any stemming filters. This token filter
// is implemented using Apache Lucene.
type StemmerOverrideTokenFilter struct {
	// Rules - A list of stemming rules in the following format: "word => stem", for example: "ran => run".
	Rules *[]string `json:"rules,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) MarshalJSON() ([]byte, error) {
	sotf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter
	objectMap := make(map[string]interface{})
	if sotf.Rules != nil {
		objectMap["rules"] = sotf.Rules
	}
	if sotf.Name != nil {
		objectMap["name"] = sotf.Name
	}
	if sotf.OdataType != "" {
		objectMap["@odata.type"] = sotf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return &sotf, true
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &sotf, true
}

// StemmerTokenFilter language specific stemming filter. This token filter is implemented using Apache
// Lucene.
type StemmerTokenFilter struct {
	// Language - The language to use. Possible values include: 'StemmerTokenFilterLanguageArabic', 'StemmerTokenFilterLanguageArmenian', 'StemmerTokenFilterLanguageBasque', 'StemmerTokenFilterLanguageBrazilian', 'StemmerTokenFilterLanguageBulgarian', 'StemmerTokenFilterLanguageCatalan', 'StemmerTokenFilterLanguageCzech', 'StemmerTokenFilterLanguageDanish', 'StemmerTokenFilterLanguageDutch', 'StemmerTokenFilterLanguageDutchKp', 'StemmerTokenFilterLanguageEnglish', 'StemmerTokenFilterLanguageLightEnglish', 'StemmerTokenFilterLanguageMinimalEnglish', 'StemmerTokenFilterLanguagePossessiveEnglish', 'StemmerTokenFilterLanguagePorter2', 'StemmerTokenFilterLanguageLovins', 'StemmerTokenFilterLanguageFinnish', 'StemmerTokenFilterLanguageLightFinnish', 'StemmerTokenFilterLanguageFrench', 'StemmerTokenFilterLanguageLightFrench', 'StemmerTokenFilterLanguageMinimalFrench', 'StemmerTokenFilterLanguageGalician', 'StemmerTokenFilterLanguageMinimalGalician', 'StemmerTokenFilterLanguageGerman', 'StemmerTokenFilterLanguageGerman2', 'StemmerTokenFilterLanguageLightGerman', 'StemmerTokenFilterLanguageMinimalGerman', 'StemmerTokenFilterLanguageGreek', 'StemmerTokenFilterLanguageHindi', 'StemmerTokenFilterLanguageHungarian', 'StemmerTokenFilterLanguageLightHungarian', 'StemmerTokenFilterLanguageIndonesian', 'StemmerTokenFilterLanguageIrish', 'StemmerTokenFilterLanguageItalian', 'StemmerTokenFilterLanguageLightItalian', 'StemmerTokenFilterLanguageSorani', 'StemmerTokenFilterLanguageLatvian', 'StemmerTokenFilterLanguageNorwegian', 'StemmerTokenFilterLanguageLightNorwegian', 'StemmerTokenFilterLanguageMinimalNorwegian', 'StemmerTokenFilterLanguageLightNynorsk', 'StemmerTokenFilterLanguageMinimalNynorsk', 'StemmerTokenFilterLanguagePortuguese', 'StemmerTokenFilterLanguageLightPortuguese', 'StemmerTokenFilterLanguageMinimalPortuguese', 'StemmerTokenFilterLanguagePortugueseRslp', 'StemmerTokenFilterLanguageRomanian', 'StemmerTokenFilterLanguageRussian', 'StemmerTokenFilterLanguageLightRussian', 'StemmerTokenFilterLanguageSpanish', 'StemmerTokenFilterLanguageLightSpanish', 'StemmerTokenFilterLanguageSwedish', 'StemmerTokenFilterLanguageLightSwedish', 'StemmerTokenFilterLanguageTurkish'
	Language StemmerTokenFilterLanguage `json:"language,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StemmerTokenFilter.
func (stf StemmerTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Language != "" {
		objectMap["language"] = stf.Language
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return &stf, true
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// StopAnalyzer divides text at non-letters; Applies the lowercase and stopword token filters. This
// analyzer is implemented using Apache Lucene.
type StopAnalyzer struct {
	// Stopwords - A list of stopwords.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeLexicalAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StopAnalyzer.
func (sa StopAnalyzer) MarshalJSON() ([]byte, error) {
	sa.OdataType = OdataTypeMicrosoftAzureSearchStopAnalyzer
	objectMap := make(map[string]interface{})
	if sa.Stopwords != nil {
		objectMap["stopwords"] = sa.Stopwords
	}
	if sa.Name != nil {
		objectMap["name"] = sa.Name
	}
	if sa.OdataType != "" {
		objectMap["@odata.type"] = sa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicLexicalAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicLexicalAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsLuceneStandardAnalyzer is the BasicLexicalAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsLuceneStandardAnalyzer() (*LuceneStandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicLexicalAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return &sa, true
}

// AsLexicalAnalyzer is the BasicLexicalAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsLexicalAnalyzer() (*LexicalAnalyzer, bool) {
	return nil, false
}

// AsBasicLexicalAnalyzer is the BasicLexicalAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsBasicLexicalAnalyzer() (BasicLexicalAnalyzer, bool) {
	return &sa, true
}

// StopwordsTokenFilter removes stop words from a token stream. This token filter is implemented using
// Apache Lucene.
type StopwordsTokenFilter struct {
	// Stopwords - The list of stopwords. This property and the stopwords list property cannot both be set.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// StopwordsList - A predefined list of stopwords to use. This property and the stopwords property cannot both be set. Default is English. Possible values include: 'StopwordsListArabic', 'StopwordsListArmenian', 'StopwordsListBasque', 'StopwordsListBrazilian', 'StopwordsListBulgarian', 'StopwordsListCatalan', 'StopwordsListCzech', 'StopwordsListDanish', 'StopwordsListDutch', 'StopwordsListEnglish', 'StopwordsListFinnish', 'StopwordsListFrench', 'StopwordsListGalician', 'StopwordsListGerman', 'StopwordsListGreek', 'StopwordsListHindi', 'StopwordsListHungarian', 'StopwordsListIndonesian', 'StopwordsListIrish', 'StopwordsListItalian', 'StopwordsListLatvian', 'StopwordsListNorwegian', 'StopwordsListPersian', 'StopwordsListPortuguese', 'StopwordsListRomanian', 'StopwordsListRussian', 'StopwordsListSorani', 'StopwordsListSpanish', 'StopwordsListSwedish', 'StopwordsListThai', 'StopwordsListTurkish'
	StopwordsList StopwordsList `json:"stopwordsList,omitempty"`
	// IgnoreCase - A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// RemoveTrailingStopWords - A value indicating whether to ignore the last search term if it's a stop word. Default is true.
	RemoveTrailingStopWords *bool `json:"removeTrailing,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Stopwords != nil {
		objectMap["stopwords"] = stf.Stopwords
	}
	if stf.StopwordsList != "" {
		objectMap["stopwordsList"] = stf.StopwordsList
	}
	if stf.IgnoreCase != nil {
		objectMap["ignoreCase"] = stf.IgnoreCase
	}
	if stf.RemoveTrailingStopWords != nil {
		objectMap["removeTrailing"] = stf.RemoveTrailingStopWords
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return &stf, true
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// Suggester defines how the Suggest API should apply to a group of fields in the index.
type Suggester struct {
	// Name - The name of the suggester.
	Name *string `json:"name,omitempty"`
	// SearchMode - A value indicating the capabilities of the suggester.
	SearchMode *string `json:"searchMode,omitempty"`
	// SourceFields - The list of field names to which the suggester applies. Each field must be searchable.
	SourceFields *[]string `json:"sourceFields,omitempty"`
}

// SynonymMap represents a synonym map definition.
type SynonymMap struct {
	autorest.Response `json:"-"`
	// Name - The name of the synonym map.
	Name *string `json:"name,omitempty"`
	// Format - The format of the synonym map. Only the 'solr' format is currently supported.
	Format *string `json:"format,omitempty"`
	// Synonyms - A series of synonym rules in the specified synonym map format. The rules must be separated by newlines.
	Synonyms *string `json:"synonyms,omitempty"`
	// EncryptionKey - A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level of encryption-at-rest for your data when you want full assurance that no one, not even Microsoft, can decrypt your data. Once you have encrypted your data, it will always remain encrypted. The search service will ignore attempts to set this property to null. You can change this property as needed if you want to rotate your encryption key; Your data will be unaffected. Encryption with customer-managed keys is not available for free search services, and is only available for paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey `json:"encryptionKey,omitempty"`
	// ETag - The ETag of the synonym map.
	ETag *string `json:"@odata.etag,omitempty"`
}

// SynonymTokenFilter matches single or multi-word synonyms in a token stream. This token filter is
// implemented using Apache Lucene.
type SynonymTokenFilter struct {
	// Synonyms - A list of synonyms in following one of two formats: 1. incredible, unbelievable, fabulous => amazing - all terms on the left side of => symbol will be replaced with all terms on its right side; 2. incredible, unbelievable, fabulous, amazing - comma separated list of equivalent words. Set the expand option to change how this list is interpreted.
	Synonyms *[]string `json:"synonyms,omitempty"`
	// IgnoreCase - A value indicating whether to case-fold input for matching. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// Expand - A value indicating whether all words in the list of synonyms (if => notation is not used) will map to one another. If true, all words in the list of synonyms (if => notation is not used) will map to one another. The following list: incredible, unbelievable, fabulous, amazing is equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable, fabulous, amazing. If false, the following list: incredible, unbelievable, fabulous, amazing will be equivalent to: incredible, unbelievable, fabulous, amazing => incredible. Default is true.
	Expand *bool `json:"expand,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SynonymTokenFilter.
func (stf SynonymTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Synonyms != nil {
		objectMap["synonyms"] = stf.Synonyms
	}
	if stf.IgnoreCase != nil {
		objectMap["ignoreCase"] = stf.IgnoreCase
	}
	if stf.Expand != nil {
		objectMap["expand"] = stf.Expand
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return &stf, true
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// TagScoringFunction defines a function that boosts scores of documents with string values matching a
// given list of tags.
type TagScoringFunction struct {
	// Parameters - Parameter values for the tag scoring function.
	Parameters *TagScoringParameters `json:"tag,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'ScoringFunctionInterpolationLinear', 'ScoringFunctionInterpolationConstant', 'ScoringFunctionInterpolationQuadratic', 'ScoringFunctionInterpolationLogarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for TagScoringFunction.
func (tsf TagScoringFunction) MarshalJSON() ([]byte, error) {
	tsf.Type = TypeTag
	objectMap := make(map[string]interface{})
	if tsf.Parameters != nil {
		objectMap["tag"] = tsf.Parameters
	}
	if tsf.FieldName != nil {
		objectMap["fieldName"] = tsf.FieldName
	}
	if tsf.Boost != nil {
		objectMap["boost"] = tsf.Boost
	}
	if tsf.Interpolation != "" {
		objectMap["interpolation"] = tsf.Interpolation
	}
	if tsf.Type != "" {
		objectMap["type"] = tsf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return &tsf, true
}

// AsScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &tsf, true
}

// TagScoringParameters provides parameter values to a tag scoring function.
type TagScoringParameters struct {
	// TagsParameter - The name of the parameter passed in search queries to specify the list of tags to compare against the target field.
	TagsParameter *string `json:"tagsParameter,omitempty"`
}

// TextTranslationSkill a skill to translate text from one language to another.
type TextTranslationSkill struct {
	// DefaultToLanguageCode - The language code to translate documents into for documents that don't specify the to language explicitly. Possible values include: 'TextTranslationSkillLanguageAf', 'TextTranslationSkillLanguageAr', 'TextTranslationSkillLanguageBn', 'TextTranslationSkillLanguageBs', 'TextTranslationSkillLanguageBg', 'TextTranslationSkillLanguageYue', 'TextTranslationSkillLanguageCa', 'TextTranslationSkillLanguageZhHans', 'TextTranslationSkillLanguageZhHant', 'TextTranslationSkillLanguageHr', 'TextTranslationSkillLanguageCs', 'TextTranslationSkillLanguageDa', 'TextTranslationSkillLanguageNl', 'TextTranslationSkillLanguageEn', 'TextTranslationSkillLanguageEt', 'TextTranslationSkillLanguageFj', 'TextTranslationSkillLanguageFil', 'TextTranslationSkillLanguageFi', 'TextTranslationSkillLanguageFr', 'TextTranslationSkillLanguageDe', 'TextTranslationSkillLanguageEl', 'TextTranslationSkillLanguageHt', 'TextTranslationSkillLanguageHe', 'TextTranslationSkillLanguageHi', 'TextTranslationSkillLanguageMww', 'TextTranslationSkillLanguageHu', 'TextTranslationSkillLanguageIs', 'TextTranslationSkillLanguageID', 'TextTranslationSkillLanguageIt', 'TextTranslationSkillLanguageJa', 'TextTranslationSkillLanguageSw', 'TextTranslationSkillLanguageTlh', 'TextTranslationSkillLanguageTlhLatn', 'TextTranslationSkillLanguageTlhPiqd', 'TextTranslationSkillLanguageKo', 'TextTranslationSkillLanguageLv', 'TextTranslationSkillLanguageLt', 'TextTranslationSkillLanguageMg', 'TextTranslationSkillLanguageMs', 'TextTranslationSkillLanguageMt', 'TextTranslationSkillLanguageNb', 'TextTranslationSkillLanguageFa', 'TextTranslationSkillLanguagePl', 'TextTranslationSkillLanguagePt', 'TextTranslationSkillLanguagePtBr', 'TextTranslationSkillLanguagePtPT', 'TextTranslationSkillLanguageOtq', 'TextTranslationSkillLanguageRo', 'TextTranslationSkillLanguageRu', 'TextTranslationSkillLanguageSm', 'TextTranslationSkillLanguageSrCyrl', 'TextTranslationSkillLanguageSrLatn', 'TextTranslationSkillLanguageSk', 'TextTranslationSkillLanguageSl', 'TextTranslationSkillLanguageEs', 'TextTranslationSkillLanguageSv', 'TextTranslationSkillLanguageTy', 'TextTranslationSkillLanguageTa', 'TextTranslationSkillLanguageTe', 'TextTranslationSkillLanguageTh', 'TextTranslationSkillLanguageTo', 'TextTranslationSkillLanguageTr', 'TextTranslationSkillLanguageUk', 'TextTranslationSkillLanguageUr', 'TextTranslationSkillLanguageVi', 'TextTranslationSkillLanguageCy', 'TextTranslationSkillLanguageYua', 'TextTranslationSkillLanguageGa', 'TextTranslationSkillLanguageKn', 'TextTranslationSkillLanguageMi', 'TextTranslationSkillLanguageMl', 'TextTranslationSkillLanguagePa'
	DefaultToLanguageCode TextTranslationSkillLanguage `json:"defaultToLanguageCode,omitempty"`
	// DefaultFromLanguageCode - The language code to translate documents from for documents that don't specify the from language explicitly. Possible values include: 'TextTranslationSkillLanguageAf', 'TextTranslationSkillLanguageAr', 'TextTranslationSkillLanguageBn', 'TextTranslationSkillLanguageBs', 'TextTranslationSkillLanguageBg', 'TextTranslationSkillLanguageYue', 'TextTranslationSkillLanguageCa', 'TextTranslationSkillLanguageZhHans', 'TextTranslationSkillLanguageZhHant', 'TextTranslationSkillLanguageHr', 'TextTranslationSkillLanguageCs', 'TextTranslationSkillLanguageDa', 'TextTranslationSkillLanguageNl', 'TextTranslationSkillLanguageEn', 'TextTranslationSkillLanguageEt', 'TextTranslationSkillLanguageFj', 'TextTranslationSkillLanguageFil', 'TextTranslationSkillLanguageFi', 'TextTranslationSkillLanguageFr', 'TextTranslationSkillLanguageDe', 'TextTranslationSkillLanguageEl', 'TextTranslationSkillLanguageHt', 'TextTranslationSkillLanguageHe', 'TextTranslationSkillLanguageHi', 'TextTranslationSkillLanguageMww', 'TextTranslationSkillLanguageHu', 'TextTranslationSkillLanguageIs', 'TextTranslationSkillLanguageID', 'TextTranslationSkillLanguageIt', 'TextTranslationSkillLanguageJa', 'TextTranslationSkillLanguageSw', 'TextTranslationSkillLanguageTlh', 'TextTranslationSkillLanguageTlhLatn', 'TextTranslationSkillLanguageTlhPiqd', 'TextTranslationSkillLanguageKo', 'TextTranslationSkillLanguageLv', 'TextTranslationSkillLanguageLt', 'TextTranslationSkillLanguageMg', 'TextTranslationSkillLanguageMs', 'TextTranslationSkillLanguageMt', 'TextTranslationSkillLanguageNb', 'TextTranslationSkillLanguageFa', 'TextTranslationSkillLanguagePl', 'TextTranslationSkillLanguagePt', 'TextTranslationSkillLanguagePtBr', 'TextTranslationSkillLanguagePtPT', 'TextTranslationSkillLanguageOtq', 'TextTranslationSkillLanguageRo', 'TextTranslationSkillLanguageRu', 'TextTranslationSkillLanguageSm', 'TextTranslationSkillLanguageSrCyrl', 'TextTranslationSkillLanguageSrLatn', 'TextTranslationSkillLanguageSk', 'TextTranslationSkillLanguageSl', 'TextTranslationSkillLanguageEs', 'TextTranslationSkillLanguageSv', 'TextTranslationSkillLanguageTy', 'TextTranslationSkillLanguageTa', 'TextTranslationSkillLanguageTe', 'TextTranslationSkillLanguageTh', 'TextTranslationSkillLanguageTo', 'TextTranslationSkillLanguageTr', 'TextTranslationSkillLanguageUk', 'TextTranslationSkillLanguageUr', 'TextTranslationSkillLanguageVi', 'TextTranslationSkillLanguageCy', 'TextTranslationSkillLanguageYua', 'TextTranslationSkillLanguageGa', 'TextTranslationSkillLanguageKn', 'TextTranslationSkillLanguageMi', 'TextTranslationSkillLanguageMl', 'TextTranslationSkillLanguagePa'
	DefaultFromLanguageCode TextTranslationSkillLanguage `json:"defaultFromLanguageCode,omitempty"`
	// SuggestedFrom - The language code to translate documents from when neither the fromLanguageCode input nor the defaultFromLanguageCode parameter are provided, and the automatic language detection is unsuccessful. Default is `en`. Possible values include: 'TextTranslationSkillLanguageAf', 'TextTranslationSkillLanguageAr', 'TextTranslationSkillLanguageBn', 'TextTranslationSkillLanguageBs', 'TextTranslationSkillLanguageBg', 'TextTranslationSkillLanguageYue', 'TextTranslationSkillLanguageCa', 'TextTranslationSkillLanguageZhHans', 'TextTranslationSkillLanguageZhHant', 'TextTranslationSkillLanguageHr', 'TextTranslationSkillLanguageCs', 'TextTranslationSkillLanguageDa', 'TextTranslationSkillLanguageNl', 'TextTranslationSkillLanguageEn', 'TextTranslationSkillLanguageEt', 'TextTranslationSkillLanguageFj', 'TextTranslationSkillLanguageFil', 'TextTranslationSkillLanguageFi', 'TextTranslationSkillLanguageFr', 'TextTranslationSkillLanguageDe', 'TextTranslationSkillLanguageEl', 'TextTranslationSkillLanguageHt', 'TextTranslationSkillLanguageHe', 'TextTranslationSkillLanguageHi', 'TextTranslationSkillLanguageMww', 'TextTranslationSkillLanguageHu', 'TextTranslationSkillLanguageIs', 'TextTranslationSkillLanguageID', 'TextTranslationSkillLanguageIt', 'TextTranslationSkillLanguageJa', 'TextTranslationSkillLanguageSw', 'TextTranslationSkillLanguageTlh', 'TextTranslationSkillLanguageTlhLatn', 'TextTranslationSkillLanguageTlhPiqd', 'TextTranslationSkillLanguageKo', 'TextTranslationSkillLanguageLv', 'TextTranslationSkillLanguageLt', 'TextTranslationSkillLanguageMg', 'TextTranslationSkillLanguageMs', 'TextTranslationSkillLanguageMt', 'TextTranslationSkillLanguageNb', 'TextTranslationSkillLanguageFa', 'TextTranslationSkillLanguagePl', 'TextTranslationSkillLanguagePt', 'TextTranslationSkillLanguagePtBr', 'TextTranslationSkillLanguagePtPT', 'TextTranslationSkillLanguageOtq', 'TextTranslationSkillLanguageRo', 'TextTranslationSkillLanguageRu', 'TextTranslationSkillLanguageSm', 'TextTranslationSkillLanguageSrCyrl', 'TextTranslationSkillLanguageSrLatn', 'TextTranslationSkillLanguageSk', 'TextTranslationSkillLanguageSl', 'TextTranslationSkillLanguageEs', 'TextTranslationSkillLanguageSv', 'TextTranslationSkillLanguageTy', 'TextTranslationSkillLanguageTa', 'TextTranslationSkillLanguageTe', 'TextTranslationSkillLanguageTh', 'TextTranslationSkillLanguageTo', 'TextTranslationSkillLanguageTr', 'TextTranslationSkillLanguageUk', 'TextTranslationSkillLanguageUr', 'TextTranslationSkillLanguageVi', 'TextTranslationSkillLanguageCy', 'TextTranslationSkillLanguageYua', 'TextTranslationSkillLanguageGa', 'TextTranslationSkillLanguageKn', 'TextTranslationSkillLanguageMi', 'TextTranslationSkillLanguageMl', 'TextTranslationSkillLanguagePa'
	SuggestedFrom TextTranslationSkillLanguage `json:"suggestedFrom,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for TextTranslationSkill.
func (tts TextTranslationSkill) MarshalJSON() ([]byte, error) {
	tts.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill
	objectMap := make(map[string]interface{})
	if tts.DefaultToLanguageCode != "" {
		objectMap["defaultToLanguageCode"] = tts.DefaultToLanguageCode
	}
	if tts.DefaultFromLanguageCode != "" {
		objectMap["defaultFromLanguageCode"] = tts.DefaultFromLanguageCode
	}
	if tts.SuggestedFrom != "" {
		objectMap["suggestedFrom"] = tts.SuggestedFrom
	}
	if tts.Name != nil {
		objectMap["name"] = tts.Name
	}
	if tts.Description != nil {
		objectMap["description"] = tts.Description
	}
	if tts.Context != nil {
		objectMap["context"] = tts.Context
	}
	if tts.Inputs != nil {
		objectMap["inputs"] = tts.Inputs
	}
	if tts.Outputs != nil {
		objectMap["outputs"] = tts.Outputs
	}
	if tts.OdataType != "" {
		objectMap["@odata.type"] = tts.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return &tts, true
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsAmlSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &tts, true
}

// TextWeights defines weights on index fields for which matches should boost scoring in search queries.
type TextWeights struct {
	// Weights - The dictionary of per-field weights to boost document scoring. The keys are field names and the values are the weights for each field.
	Weights map[string]*float64 `json:"weights"`
}

// MarshalJSON is the custom marshaler for TextWeights.
func (tw TextWeights) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if tw.Weights != nil {
		objectMap["weights"] = tw.Weights
	}
	return json.Marshal(objectMap)
}

// BasicTokenFilter base type for token filters.
type BasicTokenFilter interface {
	AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool)
	AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool)
	AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool)
	AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool)
	AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool)
	AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool)
	AsElisionTokenFilter() (*ElisionTokenFilter, bool)
	AsKeepTokenFilter() (*KeepTokenFilter, bool)
	AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool)
	AsLengthTokenFilter() (*LengthTokenFilter, bool)
	AsLimitTokenFilter() (*LimitTokenFilter, bool)
	AsNGramTokenFilter() (*NGramTokenFilter, bool)
	AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool)
	AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool)
	AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool)
	AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool)
	AsShingleTokenFilter() (*ShingleTokenFilter, bool)
	AsSnowballTokenFilter() (*SnowballTokenFilter, bool)
	AsStemmerTokenFilter() (*StemmerTokenFilter, bool)
	AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool)
	AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool)
	AsSynonymTokenFilter() (*SynonymTokenFilter, bool)
	AsTruncateTokenFilter() (*TruncateTokenFilter, bool)
	AsUniqueTokenFilter() (*UniqueTokenFilter, bool)
	AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool)
	AsTokenFilter() (*TokenFilter, bool)
}

// TokenFilter base type for token filters.
type TokenFilter struct {
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

func unmarshalBasicTokenFilter(body []byte) (BasicTokenFilter, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter):
		var aftf ASCIIFoldingTokenFilter
		err := json.Unmarshal(body, &aftf)
		return aftf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter):
		var cbtf CjkBigramTokenFilter
		err := json.Unmarshal(body, &cbtf)
		return cbtf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter):
		var cgtf CommonGramTokenFilter
		err := json.Unmarshal(body, &cgtf)
		return cgtf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter):
		var ddtf DictionaryDecompounderTokenFilter
		err := json.Unmarshal(body, &ddtf)
		return ddtf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter):
		var engtf EdgeNGramTokenFilter
		err := json.Unmarshal(body, &engtf)
		return engtf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2):
		var engtfv EdgeNGramTokenFilterV2
		err := json.Unmarshal(body, &engtfv)
		return engtfv, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter):
		var etf ElisionTokenFilter
		err := json.Unmarshal(body, &etf)
		return etf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter):
		var ktf KeepTokenFilter
		err := json.Unmarshal(body, &ktf)
		return ktf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter):
		var kmtf KeywordMarkerTokenFilter
		err := json.Unmarshal(body, &kmtf)
		return kmtf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter):
		var ltf LengthTokenFilter
		err := json.Unmarshal(body, &ltf)
		return ltf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter):
		var ltf LimitTokenFilter
		err := json.Unmarshal(body, &ltf)
		return ltf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter):
		var ngtf NGramTokenFilter
		err := json.Unmarshal(body, &ngtf)
		return ngtf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2):
		var ngtfv NGramTokenFilterV2
		err := json.Unmarshal(body, &ngtfv)
		return ngtfv, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter):
		var pctf PatternCaptureTokenFilter
		err := json.Unmarshal(body, &pctf)
		return pctf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter):
		var prtf PatternReplaceTokenFilter
		err := json.Unmarshal(body, &prtf)
		return prtf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter):
		var ptf PhoneticTokenFilter
		err := json.Unmarshal(body, &ptf)
		return ptf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter):
		var stf ShingleTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter):
		var stf SnowballTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter):
		var stf StemmerTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter):
		var sotf StemmerOverrideTokenFilter
		err := json.Unmarshal(body, &sotf)
		return sotf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter):
		var stf StopwordsTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter):
		var stf SynonymTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter):
		var ttf TruncateTokenFilter
		err := json.Unmarshal(body, &ttf)
		return ttf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter):
		var utf UniqueTokenFilter
		err := json.Unmarshal(body, &utf)
		return utf, err
	case string(OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter):
		var wdtf WordDelimiterTokenFilter
		err := json.Unmarshal(body, &wdtf)
		return wdtf, err
	default:
		var tf TokenFilter
		err := json.Unmarshal(body, &tf)
		return tf, err
	}
}
func unmarshalBasicTokenFilterArray(body []byte) ([]BasicTokenFilter, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	tfArray := make([]BasicTokenFilter, len(rawMessages))

	for index, rawMessage := range rawMessages {
		tf, err := unmarshalBasicTokenFilter(*rawMessage)
		if err != nil {
			return nil, err
		}
		tfArray[index] = tf
	}
	return tfArray, nil
}

// MarshalJSON is the custom marshaler for TokenFilter.
func (tf TokenFilter) MarshalJSON() ([]byte, error) {
	tf.OdataType = OdataTypeBasicTokenFilterOdataTypeTokenFilter
	objectMap := make(map[string]interface{})
	if tf.Name != nil {
		objectMap["name"] = tf.Name
	}
	if tf.OdataType != "" {
		objectMap["@odata.type"] = tf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return &tf, true
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &tf, true
}

// TruncateTokenFilter truncates the terms to a specific length. This token filter is implemented using
// Apache Lucene.
type TruncateTokenFilter struct {
	// Length - The length at which terms will be truncated. Default and maximum is 300.
	Length *int32 `json:"length,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for TruncateTokenFilter.
func (ttf TruncateTokenFilter) MarshalJSON() ([]byte, error) {
	ttf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter
	objectMap := make(map[string]interface{})
	if ttf.Length != nil {
		objectMap["length"] = ttf.Length
	}
	if ttf.Name != nil {
		objectMap["name"] = ttf.Name
	}
	if ttf.OdataType != "" {
		objectMap["@odata.type"] = ttf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return &ttf, true
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ttf, true
}

// UaxURLEmailTokenizer tokenizes urls and emails as one token. This tokenizer is implemented using Apache
// Lucene.
type UaxURLEmailTokenizer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicLexicalTokenizerOdataTypeLexicalTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicLexicalTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) MarshalJSON() ([]byte, error) {
	uuet.OdataType = OdataTypeBasicLexicalTokenizerOdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer
	objectMap := make(map[string]interface{})
	if uuet.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = uuet.MaxTokenLength
	}
	if uuet.Name != nil {
		objectMap["name"] = uuet.Name
	}
	if uuet.OdataType != "" {
		objectMap["@odata.type"] = uuet.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsLuceneStandardTokenizer() (*LuceneStandardTokenizer, bool) {
	return nil, false
}

// AsLuceneStandardTokenizerV2 is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsLuceneStandardTokenizerV2() (*LuceneStandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return &uuet, true
}

// AsLexicalTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsLexicalTokenizer() (*LexicalTokenizer, bool) {
	return nil, false
}

// AsBasicLexicalTokenizer is the BasicLexicalTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsBasicLexicalTokenizer() (BasicLexicalTokenizer, bool) {
	return &uuet, true
}

// UniqueTokenFilter filters out tokens with same text as the previous token. This token filter is
// implemented using Apache Lucene.
type UniqueTokenFilter struct {
	// OnlyOnSamePosition - A value indicating whether to remove duplicates only at the same position. Default is false.
	OnlyOnSamePosition *bool `json:"onlyOnSamePosition,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for UniqueTokenFilter.
func (utf UniqueTokenFilter) MarshalJSON() ([]byte, error) {
	utf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter
	objectMap := make(map[string]interface{})
	if utf.OnlyOnSamePosition != nil {
		objectMap["onlyOnSamePosition"] = utf.OnlyOnSamePosition
	}
	if utf.Name != nil {
		objectMap["name"] = utf.Name
	}
	if utf.OdataType != "" {
		objectMap["@odata.type"] = utf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return &utf, true
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &utf, true
}

// VectorSearch contains configuration options related to vector search.
type VectorSearch struct {
	// Profiles - Defines combinations of configurations to use with vector search.
	Profiles *[]VectorSearchProfile `json:"profiles,omitempty"`
	// Algorithms - Contains configuration options specific to the algorithm used during indexing or querying.
	Algorithms *[]BasicVectorSearchAlgorithmConfiguration `json:"algorithms,omitempty"`
	// Vectorizers - Contains configuration options on how to vectorize text vector queries.
	Vectorizers *[]BasicVectorSearchVectorizer `json:"vectorizers,omitempty"`
	// Compressions - Contains configuration options specific to the compression method used during indexing or querying.
	Compressions *[]BasicVectorSearchCompressionConfiguration `json:"compressions,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for VectorSearch struct.
func (vs *VectorSearch) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "profiles":
			if v != nil {
				var profiles []VectorSearchProfile
				err = json.Unmarshal(*v, &profiles)
				if err != nil {
					return err
				}
				vs.Profiles = &profiles
			}
		case "algorithms":
			if v != nil {
				algorithms, err := unmarshalBasicVectorSearchAlgorithmConfigurationArray(*v)
				if err != nil {
					return err
				}
				vs.Algorithms = &algorithms
			}
		case "vectorizers":
			if v != nil {
				vectorizers, err := unmarshalBasicVectorSearchVectorizerArray(*v)
				if err != nil {
					return err
				}
				vs.Vectorizers = &vectorizers
			}
		case "compressions":
			if v != nil {
				compressions, err := unmarshalBasicVectorSearchCompressionConfigurationArray(*v)
				if err != nil {
					return err
				}
				vs.Compressions = &compressions
			}
		}
	}

	return nil
}

// BasicVectorSearchAlgorithmConfiguration contains configuration options specific to the algorithm used during
// indexing or querying.
type BasicVectorSearchAlgorithmConfiguration interface {
	AsHnswVectorSearchAlgorithmConfiguration() (*HnswVectorSearchAlgorithmConfiguration, bool)
	AsExhaustiveKnnVectorSearchAlgorithmConfiguration() (*ExhaustiveKnnVectorSearchAlgorithmConfiguration, bool)
	AsVectorSearchAlgorithmConfiguration() (*VectorSearchAlgorithmConfiguration, bool)
}

// VectorSearchAlgorithmConfiguration contains configuration options specific to the algorithm used during
// indexing or querying.
type VectorSearchAlgorithmConfiguration struct {
	// Name - The name to associate with this particular configuration.
	Name *string `json:"name,omitempty"`
	// Kind - Possible values include: 'KindVectorSearchAlgorithmConfiguration', 'KindHnsw', 'KindExhaustiveKnn'
	Kind Kind `json:"kind,omitempty"`
}

func unmarshalBasicVectorSearchAlgorithmConfiguration(body []byte) (BasicVectorSearchAlgorithmConfiguration, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["kind"] {
	case string(KindHnsw):
		var hvsac HnswVectorSearchAlgorithmConfiguration
		err := json.Unmarshal(body, &hvsac)
		return hvsac, err
	case string(KindExhaustiveKnn):
		var ekvsac ExhaustiveKnnVectorSearchAlgorithmConfiguration
		err := json.Unmarshal(body, &ekvsac)
		return ekvsac, err
	default:
		var vsac VectorSearchAlgorithmConfiguration
		err := json.Unmarshal(body, &vsac)
		return vsac, err
	}
}
func unmarshalBasicVectorSearchAlgorithmConfigurationArray(body []byte) ([]BasicVectorSearchAlgorithmConfiguration, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	vsacArray := make([]BasicVectorSearchAlgorithmConfiguration, len(rawMessages))

	for index, rawMessage := range rawMessages {
		vsac, err := unmarshalBasicVectorSearchAlgorithmConfiguration(*rawMessage)
		if err != nil {
			return nil, err
		}
		vsacArray[index] = vsac
	}
	return vsacArray, nil
}

// MarshalJSON is the custom marshaler for VectorSearchAlgorithmConfiguration.
func (vsac VectorSearchAlgorithmConfiguration) MarshalJSON() ([]byte, error) {
	vsac.Kind = KindVectorSearchAlgorithmConfiguration
	objectMap := make(map[string]interface{})
	if vsac.Name != nil {
		objectMap["name"] = vsac.Name
	}
	if vsac.Kind != "" {
		objectMap["kind"] = vsac.Kind
	}
	return json.Marshal(objectMap)
}

// AsHnswVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for VectorSearchAlgorithmConfiguration.
func (vsac VectorSearchAlgorithmConfiguration) AsHnswVectorSearchAlgorithmConfiguration() (*HnswVectorSearchAlgorithmConfiguration, bool) {
	return nil, false
}

// AsExhaustiveKnnVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for VectorSearchAlgorithmConfiguration.
func (vsac VectorSearchAlgorithmConfiguration) AsExhaustiveKnnVectorSearchAlgorithmConfiguration() (*ExhaustiveKnnVectorSearchAlgorithmConfiguration, bool) {
	return nil, false
}

// AsVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for VectorSearchAlgorithmConfiguration.
func (vsac VectorSearchAlgorithmConfiguration) AsVectorSearchAlgorithmConfiguration() (*VectorSearchAlgorithmConfiguration, bool) {
	return &vsac, true
}

// AsBasicVectorSearchAlgorithmConfiguration is the BasicVectorSearchAlgorithmConfiguration implementation for VectorSearchAlgorithmConfiguration.
func (vsac VectorSearchAlgorithmConfiguration) AsBasicVectorSearchAlgorithmConfiguration() (BasicVectorSearchAlgorithmConfiguration, bool) {
	return &vsac, true
}

// BasicVectorSearchCompressionConfiguration contains configuration options specific to the compression method used
// during indexing or querying.
type BasicVectorSearchCompressionConfiguration interface {
	AsScalarQuantizationVectorSearchCompressionConfiguration() (*ScalarQuantizationVectorSearchCompressionConfiguration, bool)
	AsVectorSearchCompressionConfiguration() (*VectorSearchCompressionConfiguration, bool)
}

// VectorSearchCompressionConfiguration contains configuration options specific to the compression method used
// during indexing or querying.
type VectorSearchCompressionConfiguration struct {
	// Name - The name to associate with this particular configuration.
	Name *string `json:"name,omitempty"`
	// RerankWithOriginalVectors - If set to true, once the ordered set of results calculated using compressed vectors are obtained, they will be reranked again by recalculating the full-precision similarity scores. This will improve recall at the expense of latency.
	RerankWithOriginalVectors *bool `json:"rerankWithOriginalVectors,omitempty"`
	// DefaultOversampling - Default oversampling factor. Oversampling will internally request more documents (specified by this multiplier) in the initial search. This increases the set of results that will be reranked using recomputed similarity scores from full-precision vectors. Minimum value is 1, meaning no oversampling (1x). This parameter can only be set when rerankWithOriginalVectors is true. Higher values improve recall at the expense of latency.
	DefaultOversampling *float64 `json:"defaultOversampling,omitempty"`
	// Kind - Possible values include: 'KindBasicVectorSearchCompressionConfigurationKindVectorSearchCompressionConfiguration', 'KindBasicVectorSearchCompressionConfigurationKindScalarQuantization'
	Kind KindBasicVectorSearchCompressionConfiguration `json:"kind,omitempty"`
}

func unmarshalBasicVectorSearchCompressionConfiguration(body []byte) (BasicVectorSearchCompressionConfiguration, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["kind"] {
	case string(KindBasicVectorSearchCompressionConfigurationKindScalarQuantization):
		var sqvscc ScalarQuantizationVectorSearchCompressionConfiguration
		err := json.Unmarshal(body, &sqvscc)
		return sqvscc, err
	default:
		var vscc VectorSearchCompressionConfiguration
		err := json.Unmarshal(body, &vscc)
		return vscc, err
	}
}
func unmarshalBasicVectorSearchCompressionConfigurationArray(body []byte) ([]BasicVectorSearchCompressionConfiguration, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	vsccArray := make([]BasicVectorSearchCompressionConfiguration, len(rawMessages))

	for index, rawMessage := range rawMessages {
		vscc, err := unmarshalBasicVectorSearchCompressionConfiguration(*rawMessage)
		if err != nil {
			return nil, err
		}
		vsccArray[index] = vscc
	}
	return vsccArray, nil
}

// MarshalJSON is the custom marshaler for VectorSearchCompressionConfiguration.
func (vscc VectorSearchCompressionConfiguration) MarshalJSON() ([]byte, error) {
	vscc.Kind = KindBasicVectorSearchCompressionConfigurationKindVectorSearchCompressionConfiguration
	objectMap := make(map[string]interface{})
	if vscc.Name != nil {
		objectMap["name"] = vscc.Name
	}
	if vscc.RerankWithOriginalVectors != nil {
		objectMap["rerankWithOriginalVectors"] = vscc.RerankWithOriginalVectors
	}
	if vscc.DefaultOversampling != nil {
		objectMap["defaultOversampling"] = vscc.DefaultOversampling
	}
	if vscc.Kind != "" {
		objectMap["kind"] = vscc.Kind
	}
	return json.Marshal(objectMap)
}

// AsScalarQuantizationVectorSearchCompressionConfiguration is the BasicVectorSearchCompressionConfiguration implementation for VectorSearchCompressionConfiguration.
func (vscc VectorSearchCompressionConfiguration) AsScalarQuantizationVectorSearchCompressionConfiguration() (*ScalarQuantizationVectorSearchCompressionConfiguration, bool) {
	return nil, false
}

// AsVectorSearchCompressionConfiguration is the BasicVectorSearchCompressionConfiguration implementation for VectorSearchCompressionConfiguration.
func (vscc VectorSearchCompressionConfiguration) AsVectorSearchCompressionConfiguration() (*VectorSearchCompressionConfiguration, bool) {
	return &vscc, true
}

// AsBasicVectorSearchCompressionConfiguration is the BasicVectorSearchCompressionConfiguration implementation for VectorSearchCompressionConfiguration.
func (vscc VectorSearchCompressionConfiguration) AsBasicVectorSearchCompressionConfiguration() (BasicVectorSearchCompressionConfiguration, bool) {
	return &vscc, true
}

// VectorSearchProfile defines a combination of configurations to use with vector search.
type VectorSearchProfile struct {
	// Name - The name to associate with this particular vector search profile.
	Name *string `json:"name,omitempty"`
	// AlgorithmConfigurationName - The name of the vector search algorithm configuration that specifies the algorithm and optional parameters.
	AlgorithmConfigurationName *string `json:"algorithm,omitempty"`
	// Vectorizer - The name of the kind of vectorization method being configured for use with vector search.
	Vectorizer *string `json:"vectorizer,omitempty"`
	// CompressionConfigurationName - The name of the compression method configuration that specifies the compression method and optional parameters.
	CompressionConfigurationName *string `json:"compression,omitempty"`
}

// BasicVectorSearchVectorizer specifies the vectorization method to be used during query time.
type BasicVectorSearchVectorizer interface {
	AsAzureOpenAIVectorizer() (*AzureOpenAIVectorizer, bool)
	AsCustomVectorizer() (*CustomVectorizer, bool)
	AsVectorSearchVectorizer() (*VectorSearchVectorizer, bool)
}

// VectorSearchVectorizer specifies the vectorization method to be used during query time.
type VectorSearchVectorizer struct {
	// Name - The name to associate with this particular vectorization method.
	Name *string `json:"name,omitempty"`
	// Kind - Possible values include: 'KindBasicVectorSearchVectorizerKindVectorSearchVectorizer', 'KindBasicVectorSearchVectorizerKindAzureOpenAI', 'KindBasicVectorSearchVectorizerKindCustomWebAPI'
	Kind KindBasicVectorSearchVectorizer `json:"kind,omitempty"`
}

func unmarshalBasicVectorSearchVectorizer(body []byte) (BasicVectorSearchVectorizer, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["kind"] {
	case string(KindBasicVectorSearchVectorizerKindAzureOpenAI):
		var aoav AzureOpenAIVectorizer
		err := json.Unmarshal(body, &aoav)
		return aoav, err
	case string(KindBasicVectorSearchVectorizerKindCustomWebAPI):
		var cv CustomVectorizer
		err := json.Unmarshal(body, &cv)
		return cv, err
	default:
		var vsv VectorSearchVectorizer
		err := json.Unmarshal(body, &vsv)
		return vsv, err
	}
}
func unmarshalBasicVectorSearchVectorizerArray(body []byte) ([]BasicVectorSearchVectorizer, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	vsvArray := make([]BasicVectorSearchVectorizer, len(rawMessages))

	for index, rawMessage := range rawMessages {
		vsv, err := unmarshalBasicVectorSearchVectorizer(*rawMessage)
		if err != nil {
			return nil, err
		}
		vsvArray[index] = vsv
	}
	return vsvArray, nil
}

// MarshalJSON is the custom marshaler for VectorSearchVectorizer.
func (vsv VectorSearchVectorizer) MarshalJSON() ([]byte, error) {
	vsv.Kind = KindBasicVectorSearchVectorizerKindVectorSearchVectorizer
	objectMap := make(map[string]interface{})
	if vsv.Name != nil {
		objectMap["name"] = vsv.Name
	}
	if vsv.Kind != "" {
		objectMap["kind"] = vsv.Kind
	}
	return json.Marshal(objectMap)
}

// AsAzureOpenAIVectorizer is the BasicVectorSearchVectorizer implementation for VectorSearchVectorizer.
func (vsv VectorSearchVectorizer) AsAzureOpenAIVectorizer() (*AzureOpenAIVectorizer, bool) {
	return nil, false
}

// AsCustomVectorizer is the BasicVectorSearchVectorizer implementation for VectorSearchVectorizer.
func (vsv VectorSearchVectorizer) AsCustomVectorizer() (*CustomVectorizer, bool) {
	return nil, false
}

// AsVectorSearchVectorizer is the BasicVectorSearchVectorizer implementation for VectorSearchVectorizer.
func (vsv VectorSearchVectorizer) AsVectorSearchVectorizer() (*VectorSearchVectorizer, bool) {
	return &vsv, true
}

// AsBasicVectorSearchVectorizer is the BasicVectorSearchVectorizer implementation for VectorSearchVectorizer.
func (vsv VectorSearchVectorizer) AsBasicVectorSearchVectorizer() (BasicVectorSearchVectorizer, bool) {
	return &vsv, true
}

// WebAPISkill a skill that can call a Web API endpoint, allowing you to extend a skillset by having it
// call your custom code.
type WebAPISkill struct {
	// URI - The url for the Web API.
	URI *string `json:"uri,omitempty"`
	// HTTPHeaders - The headers required to make the http request.
	HTTPHeaders map[string]*string `json:"httpHeaders"`
	// HTTPMethod - The method for the http request.
	HTTPMethod *string `json:"httpMethod,omitempty"`
	// Timeout - The desired timeout for the request. Default is 30 seconds.
	Timeout *string `json:"timeout,omitempty"`
	// BatchSize - The desired batch size which indicates number of documents.
	BatchSize *int32 `json:"batchSize,omitempty"`
	// DegreeOfParallelism - If set, the number of parallel calls that can be made to the Web API.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// AuthResourceID - Applies to custom skills that connect to external code in an Azure function or some other application that provides the transformations. This value should be the application ID created for the function or app when it was registered with Azure Active Directory. When specified, the custom skill connects to the function or app using a managed ID (either system or user-assigned) of the search service and the access token of the function or app, using this value as the resource id for creating the scope of the access token.
	AuthResourceID *string `json:"authResourceId,omitempty"`
	// AuthIdentity - The user-assigned managed identity used for outbound connections. If an authResourceId is provided and it's not specified, the system-assigned managed identity is used. On updates to the indexer, if the identity is unspecified, the value remains unchanged. If set to "none", the value of this property is cleared.
	AuthIdentity BasicIndexerDataIdentity `json:"authIdentity,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicIndexerSkillOdataTypeSearchIndexerSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3SentimentSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityLinkingSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextV3EntityRecognitionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextPIIDetectionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextCustomEntityLookupSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsUtilDocumentExtractionSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomAmlSkill', 'OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsTextAzureOpenAIEmbeddingSkill'
	OdataType OdataTypeBasicIndexerSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for WebAPISkill.
func (was WebAPISkill) MarshalJSON() ([]byte, error) {
	was.OdataType = OdataTypeBasicIndexerSkillOdataTypeMicrosoftSkillsCustomWebAPISkill
	objectMap := make(map[string]interface{})
	if was.URI != nil {
		objectMap["uri"] = was.URI
	}
	if was.HTTPHeaders != nil {
		objectMap["httpHeaders"] = was.HTTPHeaders
	}
	if was.HTTPMethod != nil {
		objectMap["httpMethod"] = was.HTTPMethod
	}
	if was.Timeout != nil {
		objectMap["timeout"] = was.Timeout
	}
	if was.BatchSize != nil {
		objectMap["batchSize"] = was.BatchSize
	}
	if was.DegreeOfParallelism != nil {
		objectMap["degreeOfParallelism"] = was.DegreeOfParallelism
	}
	if was.AuthResourceID != nil {
		objectMap["authResourceId"] = was.AuthResourceID
	}
	objectMap["authIdentity"] = was.AuthIdentity
	if was.Name != nil {
		objectMap["name"] = was.Name
	}
	if was.Description != nil {
		objectMap["description"] = was.Description
	}
	if was.Context != nil {
		objectMap["context"] = was.Context
	}
	if was.Inputs != nil {
		objectMap["inputs"] = was.Inputs
	}
	if was.Outputs != nil {
		objectMap["outputs"] = was.Outputs
	}
	if was.OdataType != "" {
		objectMap["@odata.type"] = was.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSentimentSkillV3 is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsSentimentSkillV3() (*SentimentSkillV3, bool) {
	return nil, false
}

// AsEntityLinkingSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsEntityLinkingSkill() (*EntityLinkingSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkillV3 is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsEntityRecognitionSkillV3() (*EntityRecognitionSkillV3, bool) {
	return nil, false
}

// AsPIIDetectionSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsPIIDetectionSkill() (*PIIDetectionSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsCustomEntityLookupSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsCustomEntityLookupSkill() (*CustomEntityLookupSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsDocumentExtractionSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsDocumentExtractionSkill() (*DocumentExtractionSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return &was, true
}

// AsAmlSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsAmlSkill() (*AmlSkill, bool) {
	return nil, false
}

// AsAzureOpenAIEmbeddingSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsAzureOpenAIEmbeddingSkill() (*AzureOpenAIEmbeddingSkill, bool) {
	return nil, false
}

// AsIndexerSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsIndexerSkill() (*IndexerSkill, bool) {
	return nil, false
}

// AsBasicIndexerSkill is the BasicIndexerSkill implementation for WebAPISkill.
func (was WebAPISkill) AsBasicIndexerSkill() (BasicIndexerSkill, bool) {
	return &was, true
}

// UnmarshalJSON is the custom unmarshaler for WebAPISkill struct.
func (was *WebAPISkill) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "uri":
			if v != nil {
				var URI string
				err = json.Unmarshal(*v, &URI)
				if err != nil {
					return err
				}
				was.URI = &URI
			}
		case "httpHeaders":
			if v != nil {
				var HTTPHeaders map[string]*string
				err = json.Unmarshal(*v, &HTTPHeaders)
				if err != nil {
					return err
				}
				was.HTTPHeaders = HTTPHeaders
			}
		case "httpMethod":
			if v != nil {
				var HTTPMethod string
				err = json.Unmarshal(*v, &HTTPMethod)
				if err != nil {
					return err
				}
				was.HTTPMethod = &HTTPMethod
			}
		case "timeout":
			if v != nil {
				var timeout string
				err = json.Unmarshal(*v, &timeout)
				if err != nil {
					return err
				}
				was.Timeout = &timeout
			}
		case "batchSize":
			if v != nil {
				var batchSize int32
				err = json.Unmarshal(*v, &batchSize)
				if err != nil {
					return err
				}
				was.BatchSize = &batchSize
			}
		case "degreeOfParallelism":
			if v != nil {
				var degreeOfParallelism int32
				err = json.Unmarshal(*v, &degreeOfParallelism)
				if err != nil {
					return err
				}
				was.DegreeOfParallelism = &degreeOfParallelism
			}
		case "authResourceId":
			if v != nil {
				var authResourceID string
				err = json.Unmarshal(*v, &authResourceID)
				if err != nil {
					return err
				}
				was.AuthResourceID = &authResourceID
			}
		case "authIdentity":
			if v != nil {
				authIdentity, err := unmarshalBasicIndexerDataIdentity(*v)
				if err != nil {
					return err
				}
				was.AuthIdentity = authIdentity
			}
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				was.Name = &name
			}
		case "description":
			if v != nil {
				var description string
				err = json.Unmarshal(*v, &description)
				if err != nil {
					return err
				}
				was.Description = &description
			}
		case "context":
			if v != nil {
				var context string
				err = json.Unmarshal(*v, &context)
				if err != nil {
					return err
				}
				was.Context = &context
			}
		case "inputs":
			if v != nil {
				var inputs []InputFieldMappingEntry
				err = json.Unmarshal(*v, &inputs)
				if err != nil {
					return err
				}
				was.Inputs = &inputs
			}
		case "outputs":
			if v != nil {
				var outputs []OutputFieldMappingEntry
				err = json.Unmarshal(*v, &outputs)
				if err != nil {
					return err
				}
				was.Outputs = &outputs
			}
		case "@odata.type":
			if v != nil {
				var odataType OdataTypeBasicIndexerSkill
				err = json.Unmarshal(*v, &odataType)
				if err != nil {
					return err
				}
				was.OdataType = odataType
			}
		}
	}

	return nil
}

// WordDelimiterTokenFilter splits words into subwords and performs optional transformations on subword
// groups. This token filter is implemented using Apache Lucene.
type WordDelimiterTokenFilter struct {
	// GenerateWordParts - A value indicating whether to generate part words. If set, causes parts of words to be generated; for example "AzureSearch" becomes "Azure" "Search". Default is true.
	GenerateWordParts *bool `json:"generateWordParts,omitempty"`
	// GenerateNumberParts - A value indicating whether to generate number subwords. Default is true.
	GenerateNumberParts *bool `json:"generateNumberParts,omitempty"`
	// CatenateWords - A value indicating whether maximum runs of word parts will be catenated. For example, if this is set to true, "Azure-Search" becomes "AzureSearch". Default is false.
	CatenateWords *bool `json:"catenateWords,omitempty"`
	// CatenateNumbers - A value indicating whether maximum runs of number parts will be catenated. For example, if this is set to true, "1-2" becomes "12". Default is false.
	CatenateNumbers *bool `json:"catenateNumbers,omitempty"`
	// CatenateAll - A value indicating whether all subword parts will be catenated. For example, if this is set to true, "Azure-Search-1" becomes "AzureSearch1". Default is false.
	CatenateAll *bool `json:"catenateAll,omitempty"`
	// SplitOnCaseChange - A value indicating whether to split words on caseChange. For example, if this is set to true, "AzureSearch" becomes "Azure" "Search". Default is true.
	SplitOnCaseChange *bool `json:"splitOnCaseChange,omitempty"`
	// PreserveOriginal - A value indicating whether original words will be preserved and added to the subword list. Default is false.
	PreserveOriginal *bool `json:"preserveOriginal,omitempty"`
	// SplitOnNumerics - A value indicating whether to split on numbers. For example, if this is set to true, "Azure1Search" becomes "Azure" "1" "Search". Default is true.
	SplitOnNumerics *bool `json:"splitOnNumerics,omitempty"`
	// StemEnglishPossessive - A value indicating whether to remove trailing "'s" for each subword. Default is true.
	StemEnglishPossessive *bool `json:"stemEnglishPossessive,omitempty"`
	// ProtectedWords - A list of tokens to protect from being delimited.
	ProtectedWords *[]string `json:"protectedWords,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeBasicTokenFilterOdataTypeTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) MarshalJSON() ([]byte, error) {
	wdtf.OdataType = OdataTypeBasicTokenFilterOdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter
	objectMap := make(map[string]interface{})
	if wdtf.GenerateWordParts != nil {
		objectMap["generateWordParts"] = wdtf.GenerateWordParts
	}
	if wdtf.GenerateNumberParts != nil {
		objectMap["generateNumberParts"] = wdtf.GenerateNumberParts
	}
	if wdtf.CatenateWords != nil {
		objectMap["catenateWords"] = wdtf.CatenateWords
	}
	if wdtf.CatenateNumbers != nil {
		objectMap["catenateNumbers"] = wdtf.CatenateNumbers
	}
	if wdtf.CatenateAll != nil {
		objectMap["catenateAll"] = wdtf.CatenateAll
	}
	if wdtf.SplitOnCaseChange != nil {
		objectMap["splitOnCaseChange"] = wdtf.SplitOnCaseChange
	}
	if wdtf.PreserveOriginal != nil {
		objectMap["preserveOriginal"] = wdtf.PreserveOriginal
	}
	if wdtf.SplitOnNumerics != nil {
		objectMap["splitOnNumerics"] = wdtf.SplitOnNumerics
	}
	if wdtf.StemEnglishPossessive != nil {
		objectMap["stemEnglishPossessive"] = wdtf.StemEnglishPossessive
	}
	if wdtf.ProtectedWords != nil {
		objectMap["protectedWords"] = wdtf.ProtectedWords
	}
	if wdtf.Name != nil {
		objectMap["name"] = wdtf.Name
	}
	if wdtf.OdataType != "" {
		objectMap["@odata.type"] = wdtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return &wdtf, true
}

// AsTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &wdtf, true
}
